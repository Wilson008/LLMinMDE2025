@article{10.1145/3728947,
author = {Fu, Yingjie and Li, Bozhou and Li, Linyi and Zhang, Wentao and Xie, Tao},
title = {The First Prompt Counts the Most! An Evaluation of Large Language Models on Iterative Example-Based Code Generation},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728947},
doi = {10.1145/3728947},
abstract = {The capabilities of Large Language Models (LLMs) in code generation have been extensively studied, particularly for implementing target functionalities from natural-language descriptions. As an alternative to natural language, input-output (I/O) examples provide an accessible, unambiguous, and flexible way to describe functionalities. However, their inherent diversity, opaqueness, and incompleteness impose greater challenges for understanding and implementing the target requirements. Therefore, generating code from I/O examples (i.e., example-based code generation) provides a new perspective, allowing us to additionally evaluate LLMs’ capability to infer target functionalities from limited information and to process new-form requirements. However, related research about LLMs in example-based code generation remains largely unexplored. To fill this gap, this paper presents the first comprehensive study on example-based code generation using LLMs. To address the incorrectness caused by the incompleteness of I/O examples, we adopt an iterative evaluation framework and formalize the objective of example-based code generation as two sequential sub-objectives: generating code conforming to the given examples and generating code that successfully implements the target functionalities from (iteratively) given examples. We assess six state-of-the-art LLMs using a new benchmark of 172 diverse target functionalities (derived from HumanEval and CodeHunt). The results demonstrate that when requirements are described using iterative I/O examples rather than natural language, the LLMs’ score decreases by over 60\%, indicating that example-based code generation remains challenging for the evaluated LLMs. Notably, the vast majority (even over 95\%) of successfully implemented functionalities are achieved in the first round of the iterations, suggesting that the LLMs struggle to effectively utilize the iteratively supplemented requirements. Furthermore, we find that combining I/O examples with even imprecise and fragmental natural language descriptions greatly improves LLM performance, and the selection of initial I/O examples can also influence the score, suggesting opportunities for prompt optimization. These findings highlight the importance of early prompts during interactions and offer critical insights and implications for enhancing LLM-based code generation.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA070},
numpages = {24},
keywords = {Empirical Study, Example-Based Code Generation, Large Language Models, Multi-Turn Interaction, Prompt Engineering}
}

@article{10.1145/3747290,
author = {Dahou, Abdelghani and Dahou, Abdelhalim Hafedh and Cheragui, Mohamed Amine and Abdedaiem, Amin and Al-Qaness, Mohammed A. A. and Abd Elaziz, Mohamed and Ewees, Ahmed A. and Zheng, Zhonglong},
title = {A Survey on Dialect Arabic Processing and Analysis: Recent Advances and Future Trends},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3747290},
doi = {10.1145/3747290},
abstract = {Advances in language models have enabled significant strides in developing language technologies tailored for analyzing and processing Dialectical Arabic (DA), which exhibits unique linguistic features and variations compared to standard Arabic. This progress has sparked a surge of interest in various research tasks within the Arabic Natural Language Processing (ANLP) domain, encompassing areas such as sentiment analysis, dialect identification, normalization and classification, fake news detection, and part-of-speech tagging. The primary objective of this survey paper is to provide a comprehensive overview of the advancements made in dialectical ANLP from 2014 to 2024. A thorough analysis is undertaken, covering a corpus of approximately 200 research papers, to offer insights into the latest developments, resources, and applications concerning dialectical Arabic. By identifying and discussing the challenges and opportunities for future research, this study aspires to serve as a valuable reference for researchers, practitioners, and enthusiasts interested in the subject matter. Central to the investigation are the recent strides in natural language processing techniques that pertain to dialectical Arabic, namely DA sentiment analysis, DA identification, DA classification, DA normalization, DA part-of-speech tagging, and the role of DA in fake news detection, among other applications. Each research category is meticulously examined, providing a comprehensive understanding of their respective contributions, significance, encountered challenges, and the availability of pertinent datasets. This exhaustive survey paper encompasses existing studies within dialectical Arabic research categories. As a result, readers are presented with a detailed reference source in pursuing advancements and innovations within this field.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {84},
numpages = {45},
keywords = {Deep learning, arabic dialect, sentiment analysis, classification, POS tag, datasets, fake news}
}

@inproceedings{10.1145/3706598.3713472,
author = {McNutt, Andrew M and Cohen, Sam and Chugh, Ravi},
title = {Slowness, Politics, and Joy: Values That Guide Technology Choices in Creative Coding Classrooms},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713472},
doi = {10.1145/3706598.3713472},
abstract = {There are many tools and technologies for making art with code, each embodying distinct values and affordances. Within this landscape, creative coding educators must evaluate how different tools map onto their own principles and examine the potential impacts of those choices on students’ learning and artistic development. Understanding the values guiding these decisions is critical, as they reflect insights about these contexts, communities, and pedagogies. We explore these values through semi-structured interviews with (N=12) creative coding educators and toolbuilders. We identify three major themes: slowness (how friction can make room for reflection), politics (including the lasting effects of particular technologies), and joy (or the capacity for playful engagement). The lessons and priorities voiced by our participants offer valuable, transferable perspectives—like preferring community building (such as through documentation) over techno-solutionism. We demonstrate application of these critical lenses to two tool design areas (accessibility and AI assistance).},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {54},
numpages = {16},
keywords = {Creative Coding, Interview Study, Power, Reflection, Arts},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3663529.3663779,
author = {Zheng, Xi and Mok, Aloysius K. and Piskac, Ruzica and Lee, Yong Jae and Krishnamachari, Bhaskar and Zhu, Dakai and Sokolsky, Oleg and Lee, Insup},
title = {Testing Learning-Enabled Cyber-Physical Systems with Large-Language Models: A Formal Approach},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663779},
doi = {10.1145/3663529.3663779},
abstract = {The integration of machine learning into cyber-physical systems (CPS) promises enhanced efficiency and autonomous capabilities, revolutionizing fields like autonomous vehicles and telemedicine. This evolution necessitates a shift in the software development life cycle, where data and learning are pivotal. Traditional verification and validation methods are inadequate for these AI-driven systems. This study focuses on the challenges in ensuring safety in learning-enabled CPS. It emphasizes the role of testing as a primary method for verification and validation, critiques current methodologies, and advocates for a more rigorous approach to assure formal safety.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {467–471},
numpages = {5},
keywords = {AI-based Systems, LLM-based Testing, automata-learning, model-based testing},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3643834.3660731,
author = {Chan, Joannes and De Paoli, Chris and Li, Michelle and Grossman, Tovi and Santosa, Stephanie and Wigdor, Daniel and Glueck, Michael},
title = {Fidgets: Building Blocks for a Predictive UI Toolkit},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3660731},
doi = {10.1145/3643834.3660731},
abstract = {The rapid growth of AR platforms, combined with the rising predictive power of intelligent systems, will fundamentally change interactive computing. Interaction will increasingly happen on the go, causing I/O to become constrained, ultimately leading to reliance on user intent prediction for aid. In this pictorial, we argue that to support the development of such systems, new predictive UI toolkits are required. We place the reader in the shoes of an App designer and outline the challenges that will be faced. We then describe a new predictive toolkit, leveraging Fuzzy Widgets, or “Fidgets” as the main UI building block. Fidgets extend Responsive Design into the realm of intelligent systems, to adapt not only to spatial constraints, but to system predictions as well. We then describe a working implementation of a predictive music application, built using our described framework, showcasing its benefits and range of adaptive abilities.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {1290–1305},
numpages = {16},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@article{10.1145/3652594,
author = {Bayer, Markus and Kuehn, Philipp and Shanehsaz, Ramin and Reuter, Christian},
title = {CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {2471-2566},
url = {https://doi.org/10.1145/3652594},
doi = {10.1145/3652594},
abstract = {The field of cysec is evolving fast. Security professionals are in need of intelligence on past, current and —ideally — upcoming threats, because attacks are becoming more advanced and are increasingly targeting larger and more complex systems. Since the processing and analysis of such large amounts of information cannot be addressed manually, cysec experts rely on machine learning techniques. In the textual domain, pre-trained language models such as Bidirectional Encoder Representations from Transformers (BERT) have proven to be helpful as they provide a good baseline for further fine-tuning. However, due to the domain-knowledge and the many technical terms in cysec, general language models might miss the gist of textual information. For this reason, we create a high-quality dataset1 and present a language model2 specifically tailored to the cysec domain that can serve as a basic building block for cybersecurity systems. The model is compared on 15 tasks: Domain-dependent extrinsic tasks for measuring the performance on specific problems, intrinsic tasks for measuring the performance of the internal representations of the model, as well as general tasks from the SuperGLUE benchmark. The results of the intrinsic tasks show that our model improves the internal representation space of domain words compared with the other models. The extrinsic, domain-dependent tasks, consisting of sequence tagging and classification, show that the model performs best in cybersecurity scenarios. In addition, we pay special attention to the choice of hyperparameters against catastrophic forgetting, as pre-trained models tend to forget the original knowledge during further training.},
journal = {ACM Trans. Priv. Secur.},
month = apr,
articleno = {18},
numpages = {20},
keywords = {Language model, cybersecurity BERT, cybersecurity dataset}
}

@inproceedings{10.1145/3695053.3731409,
author = {Coburn, Joel and Tang, Chunqiang and Asal, Sameer Abu and Agrawal, Neeraj and Chinta, Raviteja and Dixit, Harish and Dodds, Brian and Dwarakapuram, Saritha and Firoozshahian, Amin and Gao, Cao and Gondkar, Kaustubh and Graf, Tyler and Hu, Junhan and Huang, Jian and Hughes, Sterling and Hutchin, Adam and Jakka, Bhasker and Chen, Guoqiang Jerry and Kalyanaraman, Indu and Kamath, Ashwin and Kansal, Pankaj and Kazi, Erum and Levenstein, Roman and Maddury, Mahesh and Mastro, Alex and Medaiyese, Siji and Modi, Pritesh and Montgomery, Jack and Nadathur, Satish and Nagpal, Amit and Narasimha, Ashwin and Naumov, Maxim and Ozer, Eleanor and Park, Jongsoo and Ramani, Poorvaja and Reddy, Harikrishna and Reiss, David and Roy, Deboleena and Sekar, Sathish and Sharma, Arushi and Shetty, Pavan and Sukumaran-Rajam, Aravind and Tal, Eran and Tsai, Mike and Varshini, Shreya and Wareing, Richard and Wu, Olivia and Xie, Xiaolong and Yang, Jinghan and Yu, Hangchen and Zargar, Tanmay and Zeng, Zitong and Zhang, Feixiong and Matthews, Ajit and Jiao, Xun and Zhang, Jiyuan and Menage, Emmanuel and Stokke, Truls Edvard and Sourouri, Mohammed},
title = {Meta's Second Generation AI Chip: Model-Chip Co-Design and Productionization Experiences},
year = {2025},
isbn = {9798400712616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3695053.3731409},
doi = {10.1145/3695053.3731409},
abstract = {The rapid growth of AI workloads at Meta has motivated our in-house development of AI chips, aiming to significantly reduce the total cost of ownership and mitigate risks posed by unpredictable GPU supplies. At ISCA’23, we presented Meta’s first-generation AI chip, MTIA 1. This paper describes its successor, MTIA 2i, now deployed at scale and serving billions of users. MTIA 2i significantly improves upon MTIA 1, reducing total cost of ownership by 44\% compared to GPUs while delivering competitive performance per watt. A key differentiator is its memory hierarchy: instead of costly HBM, it uses large SRAM alongside LPDDR. Although there has been a proliferation of publications on AI chips, they often focus on architectural design and overlook three critical aspects: (1)&nbsp;co-designing and optimizing ML models to work effectively with the AI chip; (2) demonstrating sufficient flexibility to support a wide range of models; and (3)&nbsp;during the productionization process, addressing challenges unanticipated or decisions deferred at design time, such as dealing with memory errors, safe overclocking, reducing provisioned power, and implementing real-time firmware updates to mitigate silicon design defects. A key contribution of this paper is sharing our experience with these aspects, based on our journey of productionizing MTIA 2i at scale.},
booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture},
pages = {1689–1702},
numpages = {14},
keywords = {Accelerators, Artificial intelligence, Inference, Machine learning, Memory hierarchy, Productionization, Deep learning recommendation models, Total Cost of Ownership},
location = {
},
series = {ISCA '25}
}

@inproceedings{10.1145/3658644.3690327,
author = {Wang, Peiran and Li, Qiyu and Yu, Longxuan and Wang, Ziyao and Li, Ang and Jin, Haojian},
title = {Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3690327},
doi = {10.1145/3658644.3690327},
abstract = {We present Moderator, a policy-based model management system that allows administrators to specify fine-grained content moderation policies and modify the weights of a text-to-image (TTI) model to make it significantly more challenging for users to produce images that violate the policies. In contrast to existing general-purpose model editing techniques, which unlearn concepts without considering the associated contexts, Moderator allows admins to specify what content should be moderated, under which context, how it should be moderated, and why moderation is necessary. Given a set of policies, Moderator first prompts the original model to generate images that need to be moderated, then uses these self-generated images to reverse fine-tune the model to compute task vectors for moderation and finally negates the original model with the task vectors to decrease its performance in generating moderated content. We evaluated Moderator with 14 participants to play the role of admins and found they could quickly learn and author policies to pass unit tests in approximately 2.29 policy iterations. Our experiment with 32 stable diffusion users suggested that Moderator can prevent 65\% of users from generating moderated content under 15 attempts and require the remaining users an average of 8.3 times more attempts to generate undesired content.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {1181–1195},
numpages = {15},
keywords = {content moderation, policy language, text-to-image model},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3718958.3750537,
author = {Wang, Zhaodong and Lin, Samuel and Yan, Guanqing and Ghorbani, Soudeh and Yu, Minlan and Zhou, Jiawei and Hu, Nathan and Baruah, Lopa and Peters, Sam and Kamath, Srikanth and Yang, Jerry and Zhang, Ying},
title = {Intent-Driven Network Management with Multi-Agent LLMs: The Confucius Framework},
year = {2025},
isbn = {9798400715242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3718958.3750537},
doi = {10.1145/3718958.3750537},
abstract = {Advancements in Large Language Models (LLMs) are significantly transforming network management practices. In this paper, we present our experience developing Confucius, a multi-agent framework for network management at Meta. We model network management workflows as directed acyclic graphs (DAGs) to aid planning. Our framework integrates LLMs with existing management tools to achieve seamless operational integration, employs retrieval-augmented generation (RAG) to improve long-term memory, and establishes a set of primitives to systematically support human/model interaction. To ensure the accuracy of critical network operations, Confucius closely integrates with existing network validation methods and incorporates its own validation framework to prevent regressions. Remarkably, Confucius is a production-ready LLM development framework that has been operational for two years, with over 60 applications onboarded. To our knowledge, this is the first report on employing multi-agent LLMs for hyper-scale networks.},
booktitle = {Proceedings of the ACM SIGCOMM 2025 Conference},
pages = {347–362},
numpages = {16},
keywords = {large language models (LLMs), RAG, network planning},
location = {S\~{a}o Francisco Convent, Coimbra, Portugal},
series = {SIGCOMM '25}
}

@article{10.1109/TCBB.2024.3412174,
author = {Dai, Yuanfei and Zhang, Bin and Wang, Shiping},
title = {Distantly Supervised Biomedical Relation Extraction via Negative Learning and Noisy Student Self-Training},
year = {2024},
issue_date = {Nov.-Dec. 2024},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {21},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2024.3412174},
doi = {10.1109/TCBB.2024.3412174},
abstract = {Biomedical relation extraction aims to identify underlying relationships among entities, such as gene associations and drug interactions, within biomedical texts. Despite advancements in relation extraction in general knowledge domains, the scarcity of labeled training data remains a significant challenge in the biomedical field. This paper provides a novel approach for biomedical relation extraction that leverages a noisy student self-training strategy combined with negative learning. This method addresses the challenge of data insufficiency by utilizing distantly supervised data to generate high-quality labeled samples. Negative learning, as opposed to traditional positive learning, offers a more robust mechanism to discern and relabel noisy samples, preventing model overfitting. The integration of these techniques ensures enhanced noise reduction and relabeling capabilities, leading to improved performance even with noisy datasets. Experimental results demonstrate the effectiveness of the proposed framework in mitigating the impact of noisy data and outperforming existing benchmarks.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jun,
pages = {1697–1708},
numpages = {12}
}

@inproceedings{10.1145/3696410.3714901,
author = {Wang, Xin and Feng, Ling and Zhang, Huijun and Cao, Lei and Zeng, Kaisheng and Li, Qi and Ding, Yang and Dai, Yi and Clifton, David},
title = {MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714901},
doi = {10.1145/3696410.3714901},
abstract = {Stress haunts people in modern society, which may cause severe health issues if left unattended. With social media becoming an integral part of daily life, leveraging social media to detect stress has gained increasing attention. While the majority of the work focuses on classifying stress states and stress categories, this study introduce a new task aimed at estimating more specific stressors (like exam, writing paper, etc.) through users' posts on social media. Unfortunately, the diversity of stressors with many different classes but a few examples per class, combined with the consistent arising of new stressors over time, hinders the machine understanding of stressors. To this end, we cast the stressor estimation problem within a practical scenario few-shot learning setting, and propose a novel meta-learning based stressor estimation framework that is enhanced by a meta-knowledge inheritance mechanism. This model can not only learn generic stressor context through meta-learning, but also has a good generalization ability to estimate new stressors with little labeled data. A fundamental breakthrough in our approach lies in the inclusion of the meta-knowledge inheritance mechanism, which equips our model with the ability to prevent catastrophic forgetting when adapting to new stressors. The experimental results show that our model achieves state-of-the-art performance compared with the baselines. Additionally, we construct a social media-based stressor estimation dataset that can help train artificial intelligence models to facilitate human well-being.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {1866–1876},
numpages = {11},
keywords = {meta-knowledge inheritance, social media, stressor estimation},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3643661.3643951,
author = {Darnell, Benjamin and Chopra, Hetarth and Councilman, Aaron and Grove, David and Wang, Yu-Xiong and Adve, Vikram},
title = {An Empirical Comparison of Code Generation Approaches for Ansible},
year = {2024},
isbn = {9798400705649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643661.3643951},
doi = {10.1145/3643661.3643951},
abstract = {The rapid proliferation of LLM-based programming assistants has enabled fast and accurate automatic code generation for general purpose programming languages. Domain-specific languages like Ansible, a DSL for IT Automation, have seen a lack of support despite being critical to many fields, due to limited public-domain code for training models and a lack of interest from tool developers. To address this issue, we collect a novel dataset of permissively licensed Ansible code, and use it to create Warp, an LLM for code fine-tuned to produce Ansible tasks from a natural language prompt. We evaluate state-of-the-art tools for LLM-based code generation models, comparing multiple common strategies, including fine-tuning base models on Ansible code and retrieval-augmented-generation using documentation, in order to understand challenges with existing methodology and identify future research directions to enable better code generation for DSLs.},
booktitle = {Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {large language models, code generation, domain specific languages, ansible},
location = {Lisbon, Portugal},
series = {InteNSE '24}
}

@inproceedings{10.1145/3696410.3714891,
author = {Gui, Yi and Wan, Yao and Li, Zhen and Zhang, Zhongyi and Chen, Dongping and Zhang, Hongyu and Su, Yi and Chen, Bohua and Zhou, Xing and Jiang, Wenbin and Zhang, Xiangliang},
title = {UICopilot: Automating UI Synthesis via Hierarchical Code Generation from Webpage Designs},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714891},
doi = {10.1145/3696410.3714891},
abstract = {Automating the synthesis of User Interfaces (UIs) plays a crucial role in enhancing productivity and accelerating the development lifecycle, reducing both development time and manual effort. Recently, the rapid development of Multimodal Large Language Models (MLLMs) has made it possible to generate front-end Hypertext Markup Language (HTML) code directly from webpage designs. However, real-world webpages encompass not only a diverse array of HTML tags but also complex stylesheets, resulting in significantly lengthy code. The lengthy code poses challenges for the performance and efficiency of MLLMs, especially in capturing the structural information of UI designs. To address these challenges, this paper proposes UICopilot, a novel approach to automating UI synthesis via hierarchical code generation from webpage designs. To validate the effectiveness of UICopilot, we conduct experiments on a real-world dataset, i.e., WebCode2M. Experimental results demonstrate that UICopilot significantly outperforms existing baselines in both automatic evaluation metrics and human evaluations. Specifically, statistical analysis reveals that the majority of human annotators prefer the webpages generated by UICopilot over those produced by GPT-4V.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {1846–1855},
numpages = {10},
keywords = {UI automation, UI synthesis, code generation, design to code},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3675888.3676107,
author = {Kumar, Pratiksh and Gupta, Rishik and Kumar, Bagesh and Kumar, Aman},
title = {Bridging the Gap: Leveraging Textual and Visual Contexts for PreciseMedical Visual Question Answering},
year = {2024},
isbn = {9798400709722},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675888.3676107},
doi = {10.1145/3675888.3676107},
abstract = {The advent of Visual Question Answering (VQA) technology has brought significant advancements in the medical field, offering transformative potential in clinical diagnostics and patient care. This research explores the application of VQA within the medical domain, highlighting its critical role in interpreting complex visual data, such as radiological images, pathology slides, and other diagnostic visuals. Traditional diagnostic processes often rely heavily on human expertise, which can be time-consuming and prone to variability. VQA systems, powered by sophisticated machine learning models, provide consistent and accurate interpretations, thus enhancing diagnostic accuracy and efficiency. Visual Question Answering (VQA) in the medical field necessitates extracting information from both textual and visual inputs to provide accurate answers, a critical requirement for supporting medical decision-making. This research introduces a novel approach to address VQA challenges in the medical domain using Bi-Directional Layout with Positional Encoding (BLIP) models. Our methodology seamlessly integrates text and image processing within a unified framework, enabling precise interactions between textual queries and medical imaging data. We commence with textual inputs, encoded by BLIP processors, and medical images, encoded by BLIP image processors. A custom VQA dataset, specifically designed for the medical field, includes textual questions and their corresponding medical image features. We employ a BLIP-based Question Answering architecture, fine-tuned on our medical VQA dataset, and optimized using the AdamW optimizer with a learning rate of 0.00005, ensuring efficient convergence. Additionally, we introduce attention mechanisms using Coarse and Fine Attention blocks for enhanced feature fusion and accurate answer prediction. Our results are highly encouraging, demonstrating competitive metrics in extensive VQA task experiments on both training and validation datasets. Qualitative analysis of sample predictions indicates the model’s capability to provide accurate answers for diverse visual and textual medical inputs. This work holds significant promise for improving automated medical image analysis and supporting clinical decision-making.},
booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing},
pages = {519–526},
numpages = {8},
keywords = {Attention Model, BLIP, Medical Visual Question Answering, PathVQA},
location = {Noida, India},
series = {IC3-2024}
}

@inproceedings{10.1145/3721145.3725774,
author = {Guan, Jiexiong and Hu, Zhenqing and Antonopoulos, Christos D. and Bellas, Nikolaos and Lalis, Spyros and Smirni, Evgenia and Zhou, Gang and Agrawal, Gagan and Ren, Bin},
title = {TMModel: Modeling Texture Memory and Mobile GPU Performance to Accelerate DNN Computations},
year = {2025},
isbn = {9798400715372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721145.3725774},
doi = {10.1145/3721145.3725774},
abstract = {The demand for Deep Neural Network (DNN) execution (including both inference and training) on mobile system-on-a-chip (SoCs) has surged, driven by factors like the need for real-time latency, privacy, and reducing vendors’ costs. Mainstream mobile GPUs (e.g., Qualcomm Adreno GPUs) usually have a 2.5D L1 texture cache that offers throughput superior to that of on-chip memory. However, to date, there is limited understanding of the performance features of such a 2.5D cache, which limits the optimization potential. This paper introduces TMModel, a framework with three components: 1) a set of micro-benchmarks and a novel performance assessment methodology to characterize a non-well-documented architecture with 2D memory, 2) a complete analytical performance model configurable for different data access pattern(s), tiling size(s), and other GPU execution parameters for a given operator (and associated size and shape), and 3) a compilation framework incorporating this model and generating optimized code with low overhead. TMModel is validated both on a set of DNN kernels and for training complete models on mobile GPU, and compared against both popular mobile DNN frameworks and another GPU performance model. Evaluation results demonstrate that TMModel outperforms all baselines, achieving 1.48 − 3.61 \texttimes{} speedup on individual kernels and 1.83 − 66.1 \texttimes{} speedup for end-to-end on-device training with only (0.25\%-18.5\%) the tuning cost of the baselines.},
booktitle = {Proceedings of the 39th ACM International Conference on Supercomputing},
pages = {205–220},
numpages = {16},
keywords = {Texture Memory, Performance Modeling, Architecture Profiling, Automatic Code Generation, On-device Training},
location = {
},
series = {ICS '25}
}

@article{10.1145/3708521,
author = {Li, Rui and Liu, Huai and Poon, Pak-Lok and Towey, Dave and Sun, Chang-Ai and Zheng, Zheng and Zhou, Zhi Quan and Chen, Tsong Yueh},
title = {Metamorphic Relation Generation: State of the Art and Research Directions},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708521},
doi = {10.1145/3708521},
abstract = {Metamorphic testing has become one mainstream technique to address the notorious oracle problem in software testing, thanks to its great successes in revealing real-life bugs in a wide variety of software systems. Metamorphic relations, the core component of metamorphic testing, have continuously attracted research interests from both academia and industry. In the last decade, a rapidly increasing number of studies have been conducted to systematically generate metamorphic relations from various sources and for different application domains. In this article, based on the systematic review on the state of the art for metamorphic relations’ generation, we summarize and highlight visions for further advancing the theory and techniques for identifying and constructing metamorphic relations and discuss promising research directions in related areas.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {149},
numpages = {25},
keywords = {Metamorphic testing, Metamorphic relation, Metamorphic relation generation}
}

@inproceedings{10.1145/3698385.3699876,
author = {Jiang, Baozheng and Zhang, Haoxiang and Li, Yanxia and Zhou, Hexiao and Xiao, Zexiao and He, Sijia and Qiu, Wenying and Li, You},
title = {A Practical Investigation of the Accuracy of Large Language Models in Various Industrial Application Scenarios},
year = {2024},
isbn = {9798400712975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698385.3699876},
doi = {10.1145/3698385.3699876},
abstract = {Large language models have revolutionized natural language processing, facilitating a wide range of industrial applications. However, their precision and reliability in executing specific tasks across various industries remain incompletely understood. This study investigates the performance of these models across eight industrial scenarios, including code generation, safety monitoring, industrial prototype design, industrial knowledge inquiry, etc. By assessing their accuracy in these domains, the research highlights their practical utility, inherent limitations, and pinpoints areas for enhancement to optimize their deployment in industrial settings.},
booktitle = {Proceedings of the First International Workshop on IoT Datasets for Multi-Modal Large Model},
pages = {44–49},
numpages = {6},
keywords = {Industrial Applications, Large Language Models, Model Accuracy Evaluation},
location = {Hangzhou, China},
series = {IOTMMIM '24}
}

@inproceedings{10.1109/ASE56229.2023.00064,
author = {Shahariar, G. M. and Hasant, Tahmid and Iqbalt, Anindya and Uddin, Gias},
title = {Contrastive Learning for API Aspect Analysis},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00064},
doi = {10.1109/ASE56229.2023.00064},
abstract = {We present a novel approach - CLAA - for API aspect detection in API reviews that utilizes transformer models trained with a supervised contrastive loss objective function. We evaluate CLAA using performance and impact analysis. For performance analysis, we utilized a benchmark dataset on developer discussions collected from Stack Overflow and compare the results to those obtained using state-of-the-art transformer models. Our experiments show that contrastive learning can significantly improve the performance of transformer models in detecting aspects such as Performance, Security, Usability, and Documentation. For impact analysis, we performed empirical and developer study. On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92\% accuracy while the SOTA baseline achieved 81.5\%. According to our developer study involving 10 participants, the use of Stack Overflow + CLAA resulted in increased accuracy and confidence during API selection. Replication package: https://github.com/disa-lab/Contrastive-Learning-API-Aspect-ASE2023.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {637–648},
numpages = {12},
keywords = {API aspects, contrastive learning, transformers, API review, aspect detection, LIME},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1145/3729362,
author = {Jang, Sujin and Ryou, Yeonhee and Lee, Heewon and Heo, Kihong},
title = {UnitCon: Synthesizing Targeted Unit Tests for Java Runtime Exceptions},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729362},
doi = {10.1145/3729362},
abstract = {We present UnitCon, a system for synthesizing targeted unit testsfor runtime exceptions in Java programs. Targeted unit tests aim to reveal a bug at a specific location in the program under test. This capability benefits various tasks in software development, such as patch testing, crash reproduction, or static analysis alarm inspection. However, conventional unit test generation tools are mainly designed for regression tests by maximizing code coverage; hence they are not effective at such target-specific tasks. In this paper, we propose a novel synthesis technique that effectively guides the search for targeted unit tests. The key idea is to use static analysis to prune and prioritize the search space by estimating the semantics of candidate test cases. This allows us to efficiently focus on promising unit tests that are likely to trigger runtime exceptions at the target location. According to our experiments on a suite of Java programs, our approach outperforms the state-of-the-art unit test generation tools. We also applied UnitCon for inspecting static analysis alarms for null pointer exceptions (NPEs) in 51 open-source projects and discovered 21 previously unknown NPE bugs.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE092},
numpages = {22},
keywords = {Program analysis, Program synthesis, Software testing}
}

@article{10.1145/3673906,
author = {Qin, Ruiyang and Yang, Kai and Abbasi, Ahmed and Dobolyi, David and Seyedi, Salman and Griner, Emily and Kwon, Hyeokhyen and Cotes, Robert and Jiang, Zifan and Clifford, Gari and Cook, Ryan A.},
title = {Language Models for Online Depression Detection: A Review and Benchmark Analysis on Remote Interviews},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3673906},
doi = {10.1145/3673906},
abstract = {The use of machine learning (ML) to detect depression in online settings has emerged as an important health and wellness use case. In particular, the use of deep learning methods for depression detection from textual content posted on social media has garnered considerable attention. Conversely, there has been relatively limited evaluation of depression detection in clinical environments involving text generated from remote interviews. In this research, we review state-of-the-art feature-based ML, deep learning, and large language models for depression detection. We use a multidimensional analysis framework to benchmark various language models on a novel testbed comprising speech-to-text transcriptions of remote interviews. Our framework considers the impact of different transcription types and interview segments on depression detection performance. Finally, we summarize the key trends and takeaways from the review and benchmark evaluation and provide suggestions to guide the design of future detection methods.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {12},
numpages = {35},
keywords = {Deep learning, language models, depression detection, mental health, remote interviews}
}

@inproceedings{10.1145/3701716.3717851,
author = {Sobti, Divyam and Agumbe Suresh, Mahima and Tortora, Cristina and Viswanathan, Vimal},
title = {Domain-Specific Aspect Extraction for Product Design},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717851},
doi = {10.1145/3701716.3717851},
abstract = {As technology advances, computers become increasingly proficient at interpreting and translating human language into machine-understandable text. With the help of algorithms in natural language processing (NLP), machines can now translate textual data. These algorithms help identify and extract specific text components known as aspects. The aspects represent specific attributes or topics within textual data. For instance, if a product review states,'' This phone has good battery life but poor camera quality,'' and attributes like 'battery life' and 'camera quality' represent aspects in the text. Aspect extraction is a pivotal process involving identifying and isolating key features or topics within text. This research aims to compare and discuss the existing aspect extraction techniques. By effectively extracting the aspects, we will help designers gain the capability to understand and analyze sentiments, thereby enhancing their ability to derive meaningful insights from diverse textual data. Aspect-based sentiment analysis (ABSA) enables the extraction of sentiments towards specific aspects of a product the user provides. For example, when a person writes a review about a restaurant, sentiment analysis can determine whether the review is positive or negative. ABSA can separately determine the review's sentiment towards different aspects of a restaurant, such as, food quality, ambiance, etc. An important input to ABSA are the aspect keywords to find in the reviews. We propose an approach to extract features/aspects from customer-based product reviews. We experiment with different word embedding and clustering techniques to identify the best set of parameters for product design. Our experiments indicate that Global Vectors (GloVe) used on Gaussian Mixture Models (GMMs) yield the most insightful results for the specific problem domain.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2754–2763},
numpages = {10},
keywords = {clustering, mixture models, product design, word embeddings},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3764593,
author = {Kumar, Smitha S and Lones, Michael and Maarek, Manuel and Zantout, Hind},
title = {Navigating the landscape of automated feedback generation techniques for programming exercises},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3764593},
doi = {10.1145/3764593},
abstract = {Programming demands a variety of cognitive skills, and mastering these competencies is essential for success in computer science education. The importance of formative feedback is well acknowledged in programming education, and thus a diverse range of techniques has been proposed to generate and enhance formative feedback for programming exercises. This paper reviews state-of-the-art automated feedback generation techniques and categorizes the various approaches based on the underlying computational techniques, programming languages, the kind of programming errors they deal with, and the type of feedback they provide. It covers data-driven techniques, those which use program repair methods, machine learning-based techniques, and techniques based around the use of large language models, particularly noting the rapid uptake of the latter. The paper provides a summary of key findings and challenges, alongside recommendations for future work. The findings reveal that although there exist numerous tools for automated programming feedback, many studies depend on non-public benchmarks, which limits reproducibility and independent evaluation of the tools and their datasets. Additionally, tools are not always language-agnostic, and in some cases involve complex configuration steps. Large language models have demonstrated transformative potential in generating feedback. However, most research has focused on introductory courses (CS1 and CS2) indicating the need to apply them in advanced fields like machine learning and image processing. Although large language models have outperformed traditional approaches, challenges related to hallucinations and incorrect responses still need to be addressed as precision is critical in a pedagogical setting. Most of the studies use proprietary models that lack transparency and customization options, emphasizing the need for further research into open LLM alternatives.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Educ.},
month = sep,
keywords = {literature review, automated feedback, hints, learning programming, programming languages}
}

@article{10.1145/3622830,
author = {Feser, Jack and Dillig, I\c{s}\i{}l and Solar-Lezama, Armando},
title = {Inductive Program Synthesis Guided by Observational Program Similarity},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622830},
doi = {10.1145/3622830},
abstract = {We present a new general-purpose synthesis technique for generating programs from input-output examples. Our method, called metric program synthesis, relaxes the observational equivalence idea (used widely in bottom-up enumerative synthesis) into a weaker notion of observational similarity, with the goal of reducing the search space that the synthesizer needs to explore. Our method clusters programs into equivalence classes based on an expert-provided distance metric and constructs a version space that compactly represents “approximately correct” programs. Then, given a “close enough” program sampled from this version space, our approach uses a distance-guided repair algorithm to find a program that exactly matches the given input-output examples. We have implemented our proposed metric program synthesis technique in a tool called SyMetric and evaluate it in three different domains considered in prior work. Our evaluation shows that SyMetric outperforms other domain-agnostic synthesizers that use observational equivalence and that it achieves results competitive with domain-specific synthesizers that are either designed for or trained on those domains.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {254},
numpages = {29},
keywords = {regular expression inference, program synthesis, inverse csg, distance metric}
}

@article{10.1145/3729348,
author = {Tu, Zhi and Niu, Liangkun and Fan, Wei and Zhang, Tianyi},
title = {Multi-modal Traffic Scenario Generation for Autonomous Driving System Testing},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729348},
doi = {10.1145/3729348},
abstract = {Autonomous driving systems (ADS) require extensive testing and validation before deployment. However, it is tedious and time-consuming to construct traffic scenarios for ADS testing. In this paper, we propose TrafficComposer, a multi-modal traffic scenario construction approach for ADS testing. TrafficComposer takes as input a natural language (NL) description of a desired traffic scenario and a complementary traffic scene image. Then, it generates the corresponding traffic scenario in a simulator, such as CARLA and LGSVL. Specifically, TrafficComposer integrates high-level dynamic information about the traffic scenario from the NL description and intricate details about the surrounding vehicles, pedestrians, and the road network from the image. The information from the two modalities is complementary to each other and helps generate high-quality traffic scenarios for ADS testing. On a benchmark of 120 traffic scenarios, TrafficComposer achieves 97.0\% accuracy, outperforming the best-performing baseline by 7.3\%. Both direct testing and fuzz testing experiments on six ADSs prove the bug detection capabilities of the traffic scenarios generated by TrafficComposer. These scenarios can directly discover 37 bugs and help two fuzzing methods find 33\%–124\% more bugs serving as initial seeds.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE078},
numpages = {24},
keywords = {Autonomous Driving System, Software Testing, Traffic Scenario Generation}
}

@inproceedings{10.1145/3646548.3672584,
author = {Kogler, Philipp and Chen, Wei and Falkner, Andreas and Haselb\"{o}ck, Alois and Wallner, Stefan},
title = {Modelling Engineering Processes in Natural Language: A Case Study},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672584},
doi = {10.1145/3646548.3672584},
abstract = {Engineering process management aims to formally specify processes which are executable, measurable, and controllable. Common representations include text-based domain-specific languages (DSLs) or graphical notations such as the Business Process Modelling Notation (BPMN). The specification itself can be seen as a Software Product Line (SPL), building upon concepts such as tasks, UI forms, fields and actions. Domain experts provide requirements for processes but often lack the technical programming skills to formalize them in a process specification language. We present an interactive SPL application prototype that allows domain experts to model simple processes in natural language. Our framework for the reliable generation of formal specifications with Large Language Models (LLMs) supports the machine-translation from natural language to a JSON-based process DSL. In this case study, five domain experts were asked to model any process of their choice through natural-language interactions. As a result, the user interface corresponding to the process DSL was shown as immediate feedback. We documented their perceived translation quality and interviewed them on their impressions of this methodology. An average user-assessed performance rating of 68\% was achieved. Even though the modelling strategies differed greatly between individuals, the tool was able to adequately capture the majority of instructions, leaving an overall positive impression on the participants. More context awareness and additional conventional interaction elements were the main aspects found to be improved for a productive implementation.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {170–178},
numpages = {9},
keywords = {Domain-specific Languages, Generative Artificial Intelligence, Large Language Models, Process Management, Process Modelling, Reliable Code Generation},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@article{10.1145/3687922,
author = {Ganeshan, Aditya and Huang, Ryan and Xu, Xianghao and Jones, R. Kenny and Ritchie, Daniel},
title = {ParSEL: Parameterized Shape Editing with Language},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3687922},
doi = {10.1145/3687922},
abstract = {The ability to edit 3D assets with natural language presents a compelling paradigm to aid in the democratization of 3D content creation. However, while natural language is often effective at communicating general intent, it is poorly suited for specifying exact manipulation. To address this gap, we introduce ParSEL, a system that enables controllable editing of high-quality 3D assets with natural language. Given a segmented 3D mesh and an editing request, ParSEL produces a parameterized editing program. Adjusting these parameters allows users to explore shape variations with exact control over the magnitude of the edits. To infer editing programs which align with an input edit request, we leverage the abilities of large-language models (LLMs). However, we find that although LLMs excel at identifying the initial edit operations, they often fail to infer complete editing programs, resulting in outputs that violate shape semantics. To overcome this issue, we introduce Analytical Edit Propagation (AEP), an algorithm which extends a seed edit with additional operations until a complete editing program has been formed. Unlike prior methods, AEP searches for analytical editing operations compatible with a range of possible user edits through the integration of computer algebra systems for geometric analysis. Experimentally, we demonstrate ParSEL's effectiveness in enabling controllable editing of 3D objects through natural language requests over alternative system designs.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {197},
numpages = {14},
keywords = {shape editing, parametric editing, large language models, computer algebra systems, neuro-symbolic methods, program synthesis}
}

@article{10.1145/3728908,
author = {Lin, Li and Zhu, Qinglin and Chen, Hongqiao and Wang, Zhuangda and Wu, Rongxin and Xie, Xiaoheng},
title = {QTRAN: Extending Metamorphic-Oracle Based Logical Bug Detection Techniques for Multiple-DBMS Dialect Support},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728908},
doi = {10.1145/3728908},
abstract = {Metamorphic testing is a widely used method to detect logical bugs in Database Management Systems (DBMSs), referred to herein as MOLT (Metamorphic-Oracle based Logical Bug Detection Technique). This technique involves constructing SQL statement pairs, including original and mutated queries, and assessing whether the execution results conform to predefined metamorphic relations to detect logical bugs. However, current MOLTs rely heavily on specific DBMS grammar to generate valid SQL statement pairs, which makes it challenging to adapt these techniques to various DBMSs with different grammatical structures. As a result, only a few popular DBMSs, such as PostgreSQL, MySQL, and MariaDB, are supported by existing MOLTs, with extensive manual effort required to expand to other DBMSs. Given that many DBMSs remain inadequately tested, there is a pressing need for a method that enables effortless extension of MOLTs across diverse DBMSs.    In this paper, we propose QTRAN, a novel LLM-powered approach that automatically extends existing MOLTs to various DBMSs. Our key insight is to translate SQL statement pairs to target DBMSs for metamorphic testing from existing MOLTs using LLMs. To address the challenges of LLMs’ limited understanding of dialect differences and metamorphic mechanisms, we propose a two-phase approach comprising the transfer and mutation phases. QTRAN tackles these challenges by drawing inspiration from the developer’s process of creating a MOLT, which includes understanding the grammar of the target DBMS to generate original queries and employing a mutator for customized mutations. The transfer phase is designed to identify potential dialects and leverage information from SQL documents to enhance query retrieval, enabling LLMs to translate original queries across different DBMSs accurately. During the mutation phase, we gather SQL statement pairs from existing MOLTs to fine-tune the pretrained model, tailoring it specifically for mutation tasks. Then we employ the customized LLM to mutate the translated original queries, preserving the defined relationships necessary for metamorphic testing.    We implement our approach as a tool and apply it to extend four state-of-the-art MOLTs for eight DBMSs: MySQL, MariaDB, TiDB, PostgreSQL, SQLite, MonetDB, DuckDB, and ClickHouse. The evaluation results show that over 99\% of the SQL statement pairs transferred by QTRAN satisfy the metamorphic relations required for testing. Furthermore, we have detected 24 logical bugs among these DBMSs, with 16 confirmed as unique and previously unknown bugs. We believe that the generality of QTRAN can significantly enhance the reliability of DBMSs.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA033},
numpages = {22},
keywords = {Database Testing, Metamorphic Testing, SQL Dialects}
}

@article{10.1145/3710904,
author = {Wu, Shiwei and Wang, Mingxiang and Shi, Chuhan and Peng, Zhenhui},
title = {ComViewer: An Interactive Visual Tool to Help Viewers Seek Social Support in Online Mental Health Communities},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3710904},
doi = {10.1145/3710904},
abstract = {Online mental health communities (OMHCs) offer rich posts and comments for viewers, who do not directly participate in the communications, to seek social support from others' experience. However, viewers could face challenges in finding helpful posts and comments and digesting the content to get needed support, as revealed in our formative study (N=10). In this work, we present an interactive visual tool named ComViewer to help viewers seek social support in OMHCs. With ComViewer, viewers can filter posts of different topics and find supportive comments via a zoomable circle packing visual component that adapts to searched keywords. Powered by LLM, ComViewer supports an interactive sensemaking process by enabling viewers to interactively highlight, summarize, and question any community content. A within-subjects study (N=20) demonstrates ComViewer's strengths in providing viewers with a more simplified, more fruitful, and more engaging support-seeking experience compared to a baseline OMHC interface without ComViewer. We further discuss design implications for facilitating information-seeking and sense making in online mental health communities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW006},
numpages = {31},
keywords = {information seeking \&amp; search, mental health, online community, social support, visualization}
}

@inproceedings{10.5555/3586210.3586431,
author = {Shrestha, Anish and Mielke, Kyle and Nguyen, Tuong Anh and Giabbanelli, Philippe J.},
title = {Automatically Explaining a Model: Using Deep Neural Networks to Generate Text from Causal Maps},
year = {2023},
publisher = {IEEE Press},
abstract = {Simulation models start as conceptual models, which list relevant factors and their relationships. In complex socio-environmental problems, these conceptual models are routinely created with participants, via a 'participatory modeling' approach. Transparency is a tenet of participatory modeling: participants should easily provide their input into the model-building process and see how that input is utilized. Although several elicitation methods are transparent, the resulting conceptual model can become too large and difficult to interpret. Usability studies have shown that participants struggle to interact with such large conceptual models, even if they contributed to creating parts of it. In this paper, we propose to automatically transform these large conceptual models into a more familiar format for participants: textual reports. We designed and implemented a process combining Natural Language Generation (via the deep learning GPT-3 model) and Network Science. Two case studies demonstrate that our prototype generates sentences that perform satisfactorily on several metrics.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2629–2640},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.5555/3643142.3643417,
author = {Taylor, Simon J. E. and Macal, Charles M. and Matta, Andrea and Rabe, Markus and Sanchez, Susan M. and Shao, Guodong},
title = {Enhancing Digital Twins with Advances in Simulation and Artificial Intelligence: Opportunities and Challenges},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {Simulations are used to investigate physical systems. A digital twin goes beyond this by connecting a simulation with the physical system with the purpose of analyzing and controlling that system in real-time. In the past 5 years there has been a substantial increase in research into Simulation and Artificial Intelligence (AI). The combination of Simulation with AI presents many possible innovations. Similarly, combining AI with Simulation presents further possibilities including approaches to developing trustworthy and explainable AI methods, solutions to problems arising from sparce or no data and better methods for time series analysis. Given the progress that has been made in Digital Twins and Simulation and AI, what opportunities are there from combining these two exciting research areas? What challenges need to be overcome to achieve these? This article discusses these from the perspectives of six leading members of the Modeling \&amp; Simulation community.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3296–3310},
numpages = {15},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@article{10.1145/3587691,
author = {Hirzel, Martin},
title = {Low-Code Programming Models},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3587691},
doi = {10.1145/3587691},
abstract = {Low-code has the potential to empower more people to automate tasks by creating computer programs.},
journal = {Commun. ACM},
month = sep,
pages = {76–85},
numpages = {10}
}

@inproceedings{10.1145/3486609.3487199,
author = {Atouani, Abdallah and Kirchhof, J\"{o}rg Christian and Kusmenko, Evgeny and Rumpe, Bernhard},
title = {Artifact and reference models for generative machine learning frameworks and build systems},
year = {2021},
isbn = {9781450391122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486609.3487199},
doi = {10.1145/3486609.3487199},
abstract = {Machine learning is a discipline which has become ubiquitous in the last few years. While the research of machine learning algorithms is very active and continues to reveal astonishing possibilities on a regular basis, the wide usage of these algorithms is shifting the research focus to the integration, maintenance, and evolution of AI-driven systems. Although there is a variety of machine learning frameworks on the market, there is little support for process automation and DevOps in machine learning-driven projects. In this paper, we discuss how metamodels can support the development of deep learning frameworks and help deal with the steadily increasing variety of learning algorithms. In particular, we present a deep learning-oriented artifact model which serves as a foundation for build automation and data management in iterative, machine learning-driven development processes. Furthermore, we show how schema and reference models can be used to structure and maintain a versatile deep learning framework. Feasibility is demonstrated on several state-of-the-art examples from the domains of image and natural language processing as well as decision making and autonomous driving.},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {55–68},
numpages = {14},
keywords = {artifact models, artificial intelligence, build systems, compiler, machine learning, metamodeling, reference models, training},
location = {Chicago, IL, USA},
series = {GPCE 2021}
}

@inproceedings{10.1145/3691620.3695550,
author = {Xia, Jingtao and Liu, Junrui and Brown, Nicholas and Chen, Yanju and Feng, Yu},
title = {Refinement Types for Visualization},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695550},
doi = {10.1145/3691620.3695550},
abstract = {Visualizations have become crucial in the contemporary data-driven world as they aid in exploring, verifying, and sharing insights obtained from data. In this paper, we propose a new paradigm of visualization synthesis based on refinement types. Besides input-output examples, users can optionally use refinement-type annotations to constrain the range of valid values in the example visualization or to express complex interactions between different visual components. Our system's outputs include both data transformation and visualization programs that are consistent with refinement-type specifications. To mitigate the scalability challenge during the synthesis process, we introduce a new visualization synthesis algorithm that uses lightweight bidirectional type checking to prune the search space. As we demonstrate experimentally, this new synthesis algorithm results in significant speed-up compared to prior work.We have implemented the proposed approach in a tool called Calico and evaluated it on 40 visualization tasks collected from online forums and tutorials. Our experiments show that Calico can solve 98\% of these benchmarks and, among those benchmarks that can be solved, the desired visualization is among the top-1 output generated by Calico. Furthermore, Calico takes an average of 1.56 seconds to generate the visualization, which is 50 times faster than Viser, a state-of-the-art synthesizer for data visualization.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1871–1881},
numpages = {11},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00008,
author = {Chaaben, Meriem Ben and Burgue\~{n}o, Lola and Sahraoui, Houari},
title = {Towards Using Few-Shot Prompt Learning for Automating Model Completion},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00008},
doi = {10.1109/ICSE-NIER58687.2023.00008},
abstract = {We propose a simple yet a novel approach to improve completion in domain modeling activities. Our approach exploits the power of large language models by using few-shot prompt learning without the need to train or fine-tune those models with large datasets that are scarce in this field. We implemented our approach and tested it on the completion of static and dynamic domain diagrams. Our initial evaluation shows that such an approach is effective and can be integrated in different ways during the modeling activities.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {7–12},
numpages = {6},
keywords = {model completion, domain modeling, prompt learning, few-shot learning, language models},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1145/3608298.3608324,
author = {Schl\"{o}r, Daniel and Pfister, Jan and Hotho, Andreas},
title = {Optimizing Medical Service Request Processes through Language Modeling and Semantic Search},
year = {2023},
isbn = {9798400700712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3608298.3608324},
doi = {10.1145/3608298.3608324},
abstract = {Medical service requests are a crucial part of the workflow in hospitals and healthcare organizations. However, the process of requesting medical services can be time consuming and can require physicians and medical personnel to navigate complex interfaces and enter detailed information about the requested service. In this paper, we propose a system that uses machine learning techniques such as large language models and semantic search to optimize the process of requesting medical services. Our approach enables physicians to request medical services using natural language rather than navigating complex interfaces, allowing for more efficient and flexible interactions with hospital information systems. We evaluate our approach on real-world data and discuss the implications of our work for the future of digital health care. Our results suggest that our approach has the potential to streamline the process of requesting medical services and reduce the time and manual effort required in the daily hospital routine.},
booktitle = {Proceedings of the 2023 7th International Conference on Medical and Health Informatics},
pages = {136–141},
numpages = {6},
keywords = {language modeling, medical service optimization, semantic search},
location = {Kyoto, Japan},
series = {ICMHI '23}
}

@article{10.1145/3744660,
author = {Rodrigues, Tiago and Teixeira Lopes, Carla},
title = {Harnessing Large Language Models for Clinical Information Extraction: A Systematic Literature Review},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744660},
doi = {10.1145/3744660},
abstract = {Electronic Health Records store extensive patient health data, playing a crucial role in healthcare management. Extracting information from these text-heavy records is difficult due to their domain-specific vocabulary, which challenges applying general-domain techniques. Recent advancements in Large Language Models (LLMs) and an increasing interest in the field have sparked considerable progress in solving Clinical Information Extraction (IE) tasks. We review these applications in Clinical IE, highlighting the most common tasks, most successful methods, and most used datasets and evaluation criteria. Examining 85 studies, we synthesize and organize the current research trends, highlighting common points between papers. The presence of LLMs can be felt in the most common tasks, with novel approaches being attempted and showing promising results. However, breakthroughs are still necessary in designing reliable end-to-end systems that can perform all the Clinical IE tasks within a single system.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = jun,
keywords = {Information Extraction, Large Language Models, BERT, Clinical Notes, Electronic Health Records}
}

@article{10.1109/TCBB.2023.3236477,
author = {Liang, Tingting and Xia, Congying and Zhao, Ziqiang and Jiang, Yixuan and Yin, Yuyu and Yu, Philip S.},
title = {Transferring From Textual Entailment to Biomedical Named Entity Recognition},
year = {2023},
issue_date = {July-Aug. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2023.3236477},
doi = {10.1109/TCBB.2023.3236477},
abstract = {Biomedical Named Entity Recognition (BioNER) aims at identifying biomedical entities such as genes, proteins, diseases, and chemical compounds in the given textual data. However, due to the issues of ethics, privacy, and high specialization of biomedical data, BioNER suffers from the more severe problem of lacking in quality labeled data than the general domain especially for the token-level. Facing the extremely limited labeled biomedical data, this work studies the problem of gazetteer-based BioNER, which aims at building a BioNER system from scratch. It needs to identify the entities in the given sentences when we have zero token-level annotations for training. Previous works usually use sequential labeling models to solve the NER or BioNER task and obtain weakly labeled data from gazetteers when we don't have full annotations. However, these labeled data are quite noisy since we need the labels for each token and the entity coverage of the gazetteers is limited. Here we propose to formulate the BioNER task as a Textual Entailment problem and solve the task via Textual Entailment with Dynamic Contrastive learning (TEDC). TEDC not only alleviates the noisy labeling issue, but also transfers the knowledge from pre-trained textual entailment models. Additionally, the dynamic contrastive learning framework contrasts the entities and non-entities in the same sentence and improves the model's discrimination ability. Experiments on two real-world biomedical datasets show that TEDC can achieve state-of-the-art performance for gazetteer-based BioNER.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {2577–2586},
numpages = {10}
}

@article{10.1145/3660825,
author = {Wu, Yaoxuan and Humayun, Ahmad and Gulzar, Muhammad Ali and Kim, Miryung},
title = {Natural Symbolic Execution-Based Testing for Big Data Analytics},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660825},
doi = {10.1145/3660825},
abstract = {Symbolic execution is an automated test input generation technique that models individual program paths as logical constraints. However, the realism of concrete test inputs generated by SMT solvers often comes into question. Existing symbolic execution tools only seek arbitrary solutions for given path constraints. These constraints do not incorporate the naturalness of inputs that observe statistical distributions, range constraints, or preferred string constants. This results in unnatural-looking inputs that fail to emulate real-world data.                In this paper, we extend symbolic execution with consideration for incorporating naturalness. Our key insight is that users typically understand the semantics of program inputs, such as the distribution of height or possible values of zipcode, which can be leveraged to advance the ability of symbolic execution to produce natural test inputs. We instantiate this idea in NaturalSym, a symbolic execution-based test generation tool for data-intensive scalable computing (DISC) applications. NaturalSym generates natural-looking data that mimics real-world distributions by utilizing user-provided input semantics to drastically enhance the naturalness of inputs, while preserving strong bug-finding potential. On DISC applications and commercial big data test benchmarks, NaturalSym achieves a higher degree of realism —as evidenced by a perplexity score 35.1 points lower on median, and detects 1.29\texttimes{} injected faults compared to the state-of-the-art symbolic executor for DISC, BigTest. This is because BigTest draws inputs purely based on the satisfiability of path constraints constructed from branch predicates, while NaturalSym is able to draw natural concrete values based on user-specified semantics and prioritize using these values in input generation. Our empirical results demonstrate that NaturalSym finds injected faults 47.8\texttimes{} more than NaturalFuzz (a coverage-guided fuzzer) and 19.1\texttimes{} more than ChatGPT. Meanwhile, TestMiner (a mining-based approach) fails to detect any injected faults. NaturalSym is the first symbolic executor that combines the notion of input naturalness in symbolic path constraints during SMT-based input generation. We make our code available at https://github.com/UCLA-SEAL/NaturalSym.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {118},
numpages = {24},
keywords = {DISC Applications, Naturalness, Symbolic Execution}
}

@inproceedings{10.1145/3650212.3652112,
author = {Ma, Yunlong and Tian, Wentong and Gao, Xiang and Sun, Hailong and Li, Li},
title = {API Misuse Detection via Probabilistic Graphical Model},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652112},
doi = {10.1145/3650212.3652112},
abstract = {API misuses can cause a range of issues in software development, including program crashes, bugs, and vulnerabilities. Different approaches have been developed to automatically detect API misuses by checking the program against usage rules extracted from extensive codebase or API documents. However, these mined rules may not be precise or complete, leading to high false positive/negative rates. In this paper, we propose a novel solution to this problem by representing the mined API usage rules as a probabilistic graphical model, where each rule's probability value represents its trustworthiness of being correct.   Our approach automatically constructs probabilistic usage rules by mining codebase and documents, and aggregating knowledge from different sources.   Here, the usage rules obtained from the codebase initialize the probabilistic model, while the knowledge from the documents serves as a supplement for adjusting and complementing the probabilities accordingly.   We evaluate our approach on the MuBench benchmark.   Experimental results show that our approach achieves 42.0\% precision and 54.5\% recall, significantly outperforming state-of-the-art approaches.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {88–99},
numpages = {12},
keywords = {API misuse detection, Document Mining, Mining Software Repository, Probabilistic Graphical Model},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3579027.3608973,
author = {Galindo, Jos\'{e} A. and Dominguez, Antonio J. and White, Jules and Benavides, David},
title = {Large Language Models to generate meaningful feature model instances},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608973},
doi = {10.1145/3579027.3608973},
abstract = {Feature models are the "de facto" standard for representing variability in software-intensive systems. Automated analysis of feature models is the computer-aided extraction of information of feature models and is used in testing, maintenance, configuration, and derivation, among other tasks. Testing the analyses of feature models often requires relying on a large number of models that are as realistic as possible. There exist different proposals to generate synthetic feature models using random techniques or metamorphic relations; however, the existing methods do not take into account the semantics of the concepts of the domain that are being represented and the interrelations between them, leading to less realistic feature models. In this paper, we propose a novel approach that uses Large Language Models (LLMs), such as Codex or GPT-3, to generate realistic feature models that preserve semantic coherence while maintaining syntactic validity. The approach automatically generates instances of feature models from a given domain. Concretely, two language models were used, first OpenAI's Codex to generate new instances of feature models using the Universal Variability Language (UVL) syntax and then Cohere's semantic analysis to verify if the newly introduced concepts are from the same domain. This approach enabled the generation of 90\% of valid instances according to the UVL syntax. In addition, the valid models score well on model complexity metrics, and the generated features mirror the domain of the original UVL instance used as prompts. With this work, we envision a new thread of research where variability is generated and analyzed using LLMs. This opens the door for a new generation of techniques and tools for variability management.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {15–26},
numpages = {12},
keywords = {deep learning, large language models, synthetic models, universal variability language},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3707292.3707358,
author = {Liu, Fei and Kang, Zejun and Han, Xing},
title = {Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama ModelsOptimizing RAG Techniques Based on Locally Deployed Ollama ModelsA Case Study with Locally Deployed Ollama Models},
year = {2025},
isbn = {9798400707308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707292.3707358},
doi = {10.1145/3707292.3707358},
abstract = {With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using locally deployed Ollama models.Based on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding pipelines and an agent supporting self-RAG based on LangGraph best practices.To evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive industry dataset.Our optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing information processing and intelligent production in the automotive industry.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Intelligent Information Processing},
pages = {152–159},
numpages = {8},
keywords = {Automotive Industry, Langchain, Ollama, PDF Processing, RAG, self-rag},
location = {
},
series = {AIIIP '24}
}

@inproceedings{10.1145/3736733.3736738,
author = {Ganti, Manasi and Zhang, Enhao and Balazinska, Magdalena},
title = {Bootstrapping Compositional Video Query Synthesis with Natural Language and Previous Queries from Users},
year = {2025},
isbn = {9798400719592},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3736733.3736738},
doi = {10.1145/3736733.3736738},
abstract = {With the emerging ubiquity of video data across diverse applications, the accessibility of video analytics is essential. To address this goal, some state-of-the-art systems synthesize declarative queries over video databases using example video fragments provided by the user. However, finding examples of what a user is looking for can still be tedious. This work presents POLY-VOCAL, a new system that eases this burden. POLY-VOCAL uses multiple forms of user input to bootstrap the synthesis of a new query, including textual descriptions of the user's search and previously synthesized queries. Our empirical evaluation demonstrates that POLY-VOCAL significantly improves accuracy and accelerates query convergence compared with query synthesis from only user-labeled examples, while lowering the effort required from users.},
booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
articleno = {8},
numpages = {7},
location = {Intercontinental Berlin, Berlin, Germany},
series = {HILDA '25}
}

@inproceedings{10.1145/3639477.3639732,
author = {Song, Yewei and Ezzini, Saad and Tang, Xunzhu and Lothritz, Cedric and Klein, Jacques and Bissyande, Tegawende and Boytsov, Andrey and Ble, Ulrick and Goujon, Anne},
title = {Enhancing Text-to-SQL Translation for Financial System Design},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639732},
doi = {10.1145/3639477.3639732},
abstract = {Text-to-SQL, the task of translating natural language questions into SQL queries, is part of various business processes. Its automation, which is an emerging challenge, will empower software practitioners to seamlessly interact with relational databases using natural language, thereby bridging the gap between business needs and software capabilities.In this paper, we consider Large Language Models (LLMs), which have achieved state of the art for various NLP tasks. Specifically, we benchmark Text-to-SQL performance, the evaluation methodologies, as well as input optimization (e.g., prompting). In light of the empirical observations that we have made, we propose two novel metrics that were designed to adequately measure the similarity between SQL queries.Overall, we share with the community various findings, notably on how to select the right LLM on Text-to-SQL tasks. We further demonstrate that a tree-based edit distance constitutes a reliable metric for assessing the similarity between generated SQL queries and the oracle for benchmarking Text2SQL approaches. This metric is important as it relieves researchers from the need to perform computationally expensive experiments such as executing generated queries as done in prior works. Our work implements financial domain use cases and, therefore contributes to the advancement of Text2SQL systems and their practical adoption in this domain.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {252–262},
numpages = {11},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3677779.3677791,
author = {Xiao, Weizhen},
title = {A Comparative Review of Advanced Techniques for Financial Sentiment Analysis},
year = {2024},
isbn = {9798400709760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677779.3677791},
doi = {10.1145/3677779.3677791},
abstract = {Financial sentiment analysis, the task of discerning market sentiment from financial texts, plays a crucial role in investment decisions, risk assessment, and understanding economic trends. Traditional sentiment analysis techniques have often faced limitations in handling the complexities and nuances of financial language. The advent of large language models (LLMs) has brought a paradigm shift in this field. With their remarkable ability to process and understand natural language, LLMs are enabling new approaches that increase the accuracy and sophistication of financial sentiment analysis. This paper provides a comparative overview of cutting-edge LLM-based techniques for financial sentiment analysis. We introduce a six-pronged classification framework covering data types, sentiment granularity, model architectures, training approaches, methodological focus, and evaluation metrics. This framework aims to provide a structured perspective for understanding recent research trends. Our analysis reveals several key developments in the field. We discuss the challenges and opportunities associated with advanced techniques, like Instruction-tuning approaches and Retrieval-augmented methods. While LLMs offer clear advantages, ensuring data quality, mitigating bias, enhancing model explainability, and scaling these models to real-world applications remain active research areas. This review offers investors and financial researchers a comprehensive guide to the evolving landscape of financial sentiment analysis, facilitating well-informed choices for different use cases and laying the groundwork for future research.},
booktitle = {Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning},
pages = {76–80},
numpages = {5},
location = {Xi'an, China},
series = {CMNM '24}
}

@inproceedings{10.1145/3580252.3586978,
author = {Rahman, M Arif and Preum, Sarah Masud and Williams, Ronald D. and Alemzadeh, Homa and Stankovic, John},
title = {EMS-BERT: A Pre-Trained Language Representation Model for the Emergency Medical Services (EMS) Domain},
year = {2024},
isbn = {9798400701023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580252.3586978},
doi = {10.1145/3580252.3586978},
abstract = {Emergency Medical Services (EMS) is an important domain of healthcare. First responders save millions of lives per year. Machine learning and sensing technologies are actively being developed to support first responders in their EMS activities. However, there are significant challenges to overcome in developing these new solutions. One of the main challenges is the limitations of existing methods for EMS text mining, and developing a highly accurate language model for the EMS domain. Several important Bidirectional Encoder Representations from Transformer (BERT) models for medical domains, i.e., BioBERT and ClinicalBERT, have significantly influenced biomedical text mining tasks. But extracting information from the EMS domain is a separate challenge due to the uniqueness of the EMS domain, and the significant scarcity of a high-quality EMS corpus. In this research, we propose EMS-BERT - a BERT model specifically developed for EMS text-mining tasks. For data augmentation on our small, classified EMS corpus which consists of nearly 2.4M words, we use a simultaneous pre-training method for transfer-learning relevant information from medical, bio-medical, and clinical domains; and train a high-performance BERT model. Our thorough evaluation shows at least 2\% to as much as 11\% improvement of F-1 scores for EMS-BERT on different classification tasks, i.e., entity recognition, relation extraction, and inferring missing information when compared both with existing state-of-the-art clinical entity recognition tools, and with various medical BERT models.},
booktitle = {Proceedings of the 8th ACM/IEEE International Conference on Connected Health: Applications, Systems and Engineering Technologies},
pages = {34–43},
numpages = {10},
keywords = {emergency medical services (EMS) data processing and analysis, EMS entity recognition, medicine and health, language model},
location = {Orlando, FL, USA},
series = {CHASE '23}
}

@inproceedings{10.1145/3665939.3665962,
author = {Beasley, Cole and Abouzied, Azza},
title = {Pipe(line) Dreams: Fully Automated End-to-End Analysis and Visualization},
year = {2024},
isbn = {9798400706936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665939.3665962},
doi = {10.1145/3665939.3665962},
abstract = {We exploit large language models (LLMs) to automate the end-to-end process of descriptive analytics and visualization. A user simply declares who they are and provides their data set. Our tool LLM4Vis sets analysis goals or metrics, generates code to process and analyze the data, visualizes the results and interprets the visualization to summarize key takeaways for our user. We examine the power of LLMs in democratizing data science for the non-technical user and in handling rich, multimodal data sets. We also explore LLM4Vis's limitations, opportunities for human-in-the-loop interventions, and challenges to measuring and improving the robustness and the utility of LLM-generated end-to-end data analysis pipelines.},
booktitle = {Proceedings of the 2024 Workshop on Human-In-the-Loop Data Analytics},
pages = {1–7},
numpages = {7},
location = {Santiago, AA, Chile},
series = {HILDA  24}
}

@inproceedings{10.1145/3691620.3695015,
author = {Lahiri, Sumit and Kalita, Pankaj Kumar and Chittora, Akshay Kumar and Vankudre, Varun and Roy, Subhajit},
title = {Program Synthesis Meets Visual What-Comes-Next Puzzles},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695015},
doi = {10.1145/3691620.3695015},
abstract = {What-Comes-Next (WCN) puzzles challenge us to identify the next figure that "logically follows" a provided sequence of figures. WCN puzzles are a favorite of interviewers and examiners---there is hardly any aptitude test that misses WCN puzzles. In this work, we propose to automatically synthesize WCN puzzles. The key insight to our methodology is that generation of WCN problems can be posed as a program synthesis problem. We design a small yet expressive language, PuzzlerLang, to capture solutions to WCN puzzles. PuzzlerLang is expressive enough to explain almost all human generated WCN puzzles that we collected, and yet, small enough to allow synthesis in a reasonable time. To ensure that the generated puzzles are appealing to humans, we infer a machine learning model to approximate the appeal factor of given WCN puzzle to humans. We use this model within our puzzle synthesizer as an optimization function to generate highly appealing and correct-by-construction WCN puzzles. We implemented our ideas in a tool, PuzzleGen; we found that PuzzleGen is fast, clocking an average time of about 3.4s per puzzle. Further, statistical tests over the responses from a user-study supported that the PuzzleGen generated puzzles were indistinguishable from puzzles created by humans.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {418–429},
numpages = {12},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3701716.3715247,
author = {Tan, Xiaoyu and Li, Bin and Qiu, Xihe and Qu, Chao and Chu, Wei and Xu, Yinghui and Qi, Yuan},
title = {Meta-Agent-Workflow: Streamlining Tool Usage in LLMs through Workflow Construction, Retrieval, and Refinement},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715247},
doi = {10.1145/3701716.3715247},
abstract = {Large language models (LLMs) have recently shown significant advancements and are increasingly used as key components in automated agents for various web-based tasks. Typically, this agentization is achieved by carefully prompting LLMs to guide their behavior in using tools for specific tasks. However, this approach can be limited by the complexity of tasks and the inherent capabilities of LLMs. To enhance task-specific performance, a pre-defined workflow approach can be employed, reducing repetitive and error-prone planning for particular tasks. This workflow-driven process is especially well-suited for industrial applications, where task-specific agents can be easily configured using visual interfaces supported by various open-source platforms. In this paper, we introduce a novel framework called Meta-Agent-Workflowto create, retrieve, and refine agent workflows. Experiments on ToolBench demonstrate that our framework effectively transforms LLM tool-reasoning processes into task-specific workflows, retrieves workflows for different tasks based on various queries, and updates them based on execution feedback. We also open-source our code and follow the workflow architecture of an open-source agent platform (e.g., Dify) to facilitate further industrial and community use. The Meta-Agent-Workflow will be open-sourced in https://github.com/testlbin/meta_agent_workflows.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {458–467},
numpages = {10},
keywords = {agent, large language models, meta agent workflow, workflow},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3649593,
author = {Chen, Zhifei and Chen, Lin and Yang, Yibiao and Feng, Qiong and Li, Xuansong and Song, Wei},
title = {Risky Dynamic Typing-related Practices in Python: An Empirical Study},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3649593},
doi = {10.1145/3649593},
abstract = {Python’s dynamic typing nature provides developers with powerful programming abstractions. However, many type-related bugs are accumulated in code bases of Python due to the misuse of dynamic typing. The goal of this article is to aid in the understanding of developers’ high-risk practices toward dynamic typing and the early detection of type-related bugs. We first formulate the rules of six types of risky dynamic typing-related practices (type smells for short) in Python. We then develop a rule-based tool named RUPOR, which builds an accurate type base to detect type smells. Our evaluation shows that RUPOR outperforms the existing type smell detection techniques (including the Large Language Models–based approaches, Mypy, and PYDYPE) on a benchmark of 900 Python methods. Based on RUPOR, we conduct an empirical study on 25 real-world projects. We find that type smells are significantly related to the occurrence of post-release faults. The fault-proneness prediction model built with type smell features slightly outperforms the model built without them. We also summarize the common patterns, including inserting type check to fix type smell bugs. These findings provide valuable insights for preventing and fixing type-related bugs in the programs written in dynamic-typed languages.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {140},
numpages = {35},
keywords = {Dynamic typing, Python, empirical study, bug fixing}
}

@inproceedings{10.1145/3639476.3639776,
author = {Rukmono, Satrio Adi and Ochoa, Lina and Chaudron, Michel},
title = {Deductive Software Architecture Recovery via Chain-of-thought Prompting},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639776},
doi = {10.1145/3639476.3639776},
abstract = {As software evolves, software architecture recovery techniques can help for effective maintenance. We envision a deductive software architecture recovery approach supported by Large Language Models (LLMs). Unlike existing inductive (bottom-up) recovery techniques, which reconstruct architecture by considering the properties observed at implementation level, our top-down approach starts with architectural properties and seeks their manifestations in the implementation. It employs a known Reference Architecture (RA) and involves two phases: RA definition and code units classification. A proof-of-concept with GPT-4 emulates deductive reasoning via chain-of-thought prompting. It demonstrates the deductive SAR approach, applying it to the Android application K-9 Mail and achieving a 70\% accuracy in classifying 54 classes and 184 methods. The future plans focus on evaluating and refining the approach through ground-truth assessments, deeper exploration of reference architectures, and advancing toward automated human-like software architecture explanations. We highlight the potential for LLMs in achieving more comprehensive and explainable software architecture recovery.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {92–96},
numpages = {5},
keywords = {software architecture, software architecture recovery, deductive SAR, chain-of-thought prompting},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

