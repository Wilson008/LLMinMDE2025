@inproceedings{10.1145/3640310.3674079,
author = {Sultan, Bastien and Apvrille, Ludovic},
title = {AI-Driven Consistency of SysML Diagrams},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674079},
doi = {10.1145/3640310.3674079},
abstract = {Graphical modeling languages, expected to simplify systems analysis and design, present a challenge in maintaining consistency across their varied views. Traditional rule-based methods for ensuring consistency in languages like UML often fall short in addressing complex semantic dimensions. Moreover, the integration of Large Language Models (LLMs) into Model Driven Engineering (MDE) introduces additional consistency challenges, as LLM's limited output contexts requires the integration of responses. This paper presents a new framework that automates the detection and correction of inconsistencies across different views, leveraging formally defined rules and incorporating OpenAI's GPT, as implemented in TTool. Focusing on the consistency between use case and block diagrams, the framework is evaluated through its application to three case studies, highlighting its potential to significantly enhance consistency management in graphical modeling.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {149–159},
numpages = {11},
location = {Linz, Austria},
series = {MODELS '24}
}

@article{10.1145/3716134,
author = {Grunde-McLaughlin, Madeleine and Lam, Michelle S. and Krishna, Ranjay and Weld, Daniel S. and Heer, Jeffrey},
title = {Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/3716134},
doi = {10.1145/3716134},
abstract = {LLM chains enable complex tasks by decomposing work into a sequence of subtasks. Similarly, the more established techniques of crowdsourcing workflows decompose complex tasks into smaller tasks for human crowdworkers. Chains address LLM errors analogously to the way crowdsourcing workflows address human error. To characterize opportunities for LLM chaining, we survey 107 papers across the crowdsourcing and chaining literature to construct a design space for chain development. The design space covers a designer’s objectives and the tactics used to build workflows. We then surface strategies that mediate how workflows use tactics to achieve objectives. To explore how techniques from crowdsourcing may apply to chaining, we adapt crowdsourcing workflows to implement LLM chains across three case studies: creating a taxonomy, shortening text, and writing a short story. From the design space and our case studies, we identify takeaways for effective chain design and raise implications for future research and development.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
articleno = {27},
numpages = {57},
keywords = {Crowdsourcing workflows, large language model chains, design space, case studies}
}

@inproceedings{10.1145/3652620.3687809,
author = {G\"{o}bel, Susanne and L\"{a}mmel, Ralf},
title = {Model-Based Trust Analysis of LLM Conversations},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687809},
doi = {10.1145/3652620.3687809},
abstract = {LLM-based chatbots are routinely advertised as supporting the collaboration of humans and AI. We study LLM conversations from a knowledge elicitation perspective with the objective of being able to understand and assess the human's trust in knowledge elicited from the LLM and complementary sources. Our approach is supported by the DSML KEML, the Knowledge Elicitation Modeling Language, subject to abstract and visual syntax as well as a model transformation-based model semantics for trust analysis. Conversations are modeled by a combination of sequence diagrams and enhanced argumentation graphs --- the latter for the purpose of relating information pieces (facts and instructions) that are extracted from messages. The analysis of the corresponding models entails trust scores for gathered information (i.e., elicited knowledge).},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {602–610},
numpages = {9},
keywords = {MDE for AI, knowledge representation models, model-based analysis of LLMS, dsmls for AI usage},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3687800,
author = {Aslan O\u{g}uz, Evin and Kuester, Jochen Malte},
title = {A Comparative Analysis of ChatGPT-Generated and Human-Written Use Case Descriptions},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687800},
doi = {10.1145/3652620.3687800},
abstract = {The development of comprehensive use case descriptions is a critical task in software engineering, providing essential insights for requirement analysis and system design. The advent of advanced natural language processing models, such as ChatGPT, has sparked interest in their potential to automate tasks traditionally performed by humans, including the generation of use case descriptions in software engineering. Understanding the capabilities and limitations of ChatGPT in generating use case descriptions is crucial for software engineers. Without a clear understanding of its performance, practitioners may either overestimate its utility, leading to reliance on suboptimal drafts, or underestimate its capabilities, missing opportunities to streamline the drafting process. This paper addresses how well ChatGPT performs in generating use case descriptions, evaluating their quality compared to human-written descriptions. To do so, we employ a structured approach using established quality guidelines and the concept of "bad smells" for use case descriptions. Our study presents the first attempt to bridge the knowledge gap by offering a comparative analysis of ChatGPT-generated and human-written use case descriptions. By providing an approach to objectively assess ChatGPT's performance, we highlight its potential and limitations, offering software engineers insights to effectively integrate AI tools into their workflows.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {533–540},
numpages = {8},
keywords = {use case description, ChatGPT, requirements engineering, quality},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3623476.3623524,
author = {Ait, Adem and C\'{a}novas Izquierdo, Javier Luis and Cabot, Jordi},
title = {A Tool for the Definition and Deployment of Platform-Independent Bots on Open Source Projects},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623476.3623524},
doi = {10.1145/3623476.3623524},
abstract = {The development of Open Source Software (OSS) projects is a collaborative process that heavily relies on active contributions by passionate developers. Creating, retaining and nurturing an active community of developers is a challenging task; and finding the appropriate expertise to drive the development process is not always easy. To alleviate this situation, many OSS projects try to use bots to automate some development tasks, thus helping community developers to cope with the daily workload of their projects. However, the techniques and support for developing bots is specific to the code hosting platform where the project is being developed (e.g., GitHub or GitLab). Furthermore, there is no support for orchestrating bots deployed in different platforms nor for building bots that go beyond pure development activities. In this paper, we propose a tool to define and deploy bots for OSS projects, which besides automation tasks they offer a more social facet, improving community interactions. The tool includes a Domain-Specific Language (DSL) which allows defining bots that can be deployed on top of several platforms and that can be triggered by different events (e.g., creation of a new issue or a pull request). We describe the design and the implementation of the tool, and illustrate its use with examples.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {214–219},
numpages = {6},
keywords = {Bot, Domain-Specific Language, Open Source},
location = {Cascais, Portugal},
series = {SLE 2023}
}

@inproceedings{10.1145/3617553.3617887,
author = {Fulcini, Tommaso and Torchiano, Marco},
title = {Is ChatGPT Capable of Crafting Gamification Strategies for Software Engineering Tasks?},
year = {2023},
isbn = {9798400703737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617553.3617887},
doi = {10.1145/3617553.3617887},
abstract = {Gamification has gained significant attention in the last decade for its potential to enhance engagement and motivation in various domains. During the last year ChatGPT, a state-of-the-art large language model has received even more attention both in the field of scientific research and in common use by individuals or companies.   In this study, we investigate the possibility of adopting ChatGPT as a tool for designing gamification platforms in the Software Engineering domain. Leveraging the capabilities of ChatGPT, we assess how good is it at generating effective suggestions and ideas for designers or developers.   To evaluate ChatGPT's potential as a gamification platform creator we narrowed the context to one particular Software Engineering activity, asking for possible aspects of the activity to be gamified. Each proposed aspect was subsequently unraveled by ChatGPT both asking in a shared and separate context, first following the conversational nature of the model, then applying a validated design framework. The study assesses ChatGPT's ability to select and integrate game elements to build a thriving gamification environment by framing the design of the platform to a state-of-the-art conceptual framework. To evaluate the goodness of the design choices made we relied both on the Octalysis framework and on personal experience.   The findings of the papers show that ChatGPT can only create simple playful experiences not very effective. Although, by instructing the model with more specific desired mechanics and dynamics, it is possible to guide it toward the application of the ideas suggested. We argue that ChatGPT is not capable of building a gamified environment on its own, but it could still be used to build the foundation of a gamification platform as long as the designers refine and rough out the advice gained from a user-centered solution.},
booktitle = {Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation},
pages = {22–28},
numpages = {7},
keywords = {Artificial Intelligence, Gamification, Large Language Model, Software Engineering, Software Lifecycle},
location = {San Francisco, CA, USA},
series = {Gamify 2023}
}

@article{10.1145/3622822,
author = {Cao, Huanqi and Tang, Shizhi and Zhu, Qianchao and Yu, Bowen and Chen, Wenguang},
title = {Mat2Stencil: A Modular Matrix-Based DSL for Explicit and Implicit Matrix-Free PDE Solvers on Structured Grid},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622822},
doi = {10.1145/3622822},
abstract = {Partial differential equation (PDE) solvers are extensively utilized across numerous scientific and engineering fields. However, achieving high performance and scalability often necessitates intricate and low-level programming, particularly when leveraging deterministic sparsity patterns in structured grids. In this paper, we propose an innovative domain-specific language (DSL), Mat2Stencil, with its compiler, for PDE solvers on structured grids. Mat2Stencil introduces a structured sparse matrix abstraction, facilitating modular, flexible, and easy-to-use expression of solvers across a broad spectrum, encompassing components such as Jacobi or Gauss-Seidel preconditioners, incomplete LU or Cholesky decompositions, and multigrid methods built upon them. Our DSL compiler subsequently generates matrix-free code consisting of generalized stencils through multi-stage programming. The code allows spatial loop-carried dependence in the form of quasi-affine loops, in addition to the Jacobi-style stencil’s embarrassingly parallel on spatial dimensions. We further propose a novel automatic parallelization technique for the spatially dependent loops, which offers a compile-time deterministic task partitioning for threading, calculates necessary inter-thread synchronization automatically, and generates an efficient multi-threaded implementation with fine-grained synchronization. Implementing 4 benchmarking programs, 3 of them being the pseudo-applications in NAS Parallel Benchmarks with 6.3\% lines of code and 1 being matrix-free High Performance Conjugate Gradients with 16.4\% lines of code, we achieve up to 1.67\texttimes{} and on average 1.03\texttimes{} performance compared to manual implementations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {246},
numpages = {30},
keywords = {compiler, domain-specific language, finite difference method, multi-stage programming, performance optimization, polyhedral compilation, stencil, structured grid}
}

@inproceedings{10.1145/3652620.3687805,
author = {Netz, Lukas and Reimer, Jan and Rumpe, Bernhard},
title = {Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687805},
doi = {10.1145/3652620.3687805},
abstract = {Low-code development platforms (LCDPs) are becoming increasingly important in industry, which confronts us in academic teaching with the challenge of educating students in the basic principles, critical engagement, and evaluation of LCDPs. This leads us to the question, how to teach the usage of different LCDPs during an university course. The short time frame of university-level courses makes it challenging to teach more than only one LCDP. In our teaching approach, students use two different LCDPs and create a web-application with both of them. Firstly, we require the students to define a target application with common modeling languages, next they use the first LCDP, at about half the time they switch to the second LCDP and present their findings of the differences in methodology and development processes at the end. We discuss this approach, show survey results from the participants, and explain lessons learned. This concept allows students critical engagement with LCDPs and model-driven software engineering. Supervisors get an insight into the learnability of each LCDP and how novices adapt to different domain-specific languages and their notations.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {115–122},
numpages = {8},
keywords = {low-code development platforms, education, university-level courses, model-driven software engineering, problem-based learning},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3597503.3639081,
author = {Jiang, Yuxuan and Zhang, Chaoyun and He, Shilin and Yang, Zhihao and Ma, Minghua and Qin, Si and Kang, Yu and Dang, Yingnong and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
title = {Xpert: Empowering Incident Management with Query Recommendations via Large Language Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639081},
doi = {10.1145/3597503.3639081},
abstract = {Large-scale cloud systems play a pivotal role in modern IT infrastructure. However, incidents occurring within these systems can lead to service disruptions and adversely affect user experience. To swiftly resolve such incidents, on-call engineers depend on crafting domain-specific language (DSL) queries to analyze telemetry data. However, writing these queries can be challenging and time-consuming. This paper presents a thorough empirical study on the utilization of queries of KQL, a DSL employed for incident management in a large-scale cloud management system at Microsoft. The findings obtained underscore the importance and viability of KQL queries recommendation to enhance incident management.Building upon these valuable insights, we introduce Xpert, an end-to-end machine learning framework that automates KQL recommendation process. By leveraging historical incident data and large language models, Xpert generates customized KQL queries tailored to new incidents. Furthermore, Xpert incorporates a novel performance metric called Xcore, enabling a thorough evaluation of query quality from three comprehensive perspectives. We conduct extensive evaluations of Xpert, demonstrating its effectiveness in offline settings. Notably, we deploy Xpert in the real production environment of a large-scale incident management system in Microsoft, validating its efficiency in supporting incident management. To the best of our knowledge, this paper represents the first empirical study of its kind, and Xpert stands as a pioneering DSL query recommendation framework designed for incident management.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {92},
numpages = {13},
keywords = {incident management, query generation, large language model},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3613904.3642267,
author = {Jun, Eunice and Misback, Edward and Heer, Jeffrey and Just, Rene},
title = {rTisane: Externalizing conceptual models for data analysis prompts reconsideration of domain assumptions and facilitates statistical modeling},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642267},
doi = {10.1145/3613904.3642267},
abstract = {Statistical models should accurately reflect analysts’ domain knowledge about variables and their relationships. While recent tools let analysts express these assumptions and use them to produce a resulting statistical model, it remains unclear what analysts want to express and how externalization impacts statistical model quality. This paper addresses these gaps. We first conduct an exploratory study of analysts using a domain-specific language (DSL) to express conceptual models. We observe a preference for detailing how variables relate and a desire to allow, and then later resolve, ambiguity in their conceptual models. We leverage these findings to develop rTisane, a DSL for expressing conceptual models augmented with an interactive disambiguation process. In a controlled evaluation, we find that analysts reconsidered their assumptions, self-reported externalizing their assumptions accurately, and maintained analysis intent with rTisane. Additionally, rTisane enabled some analysts to author statistical models they were unable to specify manually. For others, rTisane resulted in models that better fit the data or enabled iterative improvement.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {1037},
numpages = {16},
keywords = {domain-specific language, end-user elicitation, end-user programming, linear modeling, statistical analysis},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3729330,
author = {Li, Yixuan and Magalh\~{a}es, Jos\'{e} Wesley de Souza and Brauckmann, Alexander and O'Boyle, Michael F. P. and Polgreen, Elizabeth},
title = {Guided Tensor Lifting},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {PLDI},
url = {https://doi.org/10.1145/3729330},
doi = {10.1145/3729330},
abstract = {Domain-specific languages (DSLs) for machine learning are revolutionizing the speed and efficiency of machine learning workloads as they enable users easy access to high-performance compiler optimizations and accelerators. However, to take advantage of these capabilities, a user must first translate their legacy code from the language it is currently written in, into the new DSL. The process of automatically lifting code into these DSLs has been identified by several recent works, which propose program synthesis as a solution. However, synthesis is expensive and struggles to scale without carefully designed and hard-wired heuristics. In this paper, we present an approach for lifting that combines an enumerative synthesis approach with a Large Language Model used to automatically learn the domain-specific heuristics for program lifting, in the form of a probabilistic grammar. Our approach outperforms the state-of-the-art tools in this area, despite only using learned heuristics.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {227},
numpages = {23},
keywords = {Code Generation, Code Optimization, Large Language Model, Lifting, Program Synthesis, Tensor Algebra}
}

@article{10.1145/3720526,
author = {Kim, Donguk and Jeon, Minseok and Hwang, Doha and Oh, Hakjoo},
title = {PAFL: Enhancing Fault Localizers by Leveraging Project-Specific Fault Patterns},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720526},
doi = {10.1145/3720526},
abstract = {We present PAFL, a new technique for enhancing existing fault localization methods by leveraging project-specific fault patterns.                We observed that each software project has its own challenges and suffers from recurring fault patterns associated with those challenges.                 However, existing fault localization techniques use a universal localization strategy without considering those repetitive faults.                To address this limitation, our technique, called project-aware fault localization (PAFL), enables existing fault localizers to leverage project-specific fault patterns.                Given a buggy version of a project and a baseline fault localizer, PAFL first mines the fault patterns from past buggy versions of the project.                 Then, it uses the mined fault patterns to update the suspiciousness scores of statements computed by the baseline fault localizer.                To this end, we use two novel ideas.                First, we design a domain-specific fault pattern-description language to represent various fault patterns.                An instance, called crossword, in our language describes a project-specific fault pattern and how it affects the suspiciousness scores of statements.                Second, we develop an algorithm that synthesizes crosswords (i.e., fault patterns) from past buggy versions of the project.                Evaluation using seven baseline fault localizers and 12 real-world C/C++ and Python projects demonstrates that PAFL effectively, robustly, and efficiently improves the performance of the baseline fault localization techniques.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {129},
numpages = {28},
keywords = {Domain-Specific Language, Fault Localization, Program Synthesis}
}

@article{10.1145/3733719,
author = {Kreikemeyer, Justin Noah and Jankowski, Mi\l{}osz and Wilsdorf, Pia and Uhrmacher, Adelinde M.},
title = {Using (Not-so) Large Language Models to Generate Simulation Models in a Formal DSL: A Study on Reaction Networks},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-3301},
url = {https://doi.org/10.1145/3733719},
doi = {10.1145/3733719},
abstract = {Formal languages are an integral part of modeling and simulation. They allow the distillation of knowledge into concise simulation models amenable to automatic execution, interpretation, and analysis. However, the arguably most humanly accessible means of expressing models is through natural language, which is not easily interpretable by computers. Here, we evaluate how a Large Language Model (LLM) might be used for formalizing natural language into simulation models. Existing studies only explored using very large LLMs, like the commercial GPT models, without fine-tuning model weights. To close this gap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned to translate natural language descriptions to reaction network models in a domain-specific language, offering a self-hostable, compute-, and memory efficient alternative. To this end, we develop a synthetic data generator to serve as the basis for fine-tuning and evaluation. Our quantitative evaluation shows that our fine-tuned Mistral model can recover the ground truth simulation model in up to  (84.5\% )  of cases. In addition, our small-scale user study demonstrates the model’s practical potential for one-time generation as well as interactive modeling in various domains. While promising, in its current form, the fine-tuned small LLM cannot catch up with large LLMs. We conclude that higher-quality training data are required, and expect future small and open-source LLMs to offer new opportunities.},
note = {Just Accepted},
journal = {ACM Trans. Model. Comput. Simul.},
month = may,
keywords = {simulation model generation, natural language processing, language model, constrained decoding, knowledge extraction}
}

@article{10.1145/3727980,
author = {Chang, Kaiyan and Zhu, Wenlong and Wang, Kun and He, Xinyang and Yang, Nan and Chen, Zhirong and Jin, Dantong and Li, Cangyuan and Zhou, Yunhao and Yan, Hao and Zhao, Zhuoliang and Cheng, Yuan and Wang, Mengdi and Liang, Shengwen and Han, Yinhe and Li, Xiaowei and Li, Huawei and Wang, Ying},
title = {A data-centric chip design agent framework for Verilog code generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3727980},
doi = {10.1145/3727980},
abstract = {Recent advances in large language models (LLMs) have demonstrated significant potential for automated hardware description language (HDL) code generation from high-level specifications. However, two critical challenges limit further progress in this domain: the scarcity of quality Verilog training data and the inability of current approaches to generate RTL code optimized for power, performance, and area (PPA) metrics. This paper presents a comprehensive data-centric framework that addresses these limitations through innovations in both pre-fine-tuning data preparation and after-fine-tuning optimization strategies. In the pre-fine-tuning phase, we tackle the data scarcity problem with an automated design-data augmentation framework that generates high-volume, high-quality natural language specifications aligned with corresponding Verilog code and EDA scripts. Our approach creates a complete RTL-level feedback loop by augmenting EDA scripts, RTL code, and EDA tool feedback. In the after-fine-tuning phase, we focus on generating PPA-aware RTL code through a novel search and prompt framework. Our approach implements iterative filtering and selection of LLM-generated Verilog variants while providing high-quality predefined prompts, including composition and interface specifications. To evaluate the effectiveness of our data augmentation method, we fine-tune Llama 2-13B and Llama 2-7B models using the dataset generated by our augmentation framework. The results demonstrate a significant improvement in the Verilog generation tasks with LLMs. Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8\% to 70.6\% with the same benchmark. Our 13B model has a pass rate improvement compared with GPT-3.5 in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data. Additionally, to evaluate the effectiveness of the our agent framework, we compare the PPA on the GPT-3.5, where the results show that the agent refined RTL code can have a better quality than the generated RTL code only with GPT-3.5.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = apr,
keywords = {Large Language Model, Hardware Generation, Data Augmentation}
}

@inproceedings{10.1145/3650212.3680388,
author = {Xue, Zhiyi and Li, Liangguo and Tian, Senyue and Chen, Xiaohong and Li, Pingping and Chen, Liangyu and Jiang, Tingting and Zhang, Min},
title = {LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680388},
doi = {10.1145/3650212.3680388},
abstract = {FinTech software, crucial for both safety and timely market deployment, presents a compelling case for automated acceptance testing against regulatory business rules. However, the inherent challenges of comprehending unstructured natural language descriptions of these rules and crafting comprehensive test cases demand human intelligence. The emergence of Large Language Models (LLMs) holds promise for automated test case generation, leveraging their natural language processing capabilities. Yet, their dependence on human intervention for effective prompting hampers efficiency.    In response, we introduce a groundbreaking, fully automated approach for generating high-coverage test cases from natural language business rules. Our methodology seamlessly integrates the versatility of LLMs with the predictability of algorithmic methods. We fine-tune pre-trained LLMs for improved information extraction accuracy and algorithmically generate comprehensive testable scenarios for the extracted business rules.	Our prototype, LLM4Fin, is designed for testing real-world stock-trading software. Experimental results demonstrate LLM4Fin’s superiority over both state-of-the-art LLM, such as ChatGPT, and skilled testing engineers. We achieve remarkable performance, with up to 98.18\% and an average of 20\%−110\% improvement on business scenario coverage, and up to 93.72\% on code coverage, while reducing the time cost from 20 minutes to a mere 7 seconds. These results provide robust evidence of the framework’s practical applicability and efficiency, marking a significant advancement in FinTech software testing.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1643–1655},
numpages = {13},
keywords = {Software acceptance testing, fintech software, large language model, test case generation},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3652620.3687774,
author = {Chen, Kua and Chen, Boqi and Yang, Yujing and Mussbacher, Gunter and Varr\'{o}, D\'{a}niel},
title = {Embedding-based Automated Assessment of Domain Models},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687774},
doi = {10.1145/3652620.3687774},
abstract = {Domain modeling is an essential component in many software engineering courses since it serves as a way to represent and understand the concepts and relationships in a problem domain. Course instructors evaluate student-generated diagrams manually, comparing them against a reference solution and providing feedback. However, as enrollment in software engineering courses continues to rise, manual grading of a large number of student submissions becomes an overwhelming and time-intensive task for instructors. Hence, there is a need for automated assessment of domain models which assists course instructors during the grading process. In this paper, we propose a novel text embedding-based approach that automatizes the assessment of domain models expressed in a textual domain-specific language, against reference solutions created by modeling experts. Our algorithm showcases remarkable proficiency in matching model elements across domain models, achieving an F1-score of 0.82 for class matching, 0.75 for attribute matching, and 0.80 for relation matching. Our algorithm also yields grades highly correlated with human grader assessments, with correlations exceeding 0.8 and mean absolute errors below 0.05.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {87–94},
numpages = {8},
keywords = {domain modeling, text embeddings, domain model assessment},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3659677.3659742,
author = {Babaalla, Zakaria and Jakimi, Abdeslam and Oualla, Mohamed and Saadane, Rachid and Chehri, Abdellah},
title = {Towards an Automatic Extracting UML Class Diagram from System's Textual Specification},
year = {2024},
isbn = {9798400709296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659677.3659742},
doi = {10.1145/3659677.3659742},
abstract = {Developing a software system from natural language requirements is a complex and delicate task that requires a high level of design and programming expertise. Increasing the level of abstraction used to describe these requirements is the most natural solution. Model-Driven Engineering (MDE) also takes this route, using abstract models as primary entities to generate source code automatically or semi-automatically. Among these models, the UML class diagram occupies a privileged place in object-oriented systems because it not only serves as a basis for communication between developers but also provides a closely aligned static representation of the system implementation. However, creating a UML class diagram from a textual system specification poses a significant challenge due to the inherent imprecision and ambiguity commonly found in natural language expressions. In this paper, we propose a model-centric approach based on deep learning for the automatic extraction of UML class diagrams from textual requirements.},
booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
articleno = {36},
numpages = {5},
keywords = {Class diagram, Deep learning, Machine learning, Natural Language Processing, UML},
location = {Meknes, AA, Morocco},
series = {NISS '24}
}

@inproceedings{10.1145/3708036.3708111,
author = {Huang, Qinhua},
title = {Improving Explanations of Legal Judgement Prediction in Chinese Context by Legal Language Model},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708111},
doi = {10.1145/3708036.3708111},
abstract = {This paper presents an investigation into improving the explanations accompanying predictions of legal judgments in the Chinese context through the utilization of a specialized legal language model. The study address the growing interest in the field of computational legal analysis, specifically legal judgment prediction (LJP), which has emerged as a crucial tool for enhancing the efficiency, transparency, and accessibility of legal systems worldwide. Recognizing the significance of providing clear and comprehensive explanations in the legal domain, we emphasize that these elucidations serve as vital aids to judges during their decision-making processes, fostering greater efficiency, accuracy, and trustworthiness within the judiciary. Moreover, the ability to offer well-grounded explanations is particularly crucial in the context of judicial supervision, where supervisors often face time constraints and need concise yet informative summaries of case details to conduct effective reviews. To achieve this goal, we deploy the legal language model, Lawformer and other models to construct a work flow, addressing long legal document judgement prediction problem, while presenting the related explanation. We also conduct an experiment to prove the validity of the method.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {439–443},
numpages = {5},
keywords = {explanation, judgement prediction, legal large language model},
location = {
},
series = {ICCSMT '24}
}

@inproceedings{10.1145/3652620.3688557,
author = {Manellanga, Rajitha and David, Istvan},
title = {Participatory and Collaborative Modeling of Sustainable Systems: A Systematic Review},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688557},
doi = {10.1145/3652620.3688557},
abstract = {Sustainability has become a key characteristic of modern systems. Unfortunately, the convoluted nature of sustainability limits its understanding and hinders the design of sustainable systems. Thus, cooperation among a diverse set of stakeholders is paramount to sound sustainability-related decisions. Collaborative modeling has demonstrated benefits in facilitating cooperation between technical experts in engineering problems; but fails to include non-technical stakeholders in the modeling endeavor. In contrast, participatory modeling excels in facilitating high-level modeling among a diverse set of stakeholders, often of non-technical profiles; but fails to generate actionable engineering models. To instigate a convergence between the two disciplines, we systematically survey the field of collaborative and participatory modeling for sustainable systems. By analyzing 24 primary studies (published until June 2024), we identify common challenges, cooperation models, modeling formalisms and tools; and recommend future avenues of research.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {645–654},
numpages = {10},
keywords = {collaboration, MDE, model-driven, model-based, participatory modeling, survey, sustainability, systematic literture review},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3711542.3711571,
author = {undefinedlgen, Bahar and Hattab, Georges},
title = {A Lexical Simplification Framework for Turkish},
year = {2025},
isbn = {9798400717383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711542.3711571},
doi = {10.1145/3711542.3711571},
abstract = {Lexical simplification is a fundamental step towards improving the accessibility, comprehension, and readability of texts, particularly in languages with limited linguistic resources. In this study, we adopt a state-of-the-art lexical simplification approach and propose a lexical simplification framework tailored to Turkish. Our framework leverages a combination of complex word identification tasks and substitution generation through pre-trained language models to identify complex lexical units using selective substitution ranking approaches and algorithms and replace them with simpler alternatives, thereby improving text readability. This work makes three key contributions: (i) a comprehensive study of lexical simplification for Turkish, including the complex word identification subtask; (ii) a rigorous comparison of various language models for candidate generation using the masked language modeling objective; and (iii) an in-depth exploration of the impact of different complexity thresholds and additional parameters on overall performance. Our framework demonstrates a strong capability to balance simplification and contextual preservation, offering an effective solution to lexical simplification in Turkish.},
booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval},
pages = {218–224},
numpages = {7},
keywords = {BERT, Text simplification, artificial intelligence for social good, complex word identification, lexical simplification, low-resource languages, pre-trained language model},
location = {
},
series = {NLPIR '24}
}

@article{10.1145/3729364,
author = {Wan, Yuxuan and Wang, Chaozheng and Dong, Yi and Wang, Wenxuan and Li, Shuqing and Huo, Yintong and Lyu, Michael},
title = {Divide-and-Conquer: Generating UI Code from Screenshots},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729364},
doi = {10.1145/3729364},
abstract = {Websites are critical in today’s digital world, with over 1.11 billion currently active and approximately 252,000 new sites launched daily. Converting website layout design into functional UI code is a time-consuming yet indispensable step of website development. Manual methods of converting visual designs into functional code present significant challenges, especially for non-experts. To explore automatic design-to-code solutions, we first conduct a motivating study on GPT-4o and identify three types of issues in generating UI code: element omission, element distortion, and element misarrangement. We further reveal that a focus on smaller visual segments can help multimodal large language models (MLLMs) mitigate these failures in the generation process. In this paper, we propose DCGen, a divide-and-conquer-based approach to automate the translation of webpage design to UI code. DCGen starts by dividing screenshots into manageable segments, generating code for each segment, and then reassembling them into complete UI code for the entire screenshot. We conduct extensive testing with a dataset comprised of real-world websites and various MLLMs and demonstrate that DCGen achieves up to a 15\% improvement in visual similarity and 8\% in code similarity for large input images. Human evaluations show that DCGen can help developers implement webpages significantly faster and more similar to the UI designs. To the best of our knowledge, DCGen is the first segment-aware MLLM-based approach for generating UI code directly from screenshots.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE094},
numpages = {24},
keywords = {Code Generation, Multi-modal Large Language Model, User Interface, Web Development}
}

@inproceedings{10.1145/3696630.3728529,
author = {Gao, Yanjie and Luo, Jiyu and Lin, Haoxiang and Zhang, Hongyu and Wu, Ming and Yang, Mao},
title = {dl²: Detecting Communication Deadlocks in Deep Learning Jobs},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728529},
doi = {10.1145/3696630.3728529},
abstract = {In recent years, deep learning has seen widespread adoption across various domains, giving rise to large-scale models such as large language models. Training these models, particularly in distributed environments, presents substantial computational and communication challenges. A critical issue is the communication deadlock—a state in which processes become indefinitely stalled while awaiting network messages from others, which leads to resource wastage and reduced productivity. Current approaches to deadlock handling are either unsuitable for deep learning due to its unique hybrid programming paradigm or limit optimization opportunities. This paper presents dl2, a novel dynamic analysis tool designed to detect communication deadlocks in deep learning jobs. dl2 models the runtime trace of a job as an execution graph, detects unmatched communications, and constructs a wait-for graph to identify deadlock cycles. dl2 can also handle nondeterministic communication behaviors, providing replay and diagnostic support for root cause analysis. We evaluate dl2 using PyTorch with a combination of synthetic test cases and real-world deep learning workloads. The experimental results show that dl2 successfully detects all communication deadlocks, achieving 100\% precision and recall, which highlights its effectiveness.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {27–38},
numpages = {12},
keywords = {deep learning, large language model, communication deadlock},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3640310.3674096,
author = {Rajbhoj, Asha and Pathan, Ajim and Sant, Tanay and Kulkarni, Vinay and Nistala, Padmalata and Pandey, Rajesh and Narasimhan, Sabarinathan and Thiagarajan, Geetha},
title = {AutoMW: Model-based Automated Medical Writing},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674096},
doi = {10.1145/3640310.3674096},
abstract = {Medical Writing is an art of writing scientific documents which includes regulatory and research-related content. To obtain approval for marketing new medicines, pharmaceutical companies are obligated to provide drug authorities with a huge volume of documents related to clinical trials. Creating these clinical trial documents is a time, effort, and skill-intensive process as the required information exists in fragmented form distributed across various information sources. To overcome these challenges in medical writing, we propose Automated Medical Writing tool (AutoMW). AutoMW enables the digitalization of information from different sources of information using a meta-model-based approach and leverages these models for the automated generation of clinical trial documents as per the regulatory authority document templates. This paper describes the approach and illustrates its utility and efficacy in real-world clinical trial application of two use cases - breast cancer, and diabetes.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {257–267},
numpages = {11},
keywords = {Automated Content Generation, Clinical Trial Documentation, MDE, Medical Writing, NLP},
location = {Linz, Austria},
series = {MODELS '24}
}

@article{10.1145/3728955,
author = {Zheng, Mingwei and Xie, Danning and Shi, Qingkai and Wang, Chengpeng and Zhang, Xiangyu},
title = {Validating Network Protocol Parsers with Traceable RFC Document Interpretation},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728955},
doi = {10.1145/3728955},
abstract = {Validating the correctness of network protocol implementations is highly challenging due to the oracle and traceability problems. The former determines when a protocol implementation can be considered buggy, especially when the bugs do not cause any observable symptoms. The latter allows developers to understand how an implementation violates the protocol specification, thereby facilitating bug fixes. Unlike existing works that rarely take both problems into account, this work considers both and provides an effective solution using recent advances in large language models (LLMs). Our key observation is that network protocols are often released with structured specification documents, a.k.a. RFC documents, which can be systematically translated to formal protocol message specifications via LLMs. Such specifications, which may contain errors due to the hallucination of LLMs, are used as a quasi-oracle to validate protocol parsers, while the validation results in return gradually refine the oracle. Since the oracle is derived from the document, any bugs we find in a protocol implementation can be traced back to the document, thus addressing the traceability problem. We have extensively evaluated our approach using nine network protocols and their implementations written in C, Python, and Go. The results show that our approach outperforms the state-of-the-art and has detected 69 bugs, with 36 confirmed. The project also demonstrates the potential for fully automating software validation based on natural language specifications, a process previously considered predominantly manual due to the need to understand specification documents and derive expected outputs for test inputs.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA078},
numpages = {23},
keywords = {Large language model, Network protocol parsers, Traceability}
}

@inproceedings{10.1145/3649329.3657356,
author = {Chang, Kaiyan and Wang, Kun and Yang, Nan and Wang, Ying and Jin, Dantong and Zhu, Wenlong and Chen, Zhirong and Li, Cangyuan and Yan, Hao and Zhou, Yunhao and Zhao, Zhuoliang and Cheng, Yuan and Pan, Yudong and Liu, Yiqi and Wang, Mengdi and Liang, Shengwen and Han, Yinhe and Li, Huawei and Li, Xiaowei},
title = {Data is all you need:  Finetuning LLMs for Chip Design via an Automated design-data augmentation framework},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3657356},
doi = {10.1145/3649329.3657356},
abstract = {Recent advances in large language models have demonstrated their potential for automated generation of hardware description language (HDL) code from high-level prompts. Researchers have utilized fine-tuning to enhance the ability of these large language models (LLMs) in the field of Chip Design. However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs. Additionally, the absence of a Verilog and electronic design automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers. This paper proposes an automated design-data augmentation framework, which generates high-volume and high-quality natural language aligned with Verilog and EDA scripts. For Verilog generation, it translates Verilog files to an abstract syntax tree and then maps nodes to natural language with a predefined template. For Verilog repair, it uses predefined rules to generate the wrong verilog file and then pairs EDA Tool feedback with the right and wrong verilog file. For EDA Script generation, it uses existing LLM(GPT-3.5) to obtain the description of the Script. To evaluate the effectiveness of our data augmentation method, we finetune Llama2--13B and Llama2-7B models using the dataset generated by our augmentation framework. The results demonstrate a significant improvement in the Verilog generation tasks with LLMs. Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8\% to 70.6\% with the same benchmark. Our 13B model (ChipGPT-FT1) has a pass rate improvement compared with GPT-3.5 in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {60},
numpages = {6},
keywords = {data augmentation, hardware generation large language model},
location = {San Francisco, CA, USA},
series = {DAC '24}
}

@inproceedings{10.1145/3701716.3715232,
author = {Li, Xiangyu and Zeng, Yawen and Xing, Xiaofen and Xu, Jin and Xu, Xiangmin},
title = {HedgeAgents: A Balanced-aware Multi-agent Financial Trading System},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715232},
doi = {10.1145/3701716.3715232},
abstract = {As automated trading gains traction in the financial market, algorithmic investment strategies are increasingly prominent. While Large Language Models (LLMs) and Agent-based models exhibit promising potential in real-time market analysis and trading decisions, they still experience a significant -20\% loss when confronted with rapid declines or frequent fluctuations, impeding their practical application. Hence, there is an imperative to explore a more robust and resilient framework. This paper introduces an innovative multi-agent system, HedgeAgents, aimed at bolstering system robustness via ''hedging'' strategies. In this well-balanced system, an array of hedging agents has been tailored, where HedgeAgents consist of a central fund manager and multiple hedging experts specializing in various financial asset classes. These agents leverage LLMs' cognitive capabilities to make decisions and coordinate through three types of conferences. Benefiting from the powerful understanding of LLMs, our HedgeAgents attained a 70\% annualized return and a 400\% total return over a period of 3 years. Moreover, we have observed with delight that HedgeAgents can even formulate investment experience comparable to those of human experts (https://hedgeagents.github.io/).},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {296–305},
numpages = {10},
keywords = {large language model, multi-agent systems, quantization finance},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3604237.3626869,
author = {Li, Yinheng and Wang, Shaofei and Ding, Han and Chen, Hang},
title = {Large Language Models in Finance: A Survey},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626869},
doi = {10.1145/3604237.3626869},
abstract = {Recent advances in large language models (LLMs) have opened new possibilities for artificial intelligence applications in finance. In this paper, we provide a practical survey focused on two key aspects of utilizing LLMs for financial tasks: existing solutions and guidance for adoption. First, we review current approaches employing LLMs in finance, including leveraging pretrained models via zero-shot or few-shot learning, fine-tuning on domain-specific data, and training custom LLMs from scratch. We summarize key models and evaluate their performance improvements on financial natural language processing tasks. Second, we propose a decision framework to guide financial professionals in selecting the appropriate LLM solution based on their use case constraints around data, compute, and performance needs. The framework provides a pathway from lightweight experimentation to heavy investment in customized LLMs. Lastly, we discuss limitations and challenges around leveraging LLMs in financial applications. Overall, this survey aims to synthesize the state-of-the-art and provide a roadmap for responsibly applying LLMs to advance financial AI.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {374–382},
numpages = {9},
keywords = {Finance, Generative AI, Large Language Models, Natural Language Processing},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@inproceedings{10.1145/3678719.3685691,
author = {Garc\'{\i}a, Boni and Leotta, Maurizio and Ricca, Filippo and Whitehead, Jim},
title = {Use of ChatGPT as an Assistant in the End-to-End Test Script Generation for Android Apps},
year = {2024},
isbn = {9798400711091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678719.3685691},
doi = {10.1145/3678719.3685691},
abstract = {Automated testing is crucial in software development to ensure that applications perform as intended. However, generating automated End-to-End (E2E) tests can be time-consuming and challenging, especially for junior developers. This study investigates the use of ChatGPT, a popular Generative Artificial Intelligence (GenAI) model, as an assistant in developing automated E2E test scripts for Android apps. We present an empirical study that compares the effort required to create E2E test scripts and the resulting reliability of these tests using two treatments: manually and assisted by ChatGPT. We used Gherkin, a domain-specific language that allows non-technical practitioners to define test scenarios using a human-readable syntax. Our findings indicate that using ChatGPT significantly reduces the time required to develop automated test scripts without compromising the reliability of the scripts. Statistical analysis shows a notable reduction in development time for the ChatGPT-assisted group compared to the manual group, with a large effect size. While the reliability of the tests did not show a significant difference between the two groups, the results suggest practical benefits in terms of efficiency.},
booktitle = {Proceedings of the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation},
pages = {5–11},
numpages = {7},
keywords = {Android, E2E Automated Testing, Empirical Study, GenAI},
location = {Vienna, Austria},
series = {A-TEST 2024}
}

@inproceedings{10.1145/3678726.3678778,
author = {Li, Zhi-Fang and zhao, Shuo and Zhang, Ya-Chen},
title = {Research on the Transformation of Music Education Model under the Background of Generative Artificial Intelligence},
year = {2024},
isbn = {9798400717611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678726.3678778},
doi = {10.1145/3678726.3678778},
abstract = {Since 2022, the rapid development of generative artificial intelligence systems represented by Chat-GPT has profoundly influenced various fields. This article briefly introduces the development process of generative artificial intelligence models in recent years, explains the benefits of applying generative artificial intelligence in music education, and discusses the direction of transformation for music education models in the context of the development of generative artificial intelligence. It also reflects and summarizes the challenges faced by the current application of generative artificial intelligence in the field of music education.},
booktitle = {Proceedings of the 2024 8th International Conference on Education and Multimedia Technology},
pages = {60–64},
numpages = {5},
keywords = {Keywords—Generative AI, Music education, Research on Transformation Models},
location = {Tokyo, Japan},
series = {ICEMT '24}
}

@inproceedings{10.1145/3650212.3652135,
author = {Lu, You and Tian, Yifan and Bi, Yuyang and Chen, Bihuan and Peng, Xin},
title = {DiaVio: LLM-Empowered Diagnosis of Safety Violations in ADS Simulation Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652135},
doi = {10.1145/3650212.3652135},
abstract = {Simulation testing has been widely adopted by leading companies to ensure the safety of autonomous driving systems (ADSs). Anumber of scenario-based testing approaches have been developed to generate diverse driving scenarios for simulation testing, and demonstrated to be capable of finding safety violations. However, there is no automated way to diagnose whether these violations are caused by the ADS under test and which category these violations belong to. As a result, great effort is required to manually diagnose violations.     To bridge this gap, we propose DiaVio to automatically diagnose safety violations in simulation testing by leveraging large language models (LLMs). It is built on top of a new domain specific language (DSL) of crash to align real-world accident reports described in natural language and violation scenarios in simulation testing. DiaVio fine-tunes a base LLM with real-world accident reports to learn diagnosis capability, and uses the fine-tuned LLM to diagnose violation scenarios in simulation testing. Our evaluation has demonstrated the effectiveness and efficiency of DiaVio in violation diagnosis.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {376–388},
numpages = {13},
keywords = {Automated Driving System, Large Language Models, Scenario-based Testing, Violation Diagnosis},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3675094.3678494,
author = {Hu, Yongquan and Zhang, Shuning and Dang, Ting and Jia, Hong and Salim, Flora D. and Hu, Wen and Quigley, Aaron J.},
title = {Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal Data for Mental Health},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678494},
doi = {10.1145/3675094.3678494},
abstract = {Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuable multimodal insights into psychological states or neurological disorders. Recent advancements with Large Language Models (LLMs) position them as prospective "health agents'' for mental health assessment. However, current research predominantly focus on single data modalities, presenting an opportunity to advance understanding through multimodal data. Our study aims to advance this approach by investigating multimodal data using LLMs for mental health assessment, specifically through zero-shot and few-shot prompting. Three datasets are adopted for depression and emotion classifications incorporating EEG, facial expressions, and audio (text). The results indicate that multimodal information confers substantial advantages over single modality approaches in mental health assessment. Notably, integrating EEG alongside commonly used LLM modalities such as audio and images demonstrates promising potential. Moreover, our findings reveal that 1-shot learning offers greater benefits compared to zero-shot learning methods.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {412–417},
numpages = {6},
keywords = {eeg, large language model, mental health, prompt engineering},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3643795.3648384,
author = {Koziolek, Heiko and Gr\"{u}ner, Sten and Hark, Rhaban and Ashiwal, Virendra and Linsbauer, Sofia and Eskandani, Nafise},
title = {LLM-based and Retrieval-Augmented Control Code Generation},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648384},
doi = {10.1145/3643795.3648384},
abstract = {Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {22–29},
numpages = {8},
keywords = {large language models, code generation, IEC 61131-3, industrial automation, PLC, DCS, ChatGPT, GPT-4},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3652620.3687807,
author = {Yang, Yujing and Chen, Boqi and Chen, Kua and Mussbacher, Gunter and Varr\'{o}, D\'{a}niel},
title = {Multi-step Iterative Automated Domain Modeling with  Large Language Models},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687807},
doi = {10.1145/3652620.3687807},
abstract = {Domain modeling, which represents the concepts and relationships in a problem domain, is an essential part of software engineering. As large language models (LLMs) have recently exhibited remarkable ability in language understanding and generation, many approaches are designed to automate domain modeling with LLMs. However, these approaches usually formulate all input information to the LLM in a single step. Our previous single-step approach resulted in many missing modeling elements and advanced patterns. This paper introduces a novel framework designed to enhance fully automated domain model generation. The proposed multi-step automated domain modeling approach extracts model elements (e.g., classes, attributes, and relationships) from problem descriptions. The approach includes instructions and human knowledge in each step and uses an iterative process to identify complex patterns, repeatedly extracting the pattern from various instances and then synthesizing these extractions into a summarized overview. Furthermore, the framework incorporates a self-reflection mechanism. This mechanism assesses each generated model element, offering self-feedback for necessary modifications or removals, and integrates the domain model with the generated self-feedback. The proposed approach is assessed in experiments, comparing it with a baseline single-step approach from our earlier work. Experiments demonstrate a significant improvement over our earlier work, with a 22.71\% increase in the F1-score for identifying classes, 75.18\% for relationships, and a 10.39\% improvement for identifying the player-role pattern, with comparable performance for attributes. Our approach, dataset, and evaluation provide valuable insight for future research in automated LLM-based domain modeling.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {587–595},
numpages = {9},
keywords = {domain modeling, large language models, few-shot learning, prompt engineering},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3686246,
author = {Siddeshwar, Vaishali and Alwidian, Sanaa and Makrehchi, Masoud},
title = {A Comparative Study of Large Language Models for Goal Model Extraction},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3686246},
doi = {10.1145/3652620.3686246},
abstract = {User stories, expressed in snippets of natural language text, are commonly used to elicit stakeholder's needs in agile software development. Requirement engineers model user stories to interpret the relations among goals and requirements. Manual transformation of goal models has challenges such as, difficulty of converting lower-abstraction user stories into higher-level goals, and extraction of goals embedded in user stories depends on the skill of requirements engineers. In this paper we introduce a technique that leverages Large Language Models (LLMs) to automatically generate goal models from user stories. The approach uses Iterative Prompt Engineering that guides LLM to extract intentional elements and generate its XML-compatible representation in Goal-oriented Requirements Language (GRL). The generated models can be visualized using jUCMNav tool. We evaluated our approach using three LLMs: GPT-4, Llama and Cohere. Our qualitative evaluation indicates that GPT-4 or Llama can be used to assist requirements engineers in modeling as they can produce GRL goal models that are understandable. Additionally, these LLMs are capable of exposing soft goals that are not apparent to stakeholders who are new to the domain.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {253–263},
numpages = {11},
keywords = {goal-oriented requirement language (GRL), goal modeling, user story, agile development, requirements engineering, large language models (LLMS), GPT-4, llama, cohere},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3639473.3665788,
author = {Mei, Yuxuan and Jones, Benjamin and Cascaval, Dan and Mankoff, Jennifer and Vouga, Etienne and Schulz, Adriana},
title = {FabHacks: Transform Everyday Objects into Home Hacks Leveraging a Solver-aided DSL},
year = {2024},
isbn = {9798400704963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639473.3665788},
doi = {10.1145/3639473.3665788},
abstract = {Storage, organizing, and decorating are important aspects of home design. Buying commercial items for many of these tasks, this can be costly, and reuse is more sustainable. An alternative is a “home hack,” i.e., a functional assembly constructed from existing household items. However, coming up with such hacks requires combining objects to make a physically valid design, which might be difficult to test if they are large, require nailing or screwing to the wall, or if the designer has mobility limitations. We present a design and visualization system, FabHacks, for creating workable functional assemblies. The system is based on a new solver-aided domain-specific language (S-DSL) called FabHaL. By analyzing existing home hacks shared online, we create a design abstraction for connecting household items using predefined connection types. We also provide a UI for designing hack assemblies that fulfill a given specification. FabHacks leverages a physics-based solver that finds the expected physical configuration of an assembly design. Our validation includes a user study with our UI, which shows that users can easily create assemblies and explore a range of designs.},
booktitle = {Proceedings of the 9th ACM Symposium on Computational Fabrication},
articleno = {4},
numpages = {16},
keywords = {domain-specific languages, fabrication, sustainability},
location = {Aarhus, Denmark},
series = {SCF '24}
}

@inproceedings{10.1145/3711896.3737222,
author = {Wang, Jiang and Dong, Zhengxin and Bai, Bing and Jiang, Guyu and Yuan, Aiquan and Cao, Guodong},
title = {FoodGPT: Reinforcement Post-Training of Large Language Models in the Food Delivery Domain},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737222},
doi = {10.1145/3711896.3737222},
abstract = {On-demand Food Delivery (OFD) platforms, such as Ele.me and Meituan, have transformed daily life by offering convenient ordering services. However, challenges remain in understanding user intentions and processing product-related text information. Existing NLP models, while advanced in general tasks, are less effective for OFD-specific needs due to data scarcity and high computational costs. This paper introduces FoodInstruct, a Chinese dataset with 1.6 million examples across 12 OFD-related NLP tasks, and FoodGPT, a domain-specific large language model. We propose an efficient reinforcement post-training framework that combines Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Proximal Policy Optimization (PPO) with an additional rule-based reward signal. The resulting foundation model, FoodGPT, enhances model performance while minimizing resource consumption. Experimental results demonstrate that FoodGPT outperforms general models, such as Qwen 2.5, Llama3.1 and DeepSeek-LLM, on OFD tasks, using fewer data and training iterations. The model has already been deployed across numerous applications within the company. For instance, it achieved a 0.57\% increase in click-through rate and a 0.32\% increase in user visit-to-purchase rate in the ITG online experiment. The core contributions are now publicly accessible at https://huggingface.co/elemenlp.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {4966–4974},
numpages = {9},
keywords = {artificial intelligence, deep learning, large language models, natural language processing, on-demand food delivery},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3686038.3686044,
author = {Tang, Yun and Moffat, Luke and Guo, Weisi and May-Chahal, Corinne and Deville, Joe and Tsourdos, Antonios},
title = {Encoding Social \&amp; Ethical Values in Autonomous Navigation: Philosophies Behind an Interactive Online Demonstration},
year = {2024},
isbn = {9798400709890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686038.3686044},
doi = {10.1145/3686038.3686044},
abstract = {Autonomous Systems (ASs) interacting with human societies raises complex social \&amp; ethical challenges. This paper argues that one way of scaffolding human trust in ASs is through the encoding of ethical, legal and social impact (ELSI) considerations in the ASs’ decision-making processes. Existing ELSI-encoding efforts often focus on the implementation of rule-based and risk-based approaches, leaving key questions unanswered - what are the relationships between ELSI-encoding software logic in ASs and human ethical practises; what ethical approaches cannot be easily translated into software rules and numeric risks; and what are the implications of this for ethical AS? To answer these questions, we review and discuss different ELSI-encoding approaches in ASs from a new perspective, i.e., their relationships with classic human ethics philosophies. We also explore the feasibility of large language models (LLMs)-based ELSI-encoding practices in overcoming the limitations of rule-based and risk-based approaches and the associated challenges. To foster understanding, facilitate knowledge exchange and inspire discussion among cross-disciplinary research communities, we build and publish the first online interactive playground demonstrating different ELSI-encoding approaches on the same AS decision-making process. We welcome feedback and contributions in making this platform truly beneficial to trustworthy autonomous system research communities.},
booktitle = {Proceedings of the Second International Symposium on Trustworthy Autonomous Systems},
articleno = {21},
numpages = {9},
keywords = {Demonstration, ELSI-encoding, Large Language Model, Trustworthy Autonomous Systems},
location = {Austin, TX, USA},
series = {TAS '24}
}

@inproceedings{10.1145/3652620.3688335,
author = {Sedrakyan, Gayane and Iacob, Maria-Eugenia and Van Hillegersberg, Jos},
title = {Towards LowDevSecOps Framework for Low-Code Development: Integrating Process-Oriented Recommendations for Security Risk Management},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688335},
doi = {10.1145/3652620.3688335},
abstract = {The increasing demand for software solutions in the coming years will surpass the availability of IT talent, driving interest in citizen development and low-code approaches. However, the lack of technical insight among citizen developers poses potential security risks. This research aims to support businesses adopting citizen development by providing a framework that helps to proactively identify security risks by also linking them to specific actors and tools needed during the system design and development process to mitigate those risks. Additionally, this framework helps to address knowledge gaps by outlining actionable steps to ensure secure low-code development practices. The research aims to answer the question: "How can contextual information be modeled in low-code platforms to proactively identify and address security-related issues, acting as a virtual mentor for citizen / low-code developers?". To answer this question, our research conceptualizes security risks from established frameworks and operational security methodologies into a practical framework that allows mapping security risks to the context of low-code development. This framework serves as a foundational platform for designing and integrating active process-oriented guidance within low-code platforms using model-based automated prompts. This approach additionally aligns with DevSecOps principles that allows enhancing the capacity for low-code approach and citizen development in areas that currently may include manual coding and integrations.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {886–894},
numpages = {9},
keywords = {low code development, modeling, recommenders, security, devops, devsecops},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inbook{10.1145/3672608.3707984,
author = {Xia, Yuan and Pingle, Aabha and Sur, Deepayan and Deshmukh, Jyotirmoy and Raghothaman, Mukund and Ravi, Srivatsan},
title = {LLM-guided Predicate Discovery and Data Augmentation for Learning Likely Program Invariants},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707984},
abstract = {Security protocols, protocols to achieve consensus, those for maintaining memory consistency and coherence, distributed ledgers, multi-party computation, and many similar software systems are examples of distributed message-passing based computation. Ensuring correctness of such distributed systems is a challenging problem for many automatic verification approaches. The deductive verification approach for reasoning about such systems involves computing a program invariant, i.e., an expression evaluates to true for every reachable program state. Several approaches for synthesizing invariants are dynamic, i.e., runs of the program and ancillary information such as target safety properties are used to learn an invariant expression. However, most existing approaches invoke a model checker (or a theorem prover) within the synthesis loop, which makes these approaches depend on the scalability of the verification tools. In this paper, we propose a counterexample-guided inductive synthesis approach called RunVS which learns invariant expressions from program runs, but without information such as target safety properties, and without invoking a model checker/theorem prover for validation. The synthesis approach pairs a decision-tree (DT) based method with a data augmentation technique: DT-learning provides an expression that classifies observed states from augmented states that are speculated to be unreachable. Validation of the learned invariant is performed by sampling program runs and states; any run that invalidates the invariant results in counterexamples used to revises the invariant. As there is no formal proof that the learned artifact is a true invariant, we call such an expression a likely invariant. An important user input to synthesis is often the set of predicates that comprise the invariant expression; we use a novel integration with a large language model (LLM) and prompt it to provide likely predicates to be used. We show empirical results of our approach on several distributed protocols implemented in the Promela modeling language.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1721–1729},
numpages = {9}
}

@inproceedings{10.1145/3640794.3665538,
author = {S\'{a}nchez Cuadrado, Jes\'{u}s and P\'{e}rez-Soler, Sara and Guerra, Esther and De Lara, Juan},
title = {Automating the Development of Task-oriented LLM-based Chatbots},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665538},
doi = {10.1145/3640794.3665538},
abstract = {Task-oriented chatbots are increasingly used to access all sorts of services – like booking a flight, or setting a medical appointment – through natural language conversation. There are many technologies for implementing task-oriented chatbots, including Dialogflow, Watson, and Rasa. They rely on an explicit definition of the user intents, conversation flows, and chatbot outputs, which is costly to specify, and sometimes results in suboptimal user experiences and artificial conversations with limited diversity of chatbot responses. Recently, the advances in generative artificial intelligence fostered by Large Language Models (LLMs) have enabled a new range of open-domain chatbots, like ChatGPT, able to converse fluently on any topic. However, they are general-purpose, and therefore not directly usable to solve specialised tasks reliably. In this paper, we study the power of LLMs to build task-oriented chatbots, resulting in lighter specifications – no intent definition required – and more natural conversations than in intent-based approaches. To this end, we propose a lightweight domain-specific language based on YAML to specify chatbots using modules of different types (e.g., menus, question-answering, data gathering). These specifications are compiled into structured LLM prompts that use the ReAct framework to inform our runtime how to interpret the user input and coordinate the tasks that the chatbot must perform. The paper presents the design and realisation of our framework, and an assessment that encodes a set of existing intent-based chatbots using our approach, showing its benefits in terms of specification size, conversation flexibility and output diversity.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {11},
numpages = {10},
keywords = {Domain-Specific Languages, Large Language Models, Task-oriented Chatbots},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@inproceedings{10.1145/3729176.3729203,
author = {Drosos, Ian and Williams, Jack and Sarkar, Advait and Wilson, Nicholas and Rintel, Sean and Panda, Payod},
title = {Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks},
year = {2025},
isbn = {9798400713842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3729176.3729203},
doi = {10.1145/3729176.3729203},
abstract = {Prompting generative AI effectively is challenging for users, particularly in expressing context for comprehension tasks like explaining spreadsheet formulas, Python code, and text passages. Through a formative survey (n = 38), we uncovered a trade-off between standardized but predictable prompting support, and context-adaptive but unpredictable support. We explore this trade-off by implementing two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC), which generates UI elements for prompt refinement based on the user’s specific prompt, and Static Prompt Refinement Control (Static PRC), which offers generic controls. Our controlled user study (n = 16) showed that the Dynamic PRC approach afforded more control, lowered barriers to providing context, and encouraged task exploration and reflection, but reasoning about the effects of generated controls on the final output remains challenging. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows.},
booktitle = {Proceedings of the 4th Annual Symposium on Human-Computer Interaction for Work},
articleno = {24},
numpages = {23},
keywords = {Dynamic UX Generation, Prompt Middleware},
location = {
},
series = {CHIWORK '25}
}

@inproceedings{10.1145/3652620.3688206,
author = {Rabbi, Fazle},
title = {A Model-Based Framework for Exploring Conflict Dynamics},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688206},
doi = {10.1145/3652620.3688206},
abstract = {This paper introduces a novel framework for conflict analysis that leverages advanced visual modeling techniques. By employing comparative analysis, key variables influencing armed conflicts are identified and analyzed. The framework includes a meta-model representing domain concepts such as the goals and strategies of conflicting parties, escalating stages, and impacts of conflicts.Conflict escalation is a complex process characterized by interactions between opposing parties. This paper presents a structured model that outlines how conflicts evolve and intensify over time. We adapt a meta-modeling framework called the Diagram Predicate Framework (DPF) to represent conflict-related concepts and extend it to support abstract view generation. This framework facilitates the analysis of conflict trends and the study of dynamics across various levels of abstraction.A computational model based on category theory is proposed for trend analysis, enabling the extraction of patterns of conflict evolution and the comparison of strategies and goals at different escalation stages. Categorical operations such as pullback and limit construction are employed to compute conflict evolution and identify common structures among conflict instances, providing insights into conflict dynamics across diverse zones.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {745–754},
numpages = {10},
keywords = {conflict analysis, computational journalism, category theory, metamodeling},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3613904.3642517,
author = {Yang, Jackie (Junrui) and Shi, Yingtian and Zhang, Yuhan and Li, Karina and Rosli, Daniel Wan and Jain, Anisha and Zhang, Shuning and Li, Tianshi and Landay, James A. and Lam, Monica S.},
title = {ReactGenie: A Development Framework for Complex Multimodal Interactions Using Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642517},
doi = {10.1145/3613904.3642517},
abstract = {By combining voice and touch interactions, multimodal interfaces can surpass the efficiency of either modality alone. Traditional multimodal frameworks require laborious developer work to support rich multimodal commands where the user’s multimodal command involves possibly exponential combinations of actions/function invocations. This paper presents ReactGenie, a programming framework that better separates multimodal input from the computational model to enable developers to create efficient and capable multimodal interfaces with ease. ReactGenie translates multimodal user commands into NLPL (Natural Language Programming Language), a programming language we created, using a neural semantic parser based on large-language models. The ReactGenie runtime interprets the parsed NLPL and composes primitives in the computational model to implement complex user commands. As a result, ReactGenie allows easy implementation and unprecedented richness in commands for end-users of multimodal apps. Our evaluation showed that 12 developers can learn and build a non-trivial ReactGenie application in under 2.5 hours on average. In addition, compared with a traditional GUI, end-users can complete tasks faster and with less task load using ReactGenie apps.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {483},
numpages = {23},
keywords = {development frameworks, large-language model, multimodal interactions, natural language processing, programming framework},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3733155.3733192,
author = {Pinna, Simone and Massa, Silvia Maria and Fenu, Matteo and Casti, Giulio and Riboni, Daniele},
title = {Integration of Retrieval-Augmented Generation Technique for LLM-based Differential Diagnosis Assistant},
year = {2025},
isbn = {9798400714023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733155.3733192},
doi = {10.1145/3733155.3733192},
abstract = {Artificial Intelligence (AI) is increasingly transforming the medical field, offering significant potential for diagnosis, treatment, and patient care. However, its successful integration relies on healthcare professionals, such as doctors, psychologists, and nurses, trusting the technology’s reliability and accuracy. For Large Language Models (LLMs), this trust requires transparent, verifiable, and rigorously reviewed information sources. This paper presents an AI-powered tool for differential diagnosis and disease comparison, utilizing an LLM enhanced by Retrieval-Augmented Generation (RAG). RAG overcomes traditional LLM limitations by enabling access to external, domain-specific knowledge, ensuring accurate and contextually relevant responses. The system leverages PubMed, a biomedical article aggregator, to extract symptom-related information from scientific literature on various disorders. Evaluations involving psychologist-administered questionnaires demonstrate that combining a similarity score with detailed symptom descriptions provides a clear understanding of relationships between disorders. This approach may enhance diagnostic precision and build trust in AI-driven tools, encouraging their broader adoption in clinical practice.},
booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {277–284},
numpages = {8},
keywords = {Large Language Models, Retrieval-Augmented Generation, e-Health, Differential diagnosis},
location = {
},
series = {PETRA '25}
}

@inproceedings{10.1145/3652620.3687820,
author = {Moln\'{a}r, Vince and Graics, Bence and V\"{o}r\"{o}s, Andr\'{a}s and Tonetta, Stefano and Cristoforetti, Luca and Kimberly, Greg and Dyer, Pamela and Giammarco, Kristin and Koethe, Manfred and Hester, John and Smith, Jamie and Grimm, Christoph},
title = {Towards the Formal Verification of SysML v2 Models},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687820},
doi = {10.1145/3652620.3687820},
abstract = {Systems Modeling Language (SysML) is the de facto standard in the industry for modeling complex systems. SysML v2 is the new version of the language with reworked fundamentals. In this paper, we explore how the new formal semantics of SysML v2 can enable formal verification and various forms of automated reasoning. Formal verification involves mathematically proving the correctness of a system's design with respect to certain specifications or properties. This rigorous approach ensures that models behave as intended under all possible conditions. Through a detailed examination, we demonstrate how five specific tools - Gamma, MP-Firebird, Imandra, SAVVS, and SysMD - can formally analyze SysML v2 models. We show how these tools support the different concepts in the language, as well as the set of features and technologies they provide to users of SysML v2, such as model checking, theorem proving, contract-based design, or automatic fault injections. We propose a workflow for applying formal methods on SysML v2 models, illustrated by example models and artifacts generated by the above tools.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1086–1095},
numpages = {10},
keywords = {SysML V2, systems modeling, formal methods, verification and validation, automated reasoning, tools},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3723498.3723816,
author = {Shyne, Fiona and Cooper, Seth},
title = {Computational Tools for Table-Top Role-Playing Games: A Scoping Review},
year = {2025},
isbn = {9798400718564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723498.3723816},
doi = {10.1145/3723498.3723816},
abstract = {Table-top role-playing games (TTRPGs) are a form of gameplay that often requires a variety of complex tasks to be completed both in preparation and throughout gameplay: from tracking game state to the creation of fictional worlds. This has presented an opportunity for computational assistance in TTRPG sessions, both in the creation of artifacts and throughout the gameplay. We investigate the current research in computational tools for TTRPGs through a scoping review of academic works and present the major trends and opportunities from these works. We screened over one thousand works sourced from three different academic databases: ACM Digital Library, IEEE-Xplore, and Google Scholar. Papers were included based on relevance to TTRPGs, computational interface, and academic venue. In total, we evaluated 46 works in terms of produced artifacts, computational methods, evaluation, and outcomes. These papers include a diverse set of produced artifacts and computational methods, with an emphasis on tangible interfaces and generative AI systems. However, we found an opportunity for future work in terms of long-term studies, mixed-initiative methods, and different aspects of gameplay.},
booktitle = {Proceedings of the 20th International Conference on the Foundations of Digital Games},
articleno = {20},
numpages = {14},
keywords = {TTRPGs, Literature Review, Procedural Content Generation, Tangible Interface},
location = {
},
series = {FDG '25}
}

@inproceedings{10.1145/3706598.3714211,
author = {Tao, Sirui and Liang, Ivan and Peng, Cindy and Wang, Zhiqing and Palani, Srishti and Dow, Steven P.},
title = {DesignWeaver: Dimensional Scaffolding for Text-to-Image Product Design},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714211},
doi = {10.1145/3706598.3714211},
abstract = {Generative AI has enabled novice designers to quickly create professional-looking visual representations for product concepts. However, novices have limited domain knowledge that could constrain their ability to write prompts that effectively explore a product design space. To understand how experts explore and communicate about design spaces, we conducted a formative study with 12 experienced product designers and found that experts — and their less-versed clients — often use visual references to guide co-design discussions rather than written descriptions. These insights inspired DesignWeaver, an interface that helps novices generate prompts for a text-to-image model by surfacing key product design dimensions from generated images into a palette for quick selection. In a study with 52 novices, DesignWeaver enabled participants to craft longer prompts with more domain-specific vocabularies, resulting in more diverse, innovative product designs. However, the nuanced prompts heightened participants’ expectations beyond what current text-to-image models could deliver. We discuss implications for AI-based product design support tools.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {425},
numpages = {26},
keywords = {Creativity support tools, design ideation, idea management, human-AI interaction, text-to-image models},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3652620.3688197,
author = {Silva Mercado, Jonathan},
title = {AI Assisted Domain Modeling Explainability and Traceability},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688197},
doi = {10.1145/3652620.3688197},
abstract = {Domain Models are abstract representations of selected elements in a domain that is created in a collaborative process between domain and modeler experts. The participants share domain knowledge to conceptualize and reason about the elements that will create the domain models. Through this exchange, a comprehensive and accurate representation of the domain is achieved, ensuring that the model captures the relevant aspects and relationships in the domain. Research in Artificial Intelligence (AI) has explored various methods to assist in the creation of domain models from text using Natural Language Processing (NLP) and Machine Learning (ML). Recent advancements with Large Language Models (LLMs) have shown that it is possible to create domain models using prompting techniques; however, the generated domain models contain errors and remain constrained by the performance of the LLM used.Despite the impressive capabilities of LLMs to create domain models, it is evident that it does not address the needs of domain and modelers experts that participate in the creation of domain models. Every AI technique has its advantages and limitations that must be integrated with human feedback in a collaboration process. Therefore, we propose an approach that incorporates human-AI collaboration supported by AI assistants that follows a dialogue approach to understand the users needs and purpose to suggest relevant models. Our proposal combines symbolic and subsymbolic AI techniques with explainability and traceability of the decisions that assist to create domain models that are relevant for the users.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {130–135},
numpages = {6},
keywords = {domain modeling, large language models, uncertainty, explainability, traceability},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3687806,
author = {Burgue\~{n}o, Lola and Keet, Maria and Kienzle, J\"{o}rg and Michael, Judith and Babur, \"{O}nder},
title = {A Human Behavior Exploration Approach Using LLMs for Cyber-Physical Systems},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687806},
doi = {10.1145/3652620.3687806},
abstract = {In the early phases of Cyber-Physical Systems (CPS) development, scoping human behavior plays a significant role, especially when interactions extend beyond expected behavior. Here, it is especially challenging to develop cases that capture the full spectrum of human behavior. Up to now, identifying such behavior of humans remains a task for domain experts. We explore how one can use Large Languages Models (LLMs) in the design phase of systems to provide additional information about human-CPS interaction. Our approach proposes a preliminary ontology describing a hierarchy of types of behavior and relevant CPS components as input for prompt templates. It uses them to generate parts of human behavior descriptions, as well as a canned prompt with one variable about behavior. For demonstration, we take a smart building with a Home Energy System as the use case.An initial user evaluation shows that the behavior descriptions generated with standard and ontology-driven prompts complement each other and are useful when assisting humans. The discovered uncommon behaviors can be used to complete interaction scenarios that eventually result in a more robust CPS implementation.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {578–586},
numpages = {9},
keywords = {human behavior, large language models, cyber-physical systems, user scenario, digital twin},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3632913,
author = {Ding, Yuantian and Qiu, Xiaokang},
title = {Enhanced Enumeration Techniques for Syntax-Guided Synthesis of Bit-Vector Manipulations},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632913},
doi = {10.1145/3632913},
abstract = {Syntax-guided synthesis has been a prevalent theme in various computer-aided programming systems. However, the domain of bit-vector synthesis poses several unique challenges that have not yet been sufficiently addressed and resolved. In this paper, we propose a novel synthesis approach that incorporates a distinct enumeration strategy based on various factors. Technically, this approach weighs in subexpression recurrence by term-graph-based enumeration, avoids useless candidates by example-guided filtration, prioritizes valuable components identified by large language models. This approach also incorporates a bottom-up deduction step to enhance the enumeration algorithm by considering subproblems that contribute to the deductive resolution. We implement all the enhanced enumeration techniques in our SyGuS solver DryadSynth, which outperforms state-of-the-art solvers in terms of the number of solved problems, execution time, and solution size. Notably, DryadSynth successfully solved 31 synthesis problems for the first time, including 5 renowned Hacker’s Delight problems.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {71},
numpages = {31},
keywords = {Syntax-guided synthesis, Bit vector, Term graph, Enumeration, Large language model}
}

