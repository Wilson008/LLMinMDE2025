@inproceedings{10.1145/3696630.3728550,
author = {Wang, Yong and Ge, Ning and Li, Jingyao and Wang, Loulin and Zhou, Guangyu and Deng, Chengrui and Zhang, Li and Hu, Chunming},
title = {SemServGen: Advancing Industrial Domain-Specific Language Engineering through Semantic Service Generation},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728550},
doi = {10.1145/3696630.3728550},
abstract = {Domain-Specific Languages (DSLs) are widely used across industries for addressing challenges within specific domains. However, designing a DSL is just the first step in DSL engineering. To truly enhance development efficiency and system quality, comprehensive syntax and semantic services are essential. Currently, semantic services are primarily implemented manually, leading to high development costs. Automating their generation can significantly improve efficiency and reduce costs in DSL projects. This paper presents SemServGen, a framework that defines a structured approach to DSL semantic service development. SemServGen comprises SemaDSL, a unified semantic service expression language; SemServ-editor, which provides editing, syntax, semantic, template, and DSL-binding services; and SemServ-gen, an automated semantic service generator. SemaDSL enables the specification of both generic and domain-specific semantic services, while SemServ-gen translates these formal specifications into executable semantic services. We evaluated SemServGen with industry partners using CRL and SemaDSL as target DSLs. The results demonstrate that SemaDSL effectively models semantic service domain concepts with high usability, while SemServGen increases development efficiency by 2.5 times for CRL and 1.9 times for SemaDSL compared to manual implementation. Additionally, the generated semantic services meet industry-standard performance benchmarks for service analysis time.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {250–261},
numpages = {12},
keywords = {domain-specific language engineering, semantic service, semantic specification language, code generation},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3696630.3728568,
author = {Wynn-Williams, Stephen and Tyrrell, Ryan and Pantelic, Vera and Lawford, Mark and Menghi, Claudio and Nalla, Phaneendra and Artail, Hassan},
title = {Can Generative AI Produce Test Cases? An Experience from the Automotive Domain},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728568},
doi = {10.1145/3696630.3728568},
abstract = {Engineers need automated support for software testing. Generative AI is a novel technology for generating new content; however, its applicability for test case generation is still unclear. This work considers the following question: Can generative AI produce test cases in industrial software applications? We framed our question in the automotive domain. We performed our evaluation in collaboration with a large automotive manufacturer to assess to what extent generative AI can produce test cases (a.k.a. test scripts) from informal test case specifications. We considered 1) informal test case specifications defined in Rational Quality Manager, an industrial test management tool from IBM, and 2) executable test scripts specified as ecu.test packages supported by the ecu.test tool from Tracetronic. We used generative AI to produce the test scripts from the informal test case descriptions. Our results show that generative AI can produce correct or near-correct test scripts in a reasonable number of cases. We also analyzed the effects of prompt design, choice of generative AI model, and context accuracy on the effectiveness of our solution and reflected on our results.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {456–467},
numpages = {12},
keywords = {software testing, LLM, generative AI, automotive software},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@article{10.1145/3729356,
author = {Li, Yue and Liu, Bohan and Zhang, Ting and Wang, Zhiqi and Lo, David and Yang, Lanxin and Lyu, Jun and Zhang, He},
title = {A Knowledge Enhanced Large Language Model for Bug Localization},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729356},
doi = {10.1145/3729356},
abstract = {A significant number of bug reports are generated every day as software systems continue to develop. Large Language Models (LLMs) have been used to correlate bug reports with source code to locate bugs automatically. The existing research has shown that LLMs are effective for bug localization and can increase software development efficiency. However, these studies still have two limitations. First, these models fail to capture context information about bug reports and source code. Second, these models are unable to understand the domain-specific expertise inherent to particular projects, such as version information in projects that are composed of alphanumeric characters without any semantic meaning. To address these challenges, we propose a Knowledge Enhanced Pre-Trained model using project documents and historical code, called KEPT, for bug localization. Project documents record, revise, and restate project information that provides rich semantic information about those projects. Historical code contains rich code semantic information that can enhance the reasoning ability of LLMs. Specifically, we construct knowledge graphs from project documents and source code. Then, we introduce knowledge graphs to the LLM through soft-position embedding and visible matrices, enhancing its contextual and professional reasoning ability. To validate our model, we conducted a series of experiments on seven open-source software projects with over 6,000 bug reports. Compared with the traditional model (Locus), KEPT performs better by 33.2\% to 59.5\% in terms of mean reciprocal rank, mean average precision, and Top@N. Compared with the best-performing non-commercial LLM (CodeT5), KEPT achieves an improvement of 36.6\% to 63.7\%. Compared to the state-of-the-art commercial LLM developed by OpenAI, called text-embedding-ada-002, KEPT achieves an average improvement of 7.8\% to 17.4\%. The results indicate that introducing knowledge graphs contributes to enhance the effectiveness of the LLM in bug localization.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE086},
numpages = {23},
keywords = {bug localization, information retrieval, knowledge enhancement, large language model}
}

@inproceedings{10.1145/3660605.3660944,
author = {Schneider, Nadav and Hasabnis, Niranjan and Vo, Vy A. and Kadosh, Tal and Krien, Neva and Capota, Mihai and Tamir, Guy and Willke, Theodore L. and Ahmed, Nesreen and Pinter, Yuval and Mattson, Timothy and Oren, Gal},
title = {MPIrigen: MPI Code Generation through Domain-Specific Language Models},
year = {2024},
isbn = {9798400706523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660605.3660944},
doi = {10.1145/3660605.3660944},
abstract = {The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. While MPI serves as a cornerstone for large-scale parallelism, its seamless integration into codebases, especially concerning domain decomposition, has proven challenging. Static tools aimed at addressing this challenge have exhibited limited effectiveness and scalability. On the other hand, contemporary language models designed for programming problems have demonstrated utility in parallel programming tasks such as OpenMP pragma generation. However, the challenging parallel programming task of generating MPI-based parallel programs has remained unexplored.This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pre-trained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose an innovative preprocessing for completion only after observing the whole code, thus enabling better completion with a wider context. Comparative analysis against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation method, demonstrates that MPIrigen excels in generating accurate MPI functions calls. The success of this tailored solution underscores the importance of domain-specific fine-tuning in optimizing language models for parallel computing code generation, paving the way for a new generation of automatic parallelization tools.The sources of this work are available at our GitHub MPIrigen repository.},
booktitle = {Proceedings of the 2024 Workshop on AI For Systems},
pages = {1–6},
numpages = {6},
keywords = {MPI, domain decomposition, transformer, LLM, AI, code generation},
location = {Pisa, Italy},
series = {AI4Sys '24}
}

@inproceedings{10.1145/3706598.3714135,
author = {Chen, Wei-Hao and Tong, Weixi and Case, Amanda, Ph.D. and Zhang, Tianyi},
title = {Dango: A Mixed-Initiative Data Wrangling System using Large Language Model},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714135},
doi = {10.1145/3706598.3714135},
abstract = {Data wrangling is a time-consuming and challenging task in a data science pipeline. While many tools have been proposed to automate or facilitate data wrangling, they often misinterpret user intent, especially in complex tasks. We propose Dango, a mixed-initiative multi-agent system for data wrangling. Compared to existing tools, Dango enhances user communication of intent by: (1) allowing users to demonstrate on multiple tables and use natural language prompts in a conversation interface, (2) enabling users to clarify their intent by answering LLM-posed multiple-choice clarification questions, and (3) providing multiple forms of feedback such as step-by-step NL explanations and data provenance to help users evaluate the data wrangling scripts. We conducted a within-subjects user study (n=38) and demonstrated that Dango’s features can significantly improve intent clarification, accuracy, and efficiency in data wrangling. Furthermore, we demonstrated the generalizability of Dango by applying it to a broader set of data wrangling tasks.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {389},
numpages = {28},
keywords = {Data Wrangling, Data Science, Large Language Model},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3652620.3687773,
author = {Bucchiarone, Antonio and Cicchetti, Antonio and V\'{a}zquez-Ingelmo, Andrea and Adami, Filippo and Schiavo, Gianluca and Garc\'{\i}a-Holgado, Alicia and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e}},
title = {Designing and Generating Lesson Plans combining Open Educational Content and Generative AI},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687773},
doi = {10.1145/3652620.3687773},
abstract = {In this paper, we propose an approach for assisting educators in deriving lesson plans for complex learning subjects like Model-Driven Engineering (MDE) from existing educational materials, leveraging generative AI techniques. Our method focuses on guiding teachers in defining learning objectives and suggesting concrete learning activities for students. Central to our approach is the development of a metamodel that characterizes the methodology and serves as the foundation for implementing supporting tools. By utilizing available Open Educational Resources (OERs) and incorporating them into specific learning activities, our method provides a general framework for supporting educators in designing lesson plans. We present the methodology to generate lesson plans, the metamodel conceptualizing plans ingredients, and demonstrate their application through supporting tools, illustrating the potential of our approach in facilitating the development of MDE teaching materials.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {78–86},
numpages = {9},
keywords = {open educational resources, OERs, model-driven engineering, MDE, generative AI, educational paradigms, tailored learning activities, customizable learning content},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3641399.3641403,
author = {Rajbhoj, Asha and Somase, Akanksha and Kulkarni, Piyush and Kulkarni, Vinay},
title = {Accelerating Software Development Using Generative AI: ChatGPT Case Study},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641399.3641403},
doi = {10.1145/3641399.3641403},
abstract = {The Software Development Life Cycle (SDLC) comprises multiple phases, each requiring Subject Matter Experts (SMEs) with phase-specific skills. The efficacy and quality of deliverables of each phase are skill dependent. In recent times, Generative AI techniques, including Large-scale Language Models (LLMs) like GPT, have become significant players in software engineering. These models, trained on extensive text data, can offer valuable contributions to software development. Interacting with LLMs involves feeding prompts with the context information and guiding the generation of textual responses. The quality of the response is dependent on the quality of the prompt given. This paper proposes a systematic prompting approach based on meta-model concepts for SDLC phases. The approach is validated using ChatGPT for small but complex business application development. We share the approach and our experience, learnings, benefits obtained, and the challenges encountered while applying the approach using ChatGPT. Our experience indicates that Generative AI techniques, such as ChatGPT, have the potential to reduce the skills barrier and accelerate software development substantially.},
booktitle = {Proceedings of the 17th Innovations in Software Engineering Conference},
articleno = {5},
numpages = {11},
keywords = {AI in SDLC, Automated Software Development, ChatGPT, Generative AI, Large Language Models, SDLC automation},
location = {Bangalore, India},
series = {ISEC '24}
}

@inproceedings{10.1145/3560071.3560084,
author = {Cao, Yiling and Fang, Lu and Zheng, Zhongguang},
title = {Enriching Pre-Trained Language Model with Multi-Task Learning and Context for Medical Concept Normalization},
year = {2022},
isbn = {9781450397087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560071.3560084},
doi = {10.1145/3560071.3560084},
abstract = {Herein, we focus on the problem of automatically medical concept normalization in social media posts. Specifically, the task is to map medical mentions within social media texts to the suitable concepts in a reference knowledge base. We propose a new medical concept normalization model using multi-task learning. The model uses BioBERT to encode mentions and their contexts, and classifies their concept IDs and types of mention. We evaluate our approach on two datasets and achieve new state-of-the-art performance.},
booktitle = {Proceedings of the 2022 International Conference on Intelligent Medicine and Health},
pages = {79–83},
numpages = {5},
keywords = {Multi-task learning, Medical concept normalization, Context information, BioBERT},
location = {Xiamen, China},
series = {ICIMH '22}
}

@article{10.1145/3704905,
author = {Cai, Yufan and Hou, Zhe and Sanan, David and Luan, Xiaokun and Lin, Yun and Sun, Jun and Dong, Jin Song},
title = {Automated Program Refinement: Guide and Verify Code Large Language Model with Refinement Calculus},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {POPL},
url = {https://doi.org/10.1145/3704905},
doi = {10.1145/3704905},
abstract = {Recently, the rise of code-centric Large Language Models (LLMs) has reshaped the software engineering world with low-barrier tools like Copilot that can easily generate code. However, there is no correctness guarantee for the code generated by LLMs, which suffer from the hallucination problem, and their output is fraught with risks. Besides, the end-to-end process from specification to code through LLMs is a non-transparent and uncontrolled black box. This opacity makes it difficult for users to understand and trust the generated code. Addressing these challenges is both necessary and critical. In contrast, program refinement transforms high-level specification statements into executable code while preserving correctness. Traditional tools for program refinement are primarily designed for formal methods experts and lack automation and extensibility. We apply program refinement to guide LLM and validate the LLM-generated code while transforming refinement into a more accessible and flexible framework.                To initiate this vision, we propose Refine4LLM, an approach that aims to:                (1) Formally refine the specifications,                (2) Automatically prompt and guide the LLM using refinement calculus,                (3) Interact with the LLM to generate the code,                (4) Verify that the generated code satisfies the constraints, thus guaranteeing its correctness,                (5) Learn and build more advanced refinement laws to extend the refinement calculus.                We evaluated Refine4LLM against the state-of-the-art baselines on program refinement and LLMs benchmarks.The experiment results show that Refine4LLM can efficiently generate more robust code and reduce the time for refinement and verification.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {69},
numpages = {33},
keywords = {Large Language Model, Program Refinement, Program Synthesis}
}

@article{10.1145/3728878,
author = {Yu, Lei and Huang, Zhirong and Yuan, Hang and Cheng, Shiqi and Yang, Li and Zhang, Fengjun and Shen, Chenjie and Ma, Jiajia and Zhang, Jingyuan and Lu, Junyi and Zuo, Chun},
title = {Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728878},
doi = {10.1145/3728878},
abstract = {Smart contract vulnerability detection is a critical challenge in the rapidly evolving blockchain landscape. Existing vulnerability detection methods face two main issues: (1) Existing datasets lack comprehensiveness and sufficient quality, with limited vulnerability type coverage and insufficient distinction between high-quality and low-quality explanations for preference learning. (2) Large language models (LLMs) often struggle with accurately interpreting specific concepts in smart contract security. Through our empirical analysis, we found that even after continual pre-training and supervised fine-tuning, LLMs still exhibit limitations in precisely understanding the execution order of state changes in smart contracts, which can lead to incorrect vulnerability explanations despite making correct detection decisions. These limitations result in poor detection performance, leading to potentially severe financial losses. To address these challenges, we propose Smart-LLaMA-DPO, an advanced detection method based on the LLaMA-3.1-8B. First, we construct a comprehensive dataset covering four vulnerability types and machine-unauditable vulnerabilities, containing labels, detailed explanations, and precise vulnerability locations for Supervised Fine-Tuning (SFT), as well as paired high-quality and low-quality outputs for Direct Preference Optimization (DPO). Second, we perform continual pre-training using large-scale smart contract code to enhance the LLM's understanding of specific security practices in smart contracts. Futhermore, we conduct supervised fine-tuning with our comprehensive dataset. Finally, we apply DPO, which leverages human feedback to improve the quality of generated explanations. Smart-LLaMA-DPO utilizes a specially designed loss function that encourages the LLM to increase the probability of preferred outputs while decreasing the probability of non-preferred outputs, thereby enhancing the LLM's ability to generate high-quality explanations. We evaluate Smart-LLaMA-DPO on four major vulnerability types: reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall, as well as machine-unauditable vulnerabilities. Our method significantly outperforms state-of-the-art baselines, with average improvements of 10.43\% in F1 score and 7.87\% in accuracy. Moreover, both LLM evaluation and human evaluation demonstrate the superior quality of explanations generated by Smart-LLaMA-DPO in terms of correctness, thoroughness, and clarity.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA009},
numpages = {24},
keywords = {Direct Preference Optimization, Large Language Models, Smart Contract}
}

@inproceedings{10.5555/3712729.3712990,
author = {Leathrum, James F. and Shen, Yuzhong and Sosonkina, Masha},
title = {Investigating the Use of Generative AI in M&amp;S Education},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {Large Language Models (LLMs) are rapidly creating a place for themselves in society. There are numerous reports, both good and bad, of their use in business, academia, government and society. While some organizations are trying to limit, or eliminate, their use, it appears that it is inevitable they will become a common "tool". In education, there is a fear that students will not acquire critical thinking in the future, but we argue that LLMs will become a tool to assist students with critical thinking, giving guidance, feedback, and assessment. This paper investigates how the current state of LLMs can be integrated into modeling and simulation (M&amp;S) education. Example cases for modeling and simulation development are presented showing how an LLM can assist M&amp;S design and education in anticipation of LLMs becoming a common tool for M&amp;S practitioners. Current limitations are also highlighted, and where possible, short-term solutions are proposed.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3142–3153},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3659677.3659736,
author = {Amri, Samir and Bani, Rkia and Bani, Saida},
title = {An Approach to the Analysis of Financial Documents Using Generative AI},
year = {2024},
isbn = {9798400709296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659677.3659736},
doi = {10.1145/3659677.3659736},
abstract = {This project tackles the challenge of advancing document analysis with generative AI techniques. It explores two main approaches:• Fine-Tuning and Retrieval Augmented Generation (RAG).• Fine-Tuning: This approach utilizes the BART model in conjunction with a specialized vector database. The Fine-Tuning phase involves a comprehensive process of data acquisition, cleaning, and processing. This phase provides valuable insights into the challenges and considerations involved in document analysis using Fine-Tuning.Retrieval Augmented Generation (RAG): This novel method leverages generative AI for contextual understanding and response generation. The RAG section delves into the objectives, methodology, and results achieved with this cutting-edge approach.A comparative analysis is then conducted to shed light on the distinct contributions of Fine-Tuning and RAG to document analysis. Furthermore, the project extends beyond AI models by developing an interactive User Interface (UI). This UI utilizes various technologies to ensure a seamless user experience. Key features include functionalities for file upload, error handling, responsive design, and smooth integration with the backend system.},
booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
articleno = {31},
numpages = {5},
location = {Meknes, AA, Morocco},
series = {NISS '24}
}

@inproceedings{10.1145/3640310.3674081,
author = {Jahan, Munima and Hassan, Mohammad Mahdi and Golpayegani, Reza and Ranjbaran, Golshid and Roy, Chanchal and Roy, Banani and Schneider, Kevin},
title = {Automated Derivation of UML Sequence Diagrams from User Stories: Unleashing the Power of Generative AI vs. a Rule-Based Approach},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674081},
doi = {10.1145/3640310.3674081},
abstract = {User stories are informal, non-technical descriptions of features from a user's perspective that guide collaboration and iterative development in Agile projects. However, ambiguities in user stories can lead to miscommunication among stakeholders. Design models, such as UML sequence diagrams, are essential for enhancing communication, clarifying system behavior, and improving the development process. This paper presents an automated approach for generating behavioral models specifically sequence diagrams from natural language requirements expressed as user stories. We also investigate the effectiveness of a Large Language Model (LLM) in using generative AI for this task. By applying our approach and ChatGPT to two benchmark datasets with the same set of user stories, we generated corresponding sequence diagrams for comparison. Expert evaluations in Software Engineering reveal that our approach effectively produces relevant, simplified diagrams for straightforward user stories, whereas the LLM tends to create more complex diagrams that sometimes go beyond the simplicity of the original user stories.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {138–148},
numpages = {11},
keywords = {Generative Model, Large Language Model, Model Generation, Natural Language Processing, Rule-based approach, Sequence Diagram, User Story},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3675249.3675253,
author = {Li, Peiyan and Liu, Xiaomeng and Wang, Yongxing},
title = {A Novel Method based on Large Language Model for MBTI Classification: A Novel MBTI Classification Method},
year = {2024},
isbn = {9798400718267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675249.3675253},
doi = {10.1145/3675249.3675253},
abstract = {Personality analysis has a wide and important application in psychology, helping us explain various psychological phenomena and the developmental laws of personality. Accurately identifying personality will contribute to research in fields such as cognitive science, public opinion analysis, and cybersecurity. The most widely used models in the field of personality recognition are the Big Five personality model and MBTI model in trait genre. With the rise of social media and Large Language Model(LLM), massive corpora and deep learning models have been used for personality analysis and have achieved good results. In view of this, this article introduces a novel method based on transformer-based pre-trained language model named mDeBERTa and uses the MBTI-500 and MBTI-1 datasets as examples to demonstrate the advantages of this model over traditional SVM models and BERT models. At the same time, we explore the possibility of using large models and multimodal data for more accurate personality analysis.},
booktitle = {Proceedings of the 2024 International Conference on Computer and Multimedia Technology},
pages = {11–16},
numpages = {6},
location = {Sanming, China},
series = {ICCMT '24}
}

@inbook{10.5555/3712729.3712960,
author = {Carreira-Munich, Tobias and Paz-Marcolla, Valent\'{\i}n and Castro, Rodrigo},
title = {DEVS Copilot: Towards Generative AI-Assisted Formal Simulation Modelling Based on Large Language Models},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {In this paper we explore to which extent generative AI, in the form of Large Language Models such as GPT-4, can assist in obtaining a correct executable simulation model. The starting point is a high-level description of a system, expressed in natural language, which evolves through a conversational process based on user input, including suggestions for corrections. We introduce a methodology and a tool inspired by the metaphor of a copilot, a form of human-AI teaming strategy well known for its success in programming tasks. We adopt the Discrete Event System Specification (DEVS), a suitable candidate formalism that allows general-purpose simulation models to be specified in a simple yet rigorous modular and hierarchical way. The result is DEVS Copilot, an AI-based prototype that we systematically test in a case study that builds several lighting control systems of increasing complexity. In all cases, DEVS Copilot succeeds at producing correct DEVS simulations.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2785–2796},
numpages = {12}
}

@inproceedings{10.1145/3613904.3642937,
author = {Kim, Taewan and Bae, Seolyeong and Kim, Hyun Ah and Lee, Su-Woo and Hong, Hwajung and Yang, Chanmo and Kim, Young-Ho},
title = {MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients' Journaling},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642937},
doi = {10.1145/3613904.3642937},
abstract = {Large Language Models (LLMs) offer promising opportunities in mental health domains, although their inherent complexity and low controllability elicit concern regarding their applicability in clinical settings. We present MindfulDiary, an LLM-driven journaling app that helps psychiatric patients document daily experiences through conversation. Designed in collaboration with mental health professionals, MindfulDiary takes a state-based approach to safely comply with the experts’ guidelines while carrying on free-form conversations. Through a four-week field study involving 28 patients with major depressive disorder and five psychiatrists, we examined how MindfulDiary facilitates patients’ journaling practice and clinical care. The study revealed that MindfulDiary supported patients in consistently enriching their daily records and helped clinicians better empathize with their patients through an understanding of their thoughts and daily contexts. Drawing on these findings, we discuss the implications of leveraging LLMs in the mental health domain, bridging the technical feasibility and their integration into clinical settings.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {701},
numpages = {20},
keywords = {chatbot, clinical setting, journaling, large language models, mental health, psychiatric patient},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3652620.3687804,
author = {Birchler De Allende, Alan and Sultan, Bastien and Apvrille, Ludovic},
title = {From Attack Trees to Attack-Defense Trees with Generative AI \&amp; Natural Language Processing},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687804},
doi = {10.1145/3652620.3687804},
abstract = {Attack-defense trees, an extension of attack trees, are extensively used by security engineers to document potential countermeasures for security threats present in a system's design. These trees help integrate initial system models with countermeasures, allowing for early testing of their efficiency and impact in the design cycle. Despite advancements in automating attack tree construction, selecting the initial set of countermeasures for conversion into an attack-defense tree remains largely manual. This paper proposes an approach and a tool that extends the TTool-AI attack tree generation feature by leveraging large language models and natural language processing to create a set of countermeasures and generate attack-defense trees based on an input attack tree. To evaluate our contribution, our approach is tested using attack-defense trees generated from attack trees, each representing possible threats to an associated system specification. In addition, we introduce metrics to assess the semantic correctness and completeness of the generated attack-defense trees. We compared, using our metrics, the attack-defense trees created from our methodology to those created by an engineer and found that attack-defense trees created using AI and secondary mitigation data provided better trees than solely using AI. We also discovered that this approach generated trees that were comparable to the quality of attack-defense trees generated from a security engineer at the associate level. From these results, we believe that our contribution could aid engineers in identifying not only appropriate countermeasures for attack trees but also the optimal number of countermeasures, avoiding the complexity of redundant mitigations. Furthermore, our approach complements standard modeling practices, particularly during the initial design phase, reducing the need for time-consuming re-engineering throughout the system's lifecycle.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {561–569},
numpages = {9},
keywords = {artificial intelligence, large-language models, attack-defense trees, model-driven engineering},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3632410.3632465,
author = {Jain, Ayush and Padmanaban, Manikandan and Hazra, Jagabondhu and Godbole, Shantanu and Weldemariam, Komminist},
title = {A Framework for Emission Reduction in Scope 3 Climate Actions using Domain-adapted Foundation Model},
year = {2024},
isbn = {9798400716348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632410.3632465},
doi = {10.1145/3632410.3632465},
abstract = {Large enterprises face a crucial imperative to achieve the Sustainable Development Goals (SDGs), especially goal 13, which focuses on combating climate change and its impacts. To mitigate the effects of climate change, reducing enterprise Scope 3 (supply chain emissions) is vital, as it accounts for more than 90\% of total emission inventories. However, tracking Scope 3 emissions proves challenging, as data must be collected from thousands of upstream and downstream suppliers. We propose a novel framework that uses domain-adapted NLP foundation model to estimate Scope 3 emissions by leveraging financial transactions as a proxy of embodied emission of purchased goods and services and recommends appropriate climate actions to reduce scope3 emission through counterfactual queries. Our results show that the domain-adapted foundation model outperforms state-of-the-art text mining techniques and performs as well as a subject matter expert (SME). We also show how the proposed framework can identify Scope 3 hotspots and explain the factors that create them. Finally, we carry out what-if analysis to take climate actions that help achieve SDG 13. We present a case study demonstrating how this framework can be used by a real estate enterprise to take Scope 3 climate actions.},
booktitle = {Proceedings of the 7th Joint International Conference on Data Science \&amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)},
pages = {316–324},
numpages = {9},
keywords = {EEIO, Foundation model, Recommendation, Scope 3 emissions},
location = {Bangalore, India},
series = {CODS-COMAD '24}
}

@article{10.14778/3611540.3611624,
author = {Chen, Zui and Cao, Lei and Madden, Sam},
title = {Lingua Manga : A Generic Large Language Model Centric System for Data Curation},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611624},
doi = {10.14778/3611540.3611624},
abstract = {Data curation is a wide-ranging area which contains many critical but time-consuming data processing tasks. However, the diversity of such tasks makes it challenging to develop a general-purpose data curation system. To address this issue, we present Lingua Manga, a user-friendly and versatile system that utilizes pre-trained large language models. Lingua Manga offers automatic optimization for achieving high performance and label efficiency while facilitating flexible and rapid development. Through three example applications with distinct objectives and users of varying levels of technical proficiency, we demonstrate that Lingua Manga can effectively assist both skilled programmers and low-code or even no-code users in addressing data curation challenges.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4074–4077},
numpages = {4}
}

@inproceedings{10.1145/3613904.3642492,
author = {Newman, Michele and Sun, Kaiwen and Dalla Gasperina, Ilena B and Shin, Grace Y. and Pedraja, Matthew Kyle and Kanchi, Ritesh and Song, Maia B. and Li, Rannie and Lee, Jin Ha and Yip, Jason},
title = {"I want it to talk like Darth Vader": Helping Children Construct Creative Self-Efficacy with Generative AI},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642492},
doi = {10.1145/3613904.3642492},
abstract = {The emergence of generative artificial intelligence (GenAI) has ignited discussions surrounding its potential to enhance creative pursuits. However, distinctions between children’s and adult’s creative needs exist, which is important when considering the possibility of GenAI for children’s creative usage. Building upon work in Human-Computer Interaction (HCI), fostering children’s computational thinking skills, this study explores interactions between children (aged 7-13) and GenAI tools through methods of participatory design. We seek to answer two questions: (1) How do children in co-design workshops perceive GenAI tools and their usage for creative works? and (2) How do children navigate the creative process while using GenAI tools? How might these interactions support their confidence in their ability to create? Our findings contribute a model that describes the potential contexts underpinning child-GenAI creative interactions and explores implications of this model for theories of creativity, design, and use of GenAI as a constructionist tool for creative self-efficacy.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {117},
numpages = {18},
keywords = {Artificial Intelligence, Children, Co-Design, Constructionism, Creativity, Participatory Design},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3729434.3729456,
author = {B\"{o}hm, Karsten},
title = {Deep and Surface Representation of Competences in Academic Curricula: A new approach to address human consumers and formal structures using Generative AI},
year = {2025},
isbn = {9798400712630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3729434.3729456},
doi = {10.1145/3729434.3729456},
abstract = {The curricular design of programs in Higher Education is increasingly oriented towards competence-based learning goals in order to provide more specific and explicit qualifications. At the same time the descriptions of competences are mostly based on textual descriptions often in informal style targeted towards different audiences making descriptions vague and harder to compare. Formal models for the specification of competences exist for some time and even semantic models are being developed, e.g., the European Learning Model. These different approaches lead to a gap between formal and informal competence descriptions that require manual efforts of curriculum designer to maintain. This research borrows the concept of Deep and Surface representation models from the field of linguistics to unify the different approaches. Additionally, it focuses on the functionality of Generative Artificial Intelligence in the form of Large Language Models to close the aforementioned gap. It demonstrates the potential of the technology in both directions and discusses application potential of the concept.},
booktitle = {Proceedings of the 6th International Conference on Modern Educational Technology},
pages = {78–85},
numpages = {8},
keywords = {Curricular Design, European Learning Model, Generative Artificial Intelligence, Higher Education, LLM, Semantic Web},
location = {
},
series = {ICMET '24}
}

@inproceedings{10.1145/3640310.3674093,
author = {Morales, Sergio and Claris\'{o}, Robert and Cabot, Jordi},
title = {A DSL for Testing LLMs for Fairness and Bias},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674093},
doi = {10.1145/3640310.3674093},
abstract = {Large language models (LLMs) are increasingly integrated into software systems to enhance them with generative AI capabilities. But LLMs may reflect a biased behavior, resulting in systems that could discriminate against gender, age or ethnicity, among other ethical concerns. Society and upcoming regulations will force companies and development teams to ensure their AI-enhanced software is ethically fair. To facilitate such ethical assessment, we propose LangBiTe, a model-driven solution to specify ethical requirements, and customize and automate the testing of ethical biases in LLMs. The evaluation can raise awareness on the biases of the LLM-based components of the system and/or trigger a change in the LLM of choice based on the requirements of that particular application. The model-driven approach makes both the requirements specification and the test generation platform-independent, and provides end-to-end traceability between the requirements and their assessment. We have implemented an open-source tool set, available on GitHub, to support the application of our approach.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {203–213},
numpages = {11},
keywords = {Bias, Domain-Specific Language, Ethics, Large Language Models, Model-Driven Engineering, Red Teaming, Testing},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1109/CGO57630.2024.10444830,
author = {Okuda, Katsumi and Amarasinghe, Saman},
title = {AskIt: Unified Programming Interface for Programming with Large Language Models},
year = {2024},
isbn = {9798350395099},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO57630.2024.10444830},
doi = {10.1109/CGO57630.2024.10444830},
abstract = {Large Language Models (LLMs) exhibit a unique phenomenon known as emergent abilities, demonstrating adeptness across numerous tasks, from text summarization to code generation. While these abilities open up novel avenues in software design and crafting, their incorporation presents substantial challenges. Developers face decisions regarding the use of LLMs for directly performing tasks within applications as well as for generating and executing code to accomplish these tasks. Moreover, effective prompt design becomes a critical concern, given the necessity of extracting data from natural language outputs. To address these complexities, this paper introduces AskIt, a domain-specific language (DSL) specifically designed for LLMs. AskIt simplifies LLM integration by providing a unified interface that not only allows for direct task execution using LLMs but also supports the entire cycle of code generation and execution. This dual capability is achieved through (1) type-guided output control, (2) template-based function definitions, and (3) prompt generation for both usage modes. Our evaluations underscore AskIt's effectiveness. Across 50 tasks, AskIt generated concise prompts, achieving a 16.14 \% reduction in prompt length compared to benchmarks. Additionally, by enabling a seamless transition between using LLMs directly in applications and for generating code, AskIt achieved significant efficiency improvements, as observed in our GSM8K benchmark experiments. The implementations of AskIt in TypeScript and Python are available at https://github.com/katsumiok/ts-askit and https://github.com/katsumiok/pyaskit, respectively.},
booktitle = {Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {41–54},
numpages = {14},
keywords = {domain specific language, code generation, large language model, software engineering, artificial intelligence},
location = {Edinburgh, United Kingdom},
series = {CGO '24}
}

@article{10.1145/3736775,
author = {Ma, Xinyu and Meng, Hengyu and Wu, Ziwei and Wang, Zeyu and von Chamier-Waite, Clea T},
title = {Becoming Space: Exploring Agential Materiality through AI-Generated Metamorphosis in Artistic Practice},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3736775},
doi = {10.1145/3736775},
abstract = {This paper examines the intersection of materiality, agency, and transformation in AI-assisted artistic practice through Becoming Space, an installation of 3D printed sculptures and video. Drawing from Ovid’s Metamorphoses, the work investigates states of transformation between human and animal forms using generative AI and additive manufacturing. Through a theoretical investigation, this paper tries to analyze how material-discursive practices emerge in the convergence of language and physical materials. The work demonstrates how agency distributes across human and nonhuman actors in creative processes, challenging traditional notions of artistic authorship and subjectivity. This investigation contributes to understanding how generative AI technologies participate in artistic practices, revealing creativity as an entangled phenomenon where multiple agencies converge and transform.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = jul,
articleno = {29},
numpages = {11},
keywords = {Materiality, Agency, Becoming-animal, Metamorphoses, Generative AI, 3D Printing, Sculpture, Nonhuman}
}

@inproceedings{10.1145/3652620.3687811,
author = {M. Mosthaf, My and Wasowski, Andrzej},
title = {From a Natural to a Formal Language with DSL Assistant},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687811},
doi = {10.1145/3652620.3687811},
abstract = {The development of domain-specific languages (DSLs) is a laborious and iterative process that seems to naturally lean to the use of generative artificial intelligence. We design and prototype DSL Assistant, a tool that integrates generative language models to support the development of DSLs. DSL Assistant uses OpenAI's assistant API with GPT-4o to generate DSL grammars and example instances. To reflect real-world use, DSL Assistant supports several different interaction modes for evolving a DSL design, and includes automatic error repair. Our experiments show that DSL Assistant helps users to create and modify DSLs. However the quality of the generated DSLs depends on the specific domain and the followed interaction patterns.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {541–549},
numpages = {9},
keywords = {domain-specific languages, generative AI, large language models},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3687802,
author = {Buchmann, Thomas},
title = {Prompting Bidirectional Model Transformations - The Good, The Bad and The Ugly},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687802},
doi = {10.1145/3652620.3687802},
abstract = {This paper investigates the comparative effectiveness of model-to-model transformations generated by an LLM based upon user prompts versus those created with dedicated model transformation languages, using a standard benchmark. The emergence of Generative AI offers a novel approach, allowing developers to specify transformations in natural language rather than learning specialized languages. However, our findings suggest that, in its current state, generative AI does not yet pose a threat to dedicated model transformation languages. While AI-assisted approaches promise to provide flexibility and accessibility, dedicated model transformation languages still offer structured advantages crucial for complex transformations, especially when bidirectionality and incrementality are mandatory requirements. This research contributes to the ongoing discourse on the role of AI in software engineering, highlighting its potential and current limitations in enhancing model transformation processes.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {550–555},
numpages = {6},
keywords = {modeling, LLM, MDE, AI, modeltransformation, Bx},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3664647.3680814,
author = {Cai, Pengxiang and Liu, Zhiwei and Zhu, Guibo and Niu, Yunfang and Wang, Jinqiao},
title = {Auto DragGAN: Editing the Generative Image Manifold in an Autoregressive Manner},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680814},
doi = {10.1145/3664647.3680814},
abstract = {Pixel-level fine-grained image editing remains an open challenge. Previous works fail to achieve an ideal trade-off between control granularity and inference speed. They either fail to achieve pixel-level fine-grained control, or their inference speed requires optimization. To address this, this paper for the first time employs a regression-based network to learn the variation patterns of StyleGAN latent codes during the image dragging process. This method enables pixel-level precision in dragging editing with little time cost. Users can specify handle points and their corresponding target points on any GAN-generated images, and our method will move each handle point to its corresponding target point. Through experimental analysis, we discover that a short movement distance from handle points to target points yields a high-fidelity edited image, as the model only needs to predict the movement of a small portion of pixels. To achieve this, we decompose the entire movement process into multiple sub-processes. Specifically, we develop a transformer encoder-decoder based network named 'Latent Predictor' to predict the latent code motion trajectories from handle points to target points in an autoregressive manner. Moreover, to enhance the prediction stability, we introduce a component named 'Latent Regularizer', aimed at constraining the latent code motion within the distribution of natural images. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) inference speed and image editing performance at the pixel-level granularity.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {3372–3380},
numpages = {9},
keywords = {autoregressive model, gans, image editing},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3717383.3717391,
author = {Barat, Souvik and Mulpuru, Dushyanthi and Yadav, Abhishek and Korabu, Reshma and Thogaru, Himabindu and Kulkarni, Vinay},
title = {Constructing Enterprise Digital Twins by Augmenting LLMs with MDE},
year = {2025},
isbn = {9798400714245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3717383.3717391},
doi = {10.1145/3717383.3717391},
abstract = {Over the past year, Large Language Models (LLMs) have proven their value across a diverse range of industrial applications starting from supporting software development to automating customer interactions and enhancing process automation. We harness their potential for constructing Enterprise Digital Twins (EDTs), an emerging decision-making aid for a wide range of business sectors. EDT offers an effective "in silico" business experimentation leading to evidence-based informed decision-making, but its construction requires deep domain expertise spanning multiple aspects of enterprises across multiple stakeholders. Moreover, constructing an effective EDT demands seamless coordination between domain experts and expert modelers. These critical dependencies make the EDT construction challenging. This paper investigates the role of LLMs as domain experts and expert modelers to reduce excessive dependencies on both specializations and their coordination to an extent. Our approach integrates meta-modelling and Model Driven Engineering (MDE) techniques to effectively utilize LLMs with increased precision to alleviate the cognitive burden on domain experts and provide a systematic metamodel guided method for constructing purposive digital twins. We illustrate the approach and demonstrate its efficacy using a real-life EDT use case.},
booktitle = {Proceedings of the 18th Innovations in Software Engineering Conference},
articleno = {11},
numpages = {11},
keywords = {Large Language Model, LLM, ChatGPT, Model Driven Engineering, Digital Twin},
location = {
},
series = {ISEC '25}
}

@article{10.1145/3744750,
author = {Hong, Matt-Heun and Crisan, Anamaria},
title = {Data has Entered the Chat: How Data Workers Conduct Exploratory Visual Analytic Conversations with GenAI Agents},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2160-6455},
url = {https://doi.org/10.1145/3744750},
doi = {10.1145/3744750},
abstract = {We investigate the potential of leveraging the code-generating capabilities of Large Language Models (LLMs) to support exploratory visual analysis (EVA) via conversational user interfaces (CUIs). We developed a technology probe that was deployed through two studies with a total of 50 data workers to explore the structure and flow of visual analytic conversations during EVA. We analyzed conversations from both studies using thematic analysis and derived a state transition diagram summarizing the conversational flow between four states of participant utterances (Analytic Tasks, Editing Operations, Elaborations and Enrichments, and Directive Commands) and two states of Generative AI (GenAI) agent responses (visualization, text). We describe the capabilities and limitations of GenAI agents according to each state and transitions between states as three co-occurring loops: analysis elaboration, refinement, and explanation. We discuss our findings as future research trajectories to improve the experiences of data workers using GenAI.Code \&amp; Data:},
note = {Just Accepted},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
keywords = {Visual Analysis, Conversational Interfaces, Generative AI, User Study}
}

@inproceedings{10.1145/3587102.3588815,
author = {Daun, Marian and Brings, Jennifer},
title = {How ChatGPT Will Change Software Engineering Education},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588815},
doi = {10.1145/3587102.3588815},
abstract = {This position paper discusses the potential for using generative AIs like ChatGPT in software engineering education. Currently, discussions center around potential threats emerging from student's use of ChatGPT. For instance, generative AI will limit the usefulness of graded homework dramatically. However, there exist potential opportunities as well. For example, ChatGPT's ability to understand and generate human language allows providing personalized feedback to students, and can thus accompany current software engineering education approaches. This paper highlights the potential for enhancing software engineering education. The availability of generative AI will improve the individualization of education approaches. In addition, we discuss the need to adapt software engineering curricula to the changed profiles of software engineers. Moreover, we point out why it is important to provide guidance for using generative AI and, thus, integrate it in courses rather than accepting the unsupervised use by students, which can negatively impact the students' learning.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {110–116},
numpages = {7},
keywords = {ChatGPT, generative AI, software engineering education},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3652620.3688336,
author = {Bucchiarone, Antonio and Panciera, Marco and Cicchetti, Antonio and Mana, Nadia and Castelluccio, Carlotta and Stott, Lee},
title = {PromptDeck: A No-Code Platform for Modular Prompt Engineering},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688336},
doi = {10.1145/3652620.3688336},
abstract = {This paper introduces a no-code platform for modular prompt engineering, designed to democratize access to generative AI for nondevelopers. By integrating advanced technologies such as Node.js, Express, MongoDB, and Azure OpenAI services, the platform provides a robust and flexible environment for creating and managing AI-driven tasks. The intuitive frontend, built with React and TypeScript, enables users with minimal coding expertise to design, execute, and evaluate complex AI workflows. A key feature of the platform is its extensible plugin system, which allows users to easily incorporate additional functionalities to meet their specific needs. This no-code approach empowers a broader audience to harness the power of generative AI, fostering innovation and enabling diverse applications across various fields. By lowering the technical barriers, the platform paves the way for widespread adoption of AI technologies, driving the future of AI-enhanced solutions.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {895–904},
numpages = {10},
keywords = {low-code development platforms, no-code, generative AI, prompt engineering, modularization},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3716134,
author = {Grunde-McLaughlin, Madeleine and Lam, Michelle S. and Krishna, Ranjay and Weld, Daniel S. and Heer, Jeffrey},
title = {Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/3716134},
doi = {10.1145/3716134},
abstract = {LLM chains enable complex tasks by decomposing work into a sequence of subtasks. Similarly, the more established techniques of crowdsourcing workflows decompose complex tasks into smaller tasks for human crowdworkers. Chains address LLM errors analogously to the way crowdsourcing workflows address human error. To characterize opportunities for LLM chaining, we survey 107 papers across the crowdsourcing and chaining literature to construct a design space for chain development. The design space covers a designer’s objectives and the tactics used to build workflows. We then surface strategies that mediate how workflows use tactics to achieve objectives. To explore how techniques from crowdsourcing may apply to chaining, we adapt crowdsourcing workflows to implement LLM chains across three case studies: creating a taxonomy, shortening text, and writing a short story. From the design space and our case studies, we identify takeaways for effective chain design and raise implications for future research and development.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
articleno = {27},
numpages = {57},
keywords = {Crowdsourcing workflows, large language model chains, design space, case studies}
}

@inproceedings{10.1145/3623476.3623524,
author = {Ait, Adem and C\'{a}novas Izquierdo, Javier Luis and Cabot, Jordi},
title = {A Tool for the Definition and Deployment of Platform-Independent Bots on Open Source Projects},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623476.3623524},
doi = {10.1145/3623476.3623524},
abstract = {The development of Open Source Software (OSS) projects is a collaborative process that heavily relies on active contributions by passionate developers. Creating, retaining and nurturing an active community of developers is a challenging task; and finding the appropriate expertise to drive the development process is not always easy. To alleviate this situation, many OSS projects try to use bots to automate some development tasks, thus helping community developers to cope with the daily workload of their projects. However, the techniques and support for developing bots is specific to the code hosting platform where the project is being developed (e.g., GitHub or GitLab). Furthermore, there is no support for orchestrating bots deployed in different platforms nor for building bots that go beyond pure development activities. In this paper, we propose a tool to define and deploy bots for OSS projects, which besides automation tasks they offer a more social facet, improving community interactions. The tool includes a Domain-Specific Language (DSL) which allows defining bots that can be deployed on top of several platforms and that can be triggered by different events (e.g., creation of a new issue or a pull request). We describe the design and the implementation of the tool, and illustrate its use with examples.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {214–219},
numpages = {6},
keywords = {Open Source, Domain-Specific Language, Bot},
location = {Cascais, Portugal},
series = {SLE 2023}
}

@inproceedings{10.1145/3652620.3687810,
author = {Tabassum, Mirza Rehenuma and Ritchie, Matthew J. and Mustafiz, Sadaf and Kienzle, J\"{o}rg},
title = {Using LLMs for Use Case Modelling of IoT Systems: An Experience Report},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687810},
doi = {10.1145/3652620.3687810},
abstract = {Requirements engineering (RE) plays an essential role in the success of system and software development. Textual use case models are valuable for capturing diverse scenarios describing the interactions between the system and its actors, but their development, particularly for the Internet of Things (IoT), can be tedious and error-prone due to the added complexities and heterogeneous nature of such systems. Automating requirements elicitation and specification tasks with the use of generative AI is highly desirable. This paper explores the potential of large language models (LLMs) for generating interaction models for IoT systems from informal problem descriptions. We investigate the capabilities of OpenAI's GPT-4 and Google's Gemini for generating standard and UCM4IoT textual use cases by carrying out a comparative study using four IoT applications. While both of these LLMs show promise as supporting tools, our findings indicate a need for further refinement and domain-specific training to enhance their precision and reliability in requirements development for the IoT domain.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {611–619},
numpages = {9},
keywords = {requirements engineering, use case modelling, large language model, LLM, model-based development},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3650105.3652290,
author = {Abukhalaf, Seif and Hamdaqa, Mohammad and Khomh, Foutse},
title = {PathOCL: Path-Based Prompt Augmentation for OCL Generation with GPT-4},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652290},
doi = {10.1145/3650105.3652290},
abstract = {The rapid progress of AI-powered programming assistants, such as GitHub Copilot, has facilitated the development of software applications. These assistants rely on large language models (LLMs), which are foundation models (FMs) that support a wide range of tasks related to understanding and generating language. LLMs have demonstrated their ability to express UML model specifications using formal languages like the Object Constraint Language (OCL). However, the context size of the prompt is limited by the number of tokens an LLM can process. This limitation becomes significant as the size of UML class models increases. In this study, we introduce PathOCL, a novel path-based prompt augmentation technique designed to facilitate OCL generation. PathOCL addresses the limitations of LLMs, specifically their token processing limit and the challenges posed by large UML class models. PathOCL is based on the concept of chunking, which selectively augments the prompts with a subset of UML classes relevant to the English specification. Our findings demonstrate that PathOCL, compared to augmenting the complete UML class model (UML-Augmentation), generates a higher number of valid and correct OCL constraints using the GPT-4 model. Moreover, the average prompt size crafted using PathOCL significantly decreases when scaling the size of the UML class models.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {108–118},
numpages = {11},
keywords = {object constraint language (OCL), simple path, prompt engineering, large language model (LLM), generative pre-trained transformer (GPT), foundation model (FM)},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3597503.3639081,
author = {Jiang, Yuxuan and Zhang, Chaoyun and He, Shilin and Yang, Zhihao and Ma, Minghua and Qin, Si and Kang, Yu and Dang, Yingnong and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
title = {Xpert: Empowering Incident Management with Query Recommendations via Large Language Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639081},
doi = {10.1145/3597503.3639081},
abstract = {Large-scale cloud systems play a pivotal role in modern IT infrastructure. However, incidents occurring within these systems can lead to service disruptions and adversely affect user experience. To swiftly resolve such incidents, on-call engineers depend on crafting domain-specific language (DSL) queries to analyze telemetry data. However, writing these queries can be challenging and time-consuming. This paper presents a thorough empirical study on the utilization of queries of KQL, a DSL employed for incident management in a large-scale cloud management system at Microsoft. The findings obtained underscore the importance and viability of KQL queries recommendation to enhance incident management.Building upon these valuable insights, we introduce Xpert, an end-to-end machine learning framework that automates KQL recommendation process. By leveraging historical incident data and large language models, Xpert generates customized KQL queries tailored to new incidents. Furthermore, Xpert incorporates a novel performance metric called Xcore, enabling a thorough evaluation of query quality from three comprehensive perspectives. We conduct extensive evaluations of Xpert, demonstrating its effectiveness in offline settings. Notably, we deploy Xpert in the real production environment of a large-scale incident management system in Microsoft, validating its efficiency in supporting incident management. To the best of our knowledge, this paper represents the first empirical study of its kind, and Xpert stands as a pioneering DSL query recommendation framework designed for incident management.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {92},
numpages = {13},
keywords = {incident management, query generation, large language model},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3652620.3687782,
author = {Lamas, Victor and R. Luaces, Miguel and Garcia-Gonzalez, Daniel},
title = {DSL-Xpert: LLM-driven Generic DSL Code Generation},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687782},
doi = {10.1145/3652620.3687782},
abstract = {Nowadays, large language models (LLMs) are an extremely useful and fast tool to complement and help in many jobs and current problems. However, there are cases where a pretty specific vocabulary is used in which these models were not previously trained, leading to less satisfactory results. More specifically, these models are less effective when dealing with less-known or unpublished domain-specific languages (DSLs). Within this field, the automatic generation of code based on such languages, starting from natural language, would speed up the development times of any related project, as well as the understanding of such DSLs. Therefore, this paper presents a tool in which developers can perform what is known as semantic parsing. In other words, the developer can ask a pre-trained LLM to translate a natural language instruction into the vocabulary of the established DSL. Thus, by setting the DSL grammar as context (grammar prompting) and providing usage examples (few-shot learning), the LLM can quickly generate reliable domain-specific code, significantly improving the quality of life of the developers. A video demonstration of the tool is shown in the following link: https://zenodo.org/records/12610506.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {16–20},
numpages = {5},
keywords = {domain-specific languages (DSLS), large language models (LLMS), semantic parsing, grammar prompting, few-shot learning},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3720526,
author = {Kim, Donguk and Jeon, Minseok and Hwang, Doha and Oh, Hakjoo},
title = {PAFL: Enhancing Fault Localizers by Leveraging Project-Specific Fault Patterns},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720526},
doi = {10.1145/3720526},
abstract = {We present PAFL, a new technique for enhancing existing fault localization methods by leveraging project-specific fault patterns.                We observed that each software project has its own challenges and suffers from recurring fault patterns associated with those challenges.                 However, existing fault localization techniques use a universal localization strategy without considering those repetitive faults.                To address this limitation, our technique, called project-aware fault localization (PAFL), enables existing fault localizers to leverage project-specific fault patterns.                Given a buggy version of a project and a baseline fault localizer, PAFL first mines the fault patterns from past buggy versions of the project.                 Then, it uses the mined fault patterns to update the suspiciousness scores of statements computed by the baseline fault localizer.                To this end, we use two novel ideas.                First, we design a domain-specific fault pattern-description language to represent various fault patterns.                An instance, called crossword, in our language describes a project-specific fault pattern and how it affects the suspiciousness scores of statements.                Second, we develop an algorithm that synthesizes crosswords (i.e., fault patterns) from past buggy versions of the project.                Evaluation using seven baseline fault localizers and 12 real-world C/C++ and Python projects demonstrates that PAFL effectively, robustly, and efficiently improves the performance of the baseline fault localization techniques.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {129},
numpages = {28},
keywords = {Domain-Specific Language, Fault Localization, Program Synthesis}
}

@article{10.1145/3729330,
author = {Li, Yixuan and Magalh\~{a}es, Jos\'{e} Wesley de Souza and Brauckmann, Alexander and O'Boyle, Michael F. P. and Polgreen, Elizabeth},
title = {Guided Tensor Lifting},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {PLDI},
url = {https://doi.org/10.1145/3729330},
doi = {10.1145/3729330},
abstract = {Domain-specific languages (DSLs) for machine learning are revolutionizing the speed and efficiency of machine learning workloads as they enable users easy access to high-performance compiler optimizations and accelerators. However, to take advantage of these capabilities, a user must first translate their legacy code from the language it is currently written in, into the new DSL. The process of automatically lifting code into these DSLs has been identified by several recent works, which propose program synthesis as a solution. However, synthesis is expensive and struggles to scale without carefully designed and hard-wired heuristics. In this paper, we present an approach for lifting that combines an enumerative synthesis approach with a Large Language Model used to automatically learn the domain-specific heuristics for program lifting, in the form of a probabilistic grammar. Our approach outperforms the state-of-the-art tools in this area, despite only using learned heuristics.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {227},
numpages = {23},
keywords = {Code Generation, Code Optimization, Large Language Model, Lifting, Program Synthesis, Tensor Algebra}
}

@inproceedings{10.1145/3650212.3680388,
author = {Xue, Zhiyi and Li, Liangguo and Tian, Senyue and Chen, Xiaohong and Li, Pingping and Chen, Liangyu and Jiang, Tingting and Zhang, Min},
title = {LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680388},
doi = {10.1145/3650212.3680388},
abstract = {FinTech software, crucial for both safety and timely market deployment, presents a compelling case for automated acceptance testing against regulatory business rules. However, the inherent challenges of comprehending unstructured natural language descriptions of these rules and crafting comprehensive test cases demand human intelligence. The emergence of Large Language Models (LLMs) holds promise for automated test case generation, leveraging their natural language processing capabilities. Yet, their dependence on human intervention for effective prompting hampers efficiency.    In response, we introduce a groundbreaking, fully automated approach for generating high-coverage test cases from natural language business rules. Our methodology seamlessly integrates the versatility of LLMs with the predictability of algorithmic methods. We fine-tune pre-trained LLMs for improved information extraction accuracy and algorithmically generate comprehensive testable scenarios for the extracted business rules.	Our prototype, LLM4Fin, is designed for testing real-world stock-trading software. Experimental results demonstrate LLM4Fin’s superiority over both state-of-the-art LLM, such as ChatGPT, and skilled testing engineers. We achieve remarkable performance, with up to 98.18\% and an average of 20\%−110\% improvement on business scenario coverage, and up to 93.72\% on code coverage, while reducing the time cost from 20 minutes to a mere 7 seconds. These results provide robust evidence of the framework’s practical applicability and efficiency, marking a significant advancement in FinTech software testing.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1643–1655},
numpages = {13},
keywords = {Software acceptance testing, fintech software, large language model, test case generation},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3733719,
author = {Kreikemeyer, Justin Noah and Jankowski, Mi\l{}osz and Wilsdorf, Pia and Uhrmacher, Adelinde M.},
title = {Using (Not-so) Large Language Models to Generate Simulation Models in a Formal DSL: A Study on Reaction Networks},
year = {2025},
issue_date = {October 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3733719},
doi = {10.1145/3733719},
abstract = {Formal languages are an integral part of modeling and simulation. They allow the distillation of knowledge into concise simulation models amenable to automatic execution, interpretation, and analysis. However, the arguably most humanly accessible means of expressing models is through natural language, which is not easily interpretable by computers. Here, we evaluate how a Large Language Model (LLM) might be used for formalizing natural language into simulation models. Existing studies only explored using very large LLMs, like the commercial GPT models, without fine-tuning model weights. To close this gap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned to translate natural language descriptions to reaction network models in a domain-specific language, offering a self-hostable, compute-efficient, and memory efficient alternative. To this end, we develop a synthetic data generator to serve as the basis for fine-tuning and evaluation. Our quantitative evaluation shows that our fine-tuned Mistral model can recover the ground truth simulation model in up to 84.5\% of cases. In addition, our small-scale user study demonstrates the model’s practical potential for one-time generation as well as interactive modeling in various domains. While promising, in its current form, the fine-tuned small LLM cannot catch up with large LLMs. We conclude that higher-quality training data are required, and expect future small and open-source LLMs to offer new opportunities.},
journal = {ACM Trans. Model. Comput. Simul.},
month = sep,
articleno = {31},
numpages = {27},
keywords = {Simulation model generation, natural language processing, language model, constrained decoding, knowledge extraction}
}

@article{10.1145/3727980,
author = {Chang, Kaiyan and Zhu, Wenlong and Wang, Kun and He, Xinyang and Yang, Nan and Chen, Zhirong and Jin, Dantong and Li, Cangyuan and Zhou, Yunhao and Yan, Hao and Zhao, Zhuoliang and Cheng, Yuan and Wang, Mengdi and Liang, Shengwen and Han, Yinhe and Li, Xiaowei and Li, Huawei and Wang, Ying},
title = {A data-centric chip design agent framework for Verilog code generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3727980},
doi = {10.1145/3727980},
abstract = {Recent advances in large language models (LLMs) have demonstrated significant potential for automated hardware description language (HDL) code generation from high-level specifications. However, two critical challenges limit further progress in this domain: the scarcity of quality Verilog training data and the inability of current approaches to generate RTL code optimized for power, performance, and area (PPA) metrics. This paper presents a comprehensive data-centric framework that addresses these limitations through innovations in both pre-fine-tuning data preparation and after-fine-tuning optimization strategies. In the pre-fine-tuning phase, we tackle the data scarcity problem with an automated design-data augmentation framework that generates high-volume, high-quality natural language specifications aligned with corresponding Verilog code and EDA scripts. Our approach creates a complete RTL-level feedback loop by augmenting EDA scripts, RTL code, and EDA tool feedback. In the after-fine-tuning phase, we focus on generating PPA-aware RTL code through a novel search and prompt framework. Our approach implements iterative filtering and selection of LLM-generated Verilog variants while providing high-quality predefined prompts, including composition and interface specifications. To evaluate the effectiveness of our data augmentation method, we fine-tune Llama 2-13B and Llama 2-7B models using the dataset generated by our augmentation framework. The results demonstrate a significant improvement in the Verilog generation tasks with LLMs. Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8\% to 70.6\% with the same benchmark. Our 13B model has a pass rate improvement compared with GPT-3.5 in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data. Additionally, to evaluate the effectiveness of the our agent framework, we compare the PPA on the GPT-3.5, where the results show that the agent refined RTL code can have a better quality than the generated RTL code only with GPT-3.5.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = apr,
keywords = {Large Language Model, Hardware Generation, Data Augmentation}
}

@inproceedings{10.1145/3708036.3708111,
author = {Huang, Qinhua},
title = {Improving Explanations of Legal Judgement Prediction in Chinese Context by Legal Language Model},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708111},
doi = {10.1145/3708036.3708111},
abstract = {This paper presents an investigation into improving the explanations accompanying predictions of legal judgments in the Chinese context through the utilization of a specialized legal language model. The study address the growing interest in the field of computational legal analysis, specifically legal judgment prediction (LJP), which has emerged as a crucial tool for enhancing the efficiency, transparency, and accessibility of legal systems worldwide. Recognizing the significance of providing clear and comprehensive explanations in the legal domain, we emphasize that these elucidations serve as vital aids to judges during their decision-making processes, fostering greater efficiency, accuracy, and trustworthiness within the judiciary. Moreover, the ability to offer well-grounded explanations is particularly crucial in the context of judicial supervision, where supervisors often face time constraints and need concise yet informative summaries of case details to conduct effective reviews. To achieve this goal, we deploy the legal language model, Lawformer and other models to construct a work flow, addressing long legal document judgement prediction problem, while presenting the related explanation. We also conduct an experiment to prove the validity of the method.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {439–443},
numpages = {5},
keywords = {explanation, judgement prediction, legal large language model},
location = {
},
series = {ICCSMT '24}
}

@article{10.1145/3744920,
author = {Ben Chaaben, Meriem and Burgue\~{n}o, Lola and David, Istvan and Sahraoui, Houari},
title = {On the Utility of Domain Modeling Assistance with Large Language Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3744920},
doi = {10.1145/3744920},
abstract = {Model-driven engineering (MDE) simplifies software development through abstraction, yet challenges such as time constraints, incomplete domain understanding, and adherence to syntactic constraints hinder the design process. This paper presents a study to evaluate the usefulness of a novel approach utilizing large language models (LLMs) and few-shot prompt learning to assist in domain modeling. The aim of this approach is to overcome the need for extensive training of traditional AI-based completion algorithms on domain-specific datasets and to offer versatile support for various modeling activities, providing valuable recommendations to software modelers. To support this approach, we developed MAGDA, a user-friendly tool, through which we conduct a user study and assess the real-world applicability of our approach in the context of domain modeling, offering valuable insights into its usability and effectiveness.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
keywords = {domain modeling, generative AI, language models, model-driven engineering, prompt learning, user study}
}

@inproceedings{10.1145/3711542.3711571,
author = {undefinedlgen, Bahar and Hattab, Georges},
title = {A Lexical Simplification Framework for Turkish},
year = {2025},
isbn = {9798400717383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711542.3711571},
doi = {10.1145/3711542.3711571},
abstract = {Lexical simplification is a fundamental step towards improving the accessibility, comprehension, and readability of texts, particularly in languages with limited linguistic resources. In this study, we adopt a state-of-the-art lexical simplification approach and propose a lexical simplification framework tailored to Turkish. Our framework leverages a combination of complex word identification tasks and substitution generation through pre-trained language models to identify complex lexical units using selective substitution ranking approaches and algorithms and replace them with simpler alternatives, thereby improving text readability. This work makes three key contributions: (i) a comprehensive study of lexical simplification for Turkish, including the complex word identification subtask; (ii) a rigorous comparison of various language models for candidate generation using the masked language modeling objective; and (iii) an in-depth exploration of the impact of different complexity thresholds and additional parameters on overall performance. Our framework demonstrates a strong capability to balance simplification and contextual preservation, offering an effective solution to lexical simplification in Turkish.},
booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval},
pages = {218–224},
numpages = {7},
keywords = {BERT, Text simplification, artificial intelligence for social good, complex word identification, lexical simplification, low-resource languages, pre-trained language model},
location = {
},
series = {NLPIR '24}
}

@inbook{10.1145/3696630.3728529,
author = {Gao, Yanjie and Luo, Jiyu and Lin, Haoxiang and Zhang, Hongyu and Wu, Ming and Yang, Mao},
title = {dl²: Detecting Communication Deadlocks in Deep Learning Jobs},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728529},
abstract = {In recent years, deep learning has seen widespread adoption across various domains, giving rise to large-scale models such as large language models. Training these models, particularly in distributed environments, presents substantial computational and communication challenges. A critical issue is the communication deadlock—a state in which processes become indefinitely stalled while awaiting network messages from others, which leads to resource wastage and reduced productivity. Current approaches to deadlock handling are either unsuitable for deep learning due to its unique hybrid programming paradigm or limit optimization opportunities. This paper presents dl2, a novel dynamic analysis tool designed to detect communication deadlocks in deep learning jobs. dl2 models the runtime trace of a job as an execution graph, detects unmatched communications, and constructs a wait-for graph to identify deadlock cycles. dl2 can also handle nondeterministic communication behaviors, providing replay and diagnostic support for root cause analysis. We evaluate dl2 using PyTorch with a combination of synthetic test cases and real-world deep learning workloads. The experimental results show that dl2 successfully detects all communication deadlocks, achieving 100\% precision and recall, which highlights its effectiveness.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {27–38},
numpages = {12}
}

@article{10.1145/3728955,
author = {Zheng, Mingwei and Xie, Danning and Shi, Qingkai and Wang, Chengpeng and Zhang, Xiangyu},
title = {Validating Network Protocol Parsers with Traceable RFC Document Interpretation},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728955},
doi = {10.1145/3728955},
abstract = {Validating the correctness of network protocol implementations is highly challenging due to the oracle and traceability problems. The former determines when a protocol implementation can be considered buggy, especially when the bugs do not cause any observable symptoms. The latter allows developers to understand how an implementation violates the protocol specification, thereby facilitating bug fixes. Unlike existing works that rarely take both problems into account, this work considers both and provides an effective solution using recent advances in large language models (LLMs). Our key observation is that network protocols are often released with structured specification documents, a.k.a. RFC documents, which can be systematically translated to formal protocol message specifications via LLMs. Such specifications, which may contain errors due to the hallucination of LLMs, are used as a quasi-oracle to validate protocol parsers, while the validation results in return gradually refine the oracle. Since the oracle is derived from the document, any bugs we find in a protocol implementation can be traced back to the document, thus addressing the traceability problem. We have extensively evaluated our approach using nine network protocols and their implementations written in C, Python, and Go. The results show that our approach outperforms the state-of-the-art and has detected 69 bugs, with 36 confirmed. The project also demonstrates the potential for fully automating software validation based on natural language specifications, a process previously considered predominantly manual due to the need to understand specification documents and derive expected outputs for test inputs.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA078},
numpages = {23},
keywords = {Large language model, Network protocol parsers, Traceability}
}

@article{10.1145/3729364,
author = {Wan, Yuxuan and Wang, Chaozheng and Dong, Yi and Wang, Wenxuan and Li, Shuqing and Huo, Yintong and Lyu, Michael},
title = {Divide-and-Conquer: Generating UI Code from Screenshots},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729364},
doi = {10.1145/3729364},
abstract = {Websites are critical in today’s digital world, with over 1.11 billion currently active and approximately 252,000 new sites launched daily. Converting website layout design into functional UI code is a time-consuming yet indispensable step of website development. Manual methods of converting visual designs into functional code present significant challenges, especially for non-experts. To explore automatic design-to-code solutions, we first conduct a motivating study on GPT-4o and identify three types of issues in generating UI code: element omission, element distortion, and element misarrangement. We further reveal that a focus on smaller visual segments can help multimodal large language models (MLLMs) mitigate these failures in the generation process. In this paper, we propose DCGen, a divide-and-conquer-based approach to automate the translation of webpage design to UI code. DCGen starts by dividing screenshots into manageable segments, generating code for each segment, and then reassembling them into complete UI code for the entire screenshot. We conduct extensive testing with a dataset comprised of real-world websites and various MLLMs and demonstrate that DCGen achieves up to a 15\% improvement in visual similarity and 8\% in code similarity for large input images. Human evaluations show that DCGen can help developers implement webpages significantly faster and more similar to the UI designs. To the best of our knowledge, DCGen is the first segment-aware MLLM-based approach for generating UI code directly from screenshots.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE094},
numpages = {24},
keywords = {Code Generation, Multi-modal Large Language Model, User Interface, Web Development}
}

@inproceedings{10.1145/3701716.3715232,
author = {Li, Xiangyu and Zeng, Yawen and Xing, Xiaofen and Xu, Jin and Xu, Xiangmin},
title = {HedgeAgents: A Balanced-aware Multi-agent Financial Trading System},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715232},
doi = {10.1145/3701716.3715232},
abstract = {As automated trading gains traction in the financial market, algorithmic investment strategies are increasingly prominent. While Large Language Models (LLMs) and Agent-based models exhibit promising potential in real-time market analysis and trading decisions, they still experience a significant -20\% loss when confronted with rapid declines or frequent fluctuations, impeding their practical application. Hence, there is an imperative to explore a more robust and resilient framework. This paper introduces an innovative multi-agent system, HedgeAgents, aimed at bolstering system robustness via ''hedging'' strategies. In this well-balanced system, an array of hedging agents has been tailored, where HedgeAgents consist of a central fund manager and multiple hedging experts specializing in various financial asset classes. These agents leverage LLMs' cognitive capabilities to make decisions and coordinate through three types of conferences. Benefiting from the powerful understanding of LLMs, our HedgeAgents attained a 70\% annualized return and a 400\% total return over a period of 3 years. Moreover, we have observed with delight that HedgeAgents can even formulate investment experience comparable to those of human experts (https://hedgeagents.github.io/).},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {296–305},
numpages = {10},
keywords = {large language model, multi-agent systems, quantization finance},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3649329.3657356,
author = {Chang, Kaiyan and Wang, Kun and Yang, Nan and Wang, Ying and Jin, Dantong and Zhu, Wenlong and Chen, Zhirong and Li, Cangyuan and Yan, Hao and Zhou, Yunhao and Zhao, Zhuoliang and Cheng, Yuan and Pan, Yudong and Liu, Yiqi and Wang, Mengdi and Liang, Shengwen and Han, Yinhe and Li, Huawei and Li, Xiaowei},
title = {Data is all you need:  Finetuning LLMs for Chip Design via an Automated design-data augmentation framework},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3657356},
doi = {10.1145/3649329.3657356},
abstract = {Recent advances in large language models have demonstrated their potential for automated generation of hardware description language (HDL) code from high-level prompts. Researchers have utilized fine-tuning to enhance the ability of these large language models (LLMs) in the field of Chip Design. However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs. Additionally, the absence of a Verilog and electronic design automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers. This paper proposes an automated design-data augmentation framework, which generates high-volume and high-quality natural language aligned with Verilog and EDA scripts. For Verilog generation, it translates Verilog files to an abstract syntax tree and then maps nodes to natural language with a predefined template. For Verilog repair, it uses predefined rules to generate the wrong verilog file and then pairs EDA Tool feedback with the right and wrong verilog file. For EDA Script generation, it uses existing LLM(GPT-3.5) to obtain the description of the Script. To evaluate the effectiveness of our data augmentation method, we finetune Llama2--13B and Llama2-7B models using the dataset generated by our augmentation framework. The results demonstrate a significant improvement in the Verilog generation tasks with LLMs. Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8\% to 70.6\% with the same benchmark. Our 13B model (ChipGPT-FT1) has a pass rate improvement compared with GPT-3.5 in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {60},
numpages = {6},
keywords = {data augmentation, hardware generation large language model},
location = {San Francisco, CA, USA},
series = {DAC '24}
}

@inproceedings{10.1145/3604237.3626869,
author = {Li, Yinheng and Wang, Shaofei and Ding, Han and Chen, Hang},
title = {Large Language Models in Finance: A Survey},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626869},
doi = {10.1145/3604237.3626869},
abstract = {Recent advances in large language models (LLMs) have opened new possibilities for artificial intelligence applications in finance. In this paper, we provide a practical survey focused on two key aspects of utilizing LLMs for financial tasks: existing solutions and guidance for adoption. First, we review current approaches employing LLMs in finance, including leveraging pretrained models via zero-shot or few-shot learning, fine-tuning on domain-specific data, and training custom LLMs from scratch. We summarize key models and evaluate their performance improvements on financial natural language processing tasks. Second, we propose a decision framework to guide financial professionals in selecting the appropriate LLM solution based on their use case constraints around data, compute, and performance needs. The framework provides a pathway from lightweight experimentation to heavy investment in customized LLMs. Lastly, we discuss limitations and challenges around leveraging LLMs in financial applications. Overall, this survey aims to synthesize the state-of-the-art and provide a roadmap for responsibly applying LLMs to advance financial AI.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {374–382},
numpages = {9},
keywords = {Finance, Generative AI, Large Language Models, Natural Language Processing},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@inproceedings{10.1145/3650212.3652135,
author = {Lu, You and Tian, Yifan and Bi, Yuyang and Chen, Bihuan and Peng, Xin},
title = {DiaVio: LLM-Empowered Diagnosis of Safety Violations in ADS Simulation Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652135},
doi = {10.1145/3650212.3652135},
abstract = {Simulation testing has been widely adopted by leading companies to ensure the safety of autonomous driving systems (ADSs). Anumber of scenario-based testing approaches have been developed to generate diverse driving scenarios for simulation testing, and demonstrated to be capable of finding safety violations. However, there is no automated way to diagnose whether these violations are caused by the ADS under test and which category these violations belong to. As a result, great effort is required to manually diagnose violations.     To bridge this gap, we propose DiaVio to automatically diagnose safety violations in simulation testing by leveraging large language models (LLMs). It is built on top of a new domain specific language (DSL) of crash to align real-world accident reports described in natural language and violation scenarios in simulation testing. DiaVio fine-tunes a base LLM with real-world accident reports to learn diagnosis capability, and uses the fine-tuned LLM to diagnose violation scenarios in simulation testing. Our evaluation has demonstrated the effectiveness and efficiency of DiaVio in violation diagnosis.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {376–388},
numpages = {13},
keywords = {Automated Driving System, Large Language Models, Scenario-based Testing, Violation Diagnosis},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1109/ICSE55347.2025.00138,
author = {Pourasad, Ali Ebrahimi and Maalej, Walid},
title = {Does GenAI Make Usability Testing Obsolete?},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00138},
doi = {10.1109/ICSE55347.2025.00138},
abstract = {Ensuring usability is crucial for the success of mobile apps. Usability issues can compromise user experience and negatively impact the perceived app quality. This paper presents UX-LLM, a novel tool powered by a Large Vision-Language Model that predicts usability issues in iOS apps. To evaluate the performance of UX-LLM, we predicted usability issues in two open-source apps of a medium complexity and asked two usability experts to assess the predictions. We also performed traditional usability testing and expert review for both apps and compared the results to those of UX-LLM. UX-LLM demonstrated precision ranging from 0.61 and 0.66 and recall between 0.35 and 0.38, indicating its ability to identify valid usability issues, yet failing to capture the majority of issues. Finally, we conducted a focus group with an app development team of a capstone project developing a transit app for visually impaired persons. The focus group expressed positive perceptions of UX-LLM as it identified unknown usability issues in their app. However, they also raised concerns about its integration into the development workflow, suggesting potential improvements. Our results show that UX-LLM cannot fully replace traditional usability evaluation methods but serves as a valuable supplement particularly for small teams with limited resources, to identify issues in less common user paths, due to its ability to inspect the source code.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {437–449},
numpages = {13},
keywords = {app development, large language model, foundation models, usability engineering, AI4SE, recommender systems, quality requirements, AI-inspired design},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3617553.3617887,
author = {Fulcini, Tommaso and Torchiano, Marco},
title = {Is ChatGPT Capable of Crafting Gamification Strategies for Software Engineering Tasks?},
year = {2023},
isbn = {9798400703737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617553.3617887},
doi = {10.1145/3617553.3617887},
abstract = {Gamification has gained significant attention in the last decade for its potential to enhance engagement and motivation in various domains. During the last year ChatGPT, a state-of-the-art large language model has received even more attention both in the field of scientific research and in common use by individuals or companies.   In this study, we investigate the possibility of adopting ChatGPT as a tool for designing gamification platforms in the Software Engineering domain. Leveraging the capabilities of ChatGPT, we assess how good is it at generating effective suggestions and ideas for designers or developers.   To evaluate ChatGPT's potential as a gamification platform creator we narrowed the context to one particular Software Engineering activity, asking for possible aspects of the activity to be gamified. Each proposed aspect was subsequently unraveled by ChatGPT both asking in a shared and separate context, first following the conversational nature of the model, then applying a validated design framework. The study assesses ChatGPT's ability to select and integrate game elements to build a thriving gamification environment by framing the design of the platform to a state-of-the-art conceptual framework. To evaluate the goodness of the design choices made we relied both on the Octalysis framework and on personal experience.   The findings of the papers show that ChatGPT can only create simple playful experiences not very effective. Although, by instructing the model with more specific desired mechanics and dynamics, it is possible to guide it toward the application of the ideas suggested. We argue that ChatGPT is not capable of building a gamified environment on its own, but it could still be used to build the foundation of a gamification platform as long as the designers refine and rough out the advice gained from a user-centered solution.},
booktitle = {Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation},
pages = {22–28},
numpages = {7},
keywords = {Artificial Intelligence, Gamification, Large Language Model, Software Engineering, Software Lifecycle},
location = {San Francisco, CA, USA},
series = {Gamify 2023}
}

@inproceedings{10.1145/3678726.3678778,
author = {Li, Zhi-Fang and zhao, Shuo and Zhang, Ya-Chen},
title = {Research on the Transformation of Music Education Model under the Background of Generative Artificial Intelligence},
year = {2024},
isbn = {9798400717611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678726.3678778},
doi = {10.1145/3678726.3678778},
abstract = {Since 2022, the rapid development of generative artificial intelligence systems represented by Chat-GPT has profoundly influenced various fields. This article briefly introduces the development process of generative artificial intelligence models in recent years, explains the benefits of applying generative artificial intelligence in music education, and discusses the direction of transformation for music education models in the context of the development of generative artificial intelligence. It also reflects and summarizes the challenges faced by the current application of generative artificial intelligence in the field of music education.},
booktitle = {Proceedings of the 2024 8th International Conference on Education and Multimedia Technology},
pages = {60–64},
numpages = {5},
keywords = {Keywords—Generative AI, Music education, Research on Transformation Models},
location = {Tokyo, Japan},
series = {ICEMT '24}
}

@inproceedings{10.1145/3640310.3674091,
author = {L\'{o}pez, Jos\'{e} Antonio Hern\'{a}ndez and F\"{o}ldi\'{a}k, M\'{a}t\'{e} and Varr\'{o}, D\'{a}niel},
title = {Text2VQL: Teaching a Model Query Language to Open-Source Language Models with ChatGPT},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674091},
doi = {10.1145/3640310.3674091},
abstract = {While large language models (LLMs) like ChatGPT has demonstrated impressive capabilities in addressing various software engineering tasks, their use in a model-driven engineering (MDE) context is still in an early stage. Since the technology is proprietary and accessible solely through an API, its use may be incompatible with the strict protection of intellectual properties in industrial models. While there are open-source LLM alternatives, they often lack the power of proprietary models and require extensive data fine-tuning to realize their full potential. Furthermore, open-source datasets tailored for MDE tasks are scarce, posing challenges for training such models effectively.In this work, we introduce Text2VQL, a framework that generates graph queries captured in the VIATRA Query Language (VQL) from natural language specifications using open-source LLMs. Initially, we create a high-quality synthetic dataset comprising pairs of queries and their corresponding natural language descriptions using ChatGPT and VIATRA parser. Leveraging this dataset, we use parameter-efficient tuning to specialize three open-source LLMs, namely, DeepSeek Coder 1b, DeepSeek Coder 7b, and CodeLlama 7b for VQL query generation. Our experimental evaluation demonstrates that the fine-tuned models outperform the base models in query generation, highlighting the usefulness of our synthetic dataset. Moreover, one of the fine-tuned models achieves performance comparable to ChatGPT.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {13–24},
numpages = {12},
keywords = {ChatGPT, VIATRA Query Language (VQL), large language model (LLM), model query language, query generation},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3675094.3678494,
author = {Hu, Yongquan and Zhang, Shuning and Dang, Ting and Jia, Hong and Salim, Flora D. and Hu, Wen and Quigley, Aaron J.},
title = {Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal Data for Mental Health},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678494},
doi = {10.1145/3675094.3678494},
abstract = {Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuable multimodal insights into psychological states or neurological disorders. Recent advancements with Large Language Models (LLMs) position them as prospective "health agents'' for mental health assessment. However, current research predominantly focus on single data modalities, presenting an opportunity to advance understanding through multimodal data. Our study aims to advance this approach by investigating multimodal data using LLMs for mental health assessment, specifically through zero-shot and few-shot prompting. Three datasets are adopted for depression and emotion classifications incorporating EEG, facial expressions, and audio (text). The results indicate that multimodal information confers substantial advantages over single modality approaches in mental health assessment. Notably, integrating EEG alongside commonly used LLM modalities such as audio and images demonstrates promising potential. Moreover, our findings reveal that 1-shot learning offers greater benefits compared to zero-shot learning methods.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {412–417},
numpages = {6},
keywords = {eeg, large language model, mental health, prompt engineering},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3652620.3687809,
author = {G\"{o}bel, Susanne and L\"{a}mmel, Ralf},
title = {Model-Based Trust Analysis of LLM Conversations},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687809},
doi = {10.1145/3652620.3687809},
abstract = {LLM-based chatbots are routinely advertised as supporting the collaboration of humans and AI. We study LLM conversations from a knowledge elicitation perspective with the objective of being able to understand and assess the human's trust in knowledge elicited from the LLM and complementary sources. Our approach is supported by the DSML KEML, the Knowledge Elicitation Modeling Language, subject to abstract and visual syntax as well as a model transformation-based model semantics for trust analysis. Conversations are modeled by a combination of sequence diagrams and enhanced argumentation graphs --- the latter for the purpose of relating information pieces (facts and instructions) that are extracted from messages. The analysis of the corresponding models entails trust scores for gathered information (i.e., elicited knowledge).},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {602–610},
numpages = {9},
keywords = {MDE for AI, knowledge representation models, model-based analysis of LLMS, dsmls for AI usage},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3686038.3686044,
author = {Tang, Yun and Moffat, Luke and Guo, Weisi and May-Chahal, Corinne and Deville, Joe and Tsourdos, Antonios},
title = {Encoding Social \&amp; Ethical Values in Autonomous Navigation: Philosophies Behind an Interactive Online Demonstration},
year = {2024},
isbn = {9798400709890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686038.3686044},
doi = {10.1145/3686038.3686044},
abstract = {Autonomous Systems (ASs) interacting with human societies raises complex social \&amp; ethical challenges. This paper argues that one way of scaffolding human trust in ASs is through the encoding of ethical, legal and social impact (ELSI) considerations in the ASs’ decision-making processes. Existing ELSI-encoding efforts often focus on the implementation of rule-based and risk-based approaches, leaving key questions unanswered - what are the relationships between ELSI-encoding software logic in ASs and human ethical practises; what ethical approaches cannot be easily translated into software rules and numeric risks; and what are the implications of this for ethical AS? To answer these questions, we review and discuss different ELSI-encoding approaches in ASs from a new perspective, i.e., their relationships with classic human ethics philosophies. We also explore the feasibility of large language models (LLMs)-based ELSI-encoding practices in overcoming the limitations of rule-based and risk-based approaches and the associated challenges. To foster understanding, facilitate knowledge exchange and inspire discussion among cross-disciplinary research communities, we build and publish the first online interactive playground demonstrating different ELSI-encoding approaches on the same AS decision-making process. We welcome feedback and contributions in making this platform truly beneficial to trustworthy autonomous system research communities.},
booktitle = {Proceedings of the Second International Symposium on Trustworthy Autonomous Systems},
articleno = {21},
numpages = {9},
keywords = {Demonstration, ELSI-encoding, Large Language Model, Trustworthy Autonomous Systems},
location = {Austin, TX, USA},
series = {TAS '24}
}

@inbook{10.1145/3672608.3707984,
author = {Xia, Yuan and Pingle, Aabha and Sur, Deepayan and Deshmukh, Jyotirmoy and Raghothaman, Mukund and Ravi, Srivatsan},
title = {LLM-guided Predicate Discovery and Data Augmentation for Learning Likely Program Invariants},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707984},
abstract = {Security protocols, protocols to achieve consensus, those for maintaining memory consistency and coherence, distributed ledgers, multi-party computation, and many similar software systems are examples of distributed message-passing based computation. Ensuring correctness of such distributed systems is a challenging problem for many automatic verification approaches. The deductive verification approach for reasoning about such systems involves computing a program invariant, i.e., an expression evaluates to true for every reachable program state. Several approaches for synthesizing invariants are dynamic, i.e., runs of the program and ancillary information such as target safety properties are used to learn an invariant expression. However, most existing approaches invoke a model checker (or a theorem prover) within the synthesis loop, which makes these approaches depend on the scalability of the verification tools. In this paper, we propose a counterexample-guided inductive synthesis approach called RunVS which learns invariant expressions from program runs, but without information such as target safety properties, and without invoking a model checker/theorem prover for validation. The synthesis approach pairs a decision-tree (DT) based method with a data augmentation technique: DT-learning provides an expression that classifies observed states from augmented states that are speculated to be unreachable. Validation of the learned invariant is performed by sampling program runs and states; any run that invalidates the invariant results in counterexamples used to revises the invariant. As there is no formal proof that the learned artifact is a true invariant, we call such an expression a likely invariant. An important user input to synthesis is often the set of predicates that comprise the invariant expression; we use a novel integration with a large language model (LLM) and prompt it to provide likely predicates to be used. We show empirical results of our approach on several distributed protocols implemented in the Promela modeling language.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1721–1729},
numpages = {9}
}

@inproceedings{10.1145/3687997.3695650,
author = {Pontes Miranda, James William and Bruneliere, Hugo and Tisi, Massimo and Suny\'{e}, Gerson},
title = {Towards an In-Context LLM-Based Approach for Automating the Definition of Model Views},
year = {2024},
isbn = {9798400711800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687997.3695650},
doi = {10.1145/3687997.3695650},
abstract = {In the Model-Driven Engineering (MDE) of complex systems, multiple models represent various systems' aspects. In practice, these models are often unconnected and specified using different modeling languages. Model view solutions can be employed to automatically combine such models. However, writing model view definitions is not trivial. When modeling languages are semantically distant and/or have a large number of concepts, it can quickly become difficult to manually identify the language elements to be selected, associated, or queried to build a model view. As a solution, this paper proposes an in-context Large Language Model (LLM)-based approach to assist engineers in writing model-view definitions. Notably, we rely on LLMs and Prompt Engineering techniques to automatically generate drafts of model-view definitions by providing as input only minimal information on the modeling languages to be combined. We implemented our approach by integrating the EMF Views solution for model views with the LangChain framework for LLM-based applications. To this end, we tailored LangChain to handle EMF metamodels. We validated our approach and implementation on a set of model views originally specified either in VPDL, the ViewPoint Definition Language of EMF Views, or as ATL model-to-model transformations. We compared these original model view definitions with the ones we automatically generated. The obtained results show the feasibility and applicability of our approach.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {29–42},
numpages = {14},
keywords = {Large language models, Model views, Model-driven engineering, Modeling languages, Prompt engineering},
location = {Pasadena, CA, USA},
series = {SLE '24}
}

@inproceedings{10.1145/3711896.3737222,
author = {Wang, Jiang and Dong, Zhengxin and Bai, Bing and Jiang, Guyu and Yuan, Aiquan and Cao, Guodong},
title = {FoodGPT: Reinforcement Post-Training of Large Language Models in the Food Delivery Domain},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737222},
doi = {10.1145/3711896.3737222},
abstract = {On-demand Food Delivery (OFD) platforms, such as Ele.me and Meituan, have transformed daily life by offering convenient ordering services. However, challenges remain in understanding user intentions and processing product-related text information. Existing NLP models, while advanced in general tasks, are less effective for OFD-specific needs due to data scarcity and high computational costs. This paper introduces FoodInstruct, a Chinese dataset with 1.6 million examples across 12 OFD-related NLP tasks, and FoodGPT, a domain-specific large language model. We propose an efficient reinforcement post-training framework that combines Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Proximal Policy Optimization (PPO) with an additional rule-based reward signal. The resulting foundation model, FoodGPT, enhances model performance while minimizing resource consumption. Experimental results demonstrate that FoodGPT outperforms general models, such as Qwen 2.5, Llama3.1 and DeepSeek-LLM, on OFD tasks, using fewer data and training iterations. The model has already been deployed across numerous applications within the company. For instance, it achieved a 0.57\% increase in click-through rate and a 0.32\% increase in user visit-to-purchase rate in the ITG online experiment. The core contributions are now publicly accessible at https://huggingface.co/elemenlp.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {4966–4974},
numpages = {9},
keywords = {artificial intelligence, deep learning, large language models, natural language processing, on-demand food delivery},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3652620.3688334,
author = {Hagel, Nathan and Hili, Nicolas and Schwab, Didier},
title = {Turning Low-Code Development Platforms into True No-Code with LLMs},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688334},
doi = {10.1145/3652620.3688334},
abstract = {The relevance of low-code / no-code development has grown substantially in research and practice over the years to allow nontechnical users to create applications and, therefore, democratise software development. One problem in this domain still persists: many platforms remain low-code as the underlying modeling layer still requires professionals to write/design a model, often using Domain Specific Languages (DSLs). With the rise of generative AI and Large Language Models (LLMs) and their capabilities, new possibilities emerge on how Low Code Development Platforms (LCDPs) can be improved.This paper shows how the capabilities of LLMs can be leveraged to turn DSL-based low-code platforms into true no-code. We analyzed how textual modeling can be replaced by generating the required model using LLMs. We conducted a user experiment to compare textual modeling with the use of LLMs for that task. Our results show that task completion time could be significantly improved, and the majority of users prefer using the LLM-aided modeling. Based on these findings, we discuss the integration of these techniques into an existing low-code platform to transform it into true no-code.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {876–885},
numpages = {10},
keywords = {LLM, AI, low-code development platform, meta-model, model-driven engineering, DSL},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3729176.3729203,
author = {Drosos, Ian and Williams, Jack and Sarkar, Advait and Wilson, Nicholas and Rintel, Sean and Panda, Payod},
title = {Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks},
year = {2025},
isbn = {9798400713842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3729176.3729203},
doi = {10.1145/3729176.3729203},
abstract = {Prompting generative AI effectively is challenging for users, particularly in expressing context for comprehension tasks like explaining spreadsheet formulas, Python code, and text passages. Through a formative survey (n = 38), we uncovered a trade-off between standardized but predictable prompting support, and context-adaptive but unpredictable support. We explore this trade-off by implementing two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC), which generates UI elements for prompt refinement based on the user’s specific prompt, and Static Prompt Refinement Control (Static PRC), which offers generic controls. Our controlled user study (n = 16) showed that the Dynamic PRC approach afforded more control, lowered barriers to providing context, and encouraged task exploration and reflection, but reasoning about the effects of generated controls on the final output remains challenging. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows.},
booktitle = {Proceedings of the 4th Annual Symposium on Human-Computer Interaction for Work},
articleno = {24},
numpages = {23},
keywords = {Dynamic UX Generation, Prompt Middleware},
location = {
},
series = {CHIWORK '25}
}

@inproceedings{10.1109/ICSE55347.2025.00216,
author = {Sun, Yang and Poskitt, Christopher M. and Wang, Kun and Sun, Jun},
title = {FixDrive: Automatically Repairing Autonomous Vehicle Driving Behaviour for $0.08 per Violation},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00216},
doi = {10.1109/ICSE55347.2025.00216},
abstract = {Autonomous Vehicles (AVs) are advancing rapidly, with Level-4 AVs already operating in real-world conditions. Current AVs, however, still lag behind human drivers in adaptability and performance, often exhibiting overly conservative behaviours and occasionally violating traffic laws. Existing solutions, such as runtime enforcement, mitigate this by automatically repairing the AV's planned trajectory at runtime, but such approaches lack transparency and should be a measure of last resort. It would be preferable for AV repairs to generalise beyond specific incidents and to be interpretable for users. In this work, we propose FixDrive, a framework that analyses driving records from near-misses or law violations to generate AV driving strategy repairs that reduce the chance of such incidents occurring again. These repairs are captured in μDrive, a high-level domain-specific language for specifying driving behaviours in response to event-based triggers. Implemented for the state-of-the-art autonomous driving system Apollo, FixDrive identifies and visualises critical moments from driving records, then uses a Multimodal Large Language Model (MLLM) with zero-shot learning to generate μDrive programs. We tested FixDrive on various benchmark scenarios, and found that the generated repairs improved the AV's performance with respect to following traffic laws, avoiding collisions, and successfully reaching destinations. Furthermore, the direct costs of repairing an AV—15 minutes of offline analysis and $0.08 per violation—are reasonable in practice.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1921–1933},
numpages = {13},
keywords = {autonomous vehicles, autonomous driving systems, multimodal large language models, driving compliance},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3640794.3665538,
author = {S\'{a}nchez Cuadrado, Jes\'{u}s and P\'{e}rez-Soler, Sara and Guerra, Esther and De Lara, Juan},
title = {Automating the Development of Task-oriented LLM-based Chatbots},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665538},
doi = {10.1145/3640794.3665538},
abstract = {Task-oriented chatbots are increasingly used to access all sorts of services – like booking a flight, or setting a medical appointment – through natural language conversation. There are many technologies for implementing task-oriented chatbots, including Dialogflow, Watson, and Rasa. They rely on an explicit definition of the user intents, conversation flows, and chatbot outputs, which is costly to specify, and sometimes results in suboptimal user experiences and artificial conversations with limited diversity of chatbot responses. Recently, the advances in generative artificial intelligence fostered by Large Language Models (LLMs) have enabled a new range of open-domain chatbots, like ChatGPT, able to converse fluently on any topic. However, they are general-purpose, and therefore not directly usable to solve specialised tasks reliably. In this paper, we study the power of LLMs to build task-oriented chatbots, resulting in lighter specifications – no intent definition required – and more natural conversations than in intent-based approaches. To this end, we propose a lightweight domain-specific language based on YAML to specify chatbots using modules of different types (e.g., menus, question-answering, data gathering). These specifications are compiled into structured LLM prompts that use the ReAct framework to inform our runtime how to interpret the user input and coordinate the tasks that the chatbot must perform. The paper presents the design and realisation of our framework, and an assessment that encodes a set of existing intent-based chatbots using our approach, showing its benefits in terms of specification size, conversation flexibility and output diversity.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {11},
numpages = {10},
keywords = {Domain-Specific Languages, Large Language Models, Task-oriented Chatbots},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@inproceedings{10.1145/3748355.3748369,
author = {Gao, Xiangyu and Zhu, Xiangfeng and Shobhana, Bhavana Vannarth and Yang, Yiwei and Krishnamurthy, Arvind and Mahajan, Ratul},
title = {Offloading the Tedious Task of Writing eBPF Programs},
year = {2025},
isbn = {9798400720840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3748355.3748369},
doi = {10.1145/3748355.3748369},
abstract = {eBPF offers a lightweight method to extend the Linux kernel without modifying the source code in existing modules. However, writing correct and efficient eBPF programs is hard due to its unique verifier constraints and cumbersome debugging processes specific to the kernel execution environment. To tackle such an obstacle, we present a system, SimpleBPF, aiming at offloading the tedious eBPF development task. Developers only need to express their intent in a high-level domain-specific language, while the underlying eBPF code generation is handled automatically. SimpleBPF integrates four key components: a concise DSL, an LLM-based generator, a semantic checker, and an LLM-based optimizer. We use few-shot prompting to build both the code generator and optimizer in SimpleBPF, and evaluate the system on programs written in a representative DSL. The preliminary evaluation result shows that SimpleBPF can generate valid eBPF programs that pass the kernel verifier and exhibit competitive runtime performance. We also outline future directions based on current findings.},
booktitle = {Proceedings of the 3rd Workshop on EBPF and Kernel Extensions},
pages = {63–69},
numpages = {7},
keywords = {Code Generation, LLM Inference, Prompt Engineering, eBPF},
location = {Coimbra, Portugal},
series = {eBPF '25}
}

@inproceedings{10.1145/3613904.3642517,
author = {Yang, Jackie (Junrui) and Shi, Yingtian and Zhang, Yuhan and Li, Karina and Rosli, Daniel Wan and Jain, Anisha and Zhang, Shuning and Li, Tianshi and Landay, James A. and Lam, Monica S.},
title = {ReactGenie: A Development Framework for Complex Multimodal Interactions Using Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642517},
doi = {10.1145/3613904.3642517},
abstract = {By combining voice and touch interactions, multimodal interfaces can surpass the efficiency of either modality alone. Traditional multimodal frameworks require laborious developer work to support rich multimodal commands where the user’s multimodal command involves possibly exponential combinations of actions/function invocations. This paper presents ReactGenie, a programming framework that better separates multimodal input from the computational model to enable developers to create efficient and capable multimodal interfaces with ease. ReactGenie translates multimodal user commands into NLPL (Natural Language Programming Language), a programming language we created, using a neural semantic parser based on large-language models. The ReactGenie runtime interprets the parsed NLPL and composes primitives in the computational model to implement complex user commands. As a result, ReactGenie allows easy implementation and unprecedented richness in commands for end-users of multimodal apps. Our evaluation showed that 12 developers can learn and build a non-trivial ReactGenie application in under 2.5 hours on average. In addition, compared with a traditional GUI, end-users can complete tasks faster and with less task load using ReactGenie apps.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {483},
numpages = {23},
keywords = {development frameworks, large-language model, multimodal interactions, natural language processing, programming framework},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3733155.3733192,
author = {Pinna, Simone and Massa, Silvia Maria and Fenu, Matteo and Casti, Giulio and Riboni, Daniele},
title = {Integration of Retrieval-Augmented Generation Technique for LLM-based Differential Diagnosis Assistant},
year = {2025},
isbn = {9798400714023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733155.3733192},
doi = {10.1145/3733155.3733192},
abstract = {Artificial Intelligence (AI) is increasingly transforming the medical field, offering significant potential for diagnosis, treatment, and patient care. However, its successful integration relies on healthcare professionals, such as doctors, psychologists, and nurses, trusting the technology’s reliability and accuracy. For Large Language Models (LLMs), this trust requires transparent, verifiable, and rigorously reviewed information sources. This paper presents an AI-powered tool for differential diagnosis and disease comparison, utilizing an LLM enhanced by Retrieval-Augmented Generation (RAG). RAG overcomes traditional LLM limitations by enabling access to external, domain-specific knowledge, ensuring accurate and contextually relevant responses. The system leverages PubMed, a biomedical article aggregator, to extract symptom-related information from scientific literature on various disorders. Evaluations involving psychologist-administered questionnaires demonstrate that combining a similarity score with detailed symptom descriptions provides a clear understanding of relationships between disorders. This approach may enhance diagnostic precision and build trust in AI-driven tools, encouraging their broader adoption in clinical practice.},
booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {277–284},
numpages = {8},
keywords = {Large Language Models, Retrieval-Augmented Generation, e-Health, Differential diagnosis},
location = {
},
series = {PETRA '25}
}

@article{10.1109/TCBB.2024.3480088,
author = {Su, Fangfang and Teng, Chong and Li, Fei and Li, Bobo and Zhou, Jun and Ji, Donghong},
title = {Generative Biomedical Event Extraction With Constrained Decoding Strategy},
year = {2024},
issue_date = {Nov.-Dec. 2024},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {21},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2024.3480088},
doi = {10.1109/TCBB.2024.3480088},
abstract = {Currently, biomedical event extraction has received considerable attention in various fields, including natural language processing, bioinformatics, and computational biomedicine. This has led to the emergence of numerous machine learning and deep learning models that have been proposed and applied to tackle this complex task. While existing models typically adopt an extraction-based approach, which requires breaking down the extraction of biomedical events into multiple subtasks for sequential processing, making it prone to cascading errors. This paper presents a novel approach by constructing a biomedical event generation model based on the framework of the pre-trained language model &lt;italic&gt;T5&lt;/italic&gt;. We employ a sequence-to-sequence generation paradigm to obtain events, the model utilizes constrained decoding algorithm to guide sequence generation, and a curriculum learning algorithm for efficient model learning. To demonstrate the effectiveness of our model, we evaluate it on two public benchmark datasets, Genia 2011 and Genia 2013. Our model achieves superior performance, illustrating the effectiveness of generative modeling of biomedical events.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = oct,
pages = {2471–2484},
numpages = {14}
}

@inproceedings{10.1145/3708036.3708114,
author = {Qiu, Han and Sun, Chuanqiang and Chen, Hongyun and Zou, Baoyu and Dong, Zizheng},
title = {Design and Implementation of an Intelligent Document Extraction and Review System Based on Multimodal Large Language Models},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708114},
doi = {10.1145/3708036.3708114},
abstract = {This study aims to address the issues of low efficiency and high manual involvement in existing government affairs processes by proposing an automated system based on large model technology. The system integrates modules such as data collection, natural language processing, business rule engine, process generation, and intelligent applications. Innovations include using large models to transform unstructured government information into executable rules, automatically generating process models and documents, and implementing intelligent approval and automated verification at key nodes. Experimental results indicate that the system significantly enhances the automation level and execution efficiency of government affairs processes, providing a new solution for the intelligent transformation of government management.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {455–465},
numpages = {11},
keywords = {Government Affairs Process Automation, Intelligent Governance, Large Language Model, Natural Language Processing},
location = {
},
series = {ICCSMT '24}
}

@inproceedings{10.1145/3723498.3723816,
author = {Shyne, Fiona and Cooper, Seth},
title = {Computational Tools for Table-Top Role-Playing Games: A Scoping Review},
year = {2025},
isbn = {9798400718564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723498.3723816},
doi = {10.1145/3723498.3723816},
abstract = {Table-top role-playing games (TTRPGs) are a form of gameplay that often requires a variety of complex tasks to be completed both in preparation and throughout gameplay: from tracking game state to the creation of fictional worlds. This has presented an opportunity for computational assistance in TTRPG sessions, both in the creation of artifacts and throughout the gameplay. We investigate the current research in computational tools for TTRPGs through a scoping review of academic works and present the major trends and opportunities from these works. We screened over one thousand works sourced from three different academic databases: ACM Digital Library, IEEE-Xplore, and Google Scholar. Papers were included based on relevance to TTRPGs, computational interface, and academic venue. In total, we evaluated 46 works in terms of produced artifacts, computational methods, evaluation, and outcomes. These papers include a diverse set of produced artifacts and computational methods, with an emphasis on tangible interfaces and generative AI systems. However, we found an opportunity for future work in terms of long-term studies, mixed-initiative methods, and different aspects of gameplay.},
booktitle = {Proceedings of the 20th International Conference on the Foundations of Digital Games},
articleno = {20},
numpages = {14},
keywords = {TTRPGs, Literature Review, Procedural Content Generation, Tangible Interface},
location = {
},
series = {FDG '25}
}

@inproceedings{10.1145/3706598.3714211,
author = {Tao, Sirui and Liang, Ivan and Peng, Cindy and Wang, Zhiqing and Palani, Srishti and Dow, Steven P.},
title = {DesignWeaver: Dimensional Scaffolding for Text-to-Image Product Design},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714211},
doi = {10.1145/3706598.3714211},
abstract = {Generative AI has enabled novice designers to quickly create professional-looking visual representations for product concepts. However, novices have limited domain knowledge that could constrain their ability to write prompts that effectively explore a product design space. To understand how experts explore and communicate about design spaces, we conducted a formative study with 12 experienced product designers and found that experts — and their less-versed clients — often use visual references to guide co-design discussions rather than written descriptions. These insights inspired DesignWeaver, an interface that helps novices generate prompts for a text-to-image model by surfacing key product design dimensions from generated images into a palette for quick selection. In a study with 52 novices, DesignWeaver enabled participants to craft longer prompts with more domain-specific vocabularies, resulting in more diverse, innovative product designs. However, the nuanced prompts heightened participants’ expectations beyond what current text-to-image models could deliver. We discuss implications for AI-based product design support tools.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {425},
numpages = {26},
keywords = {Creativity support tools, design ideation, idea management, human-AI interaction, text-to-image models},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3708282.3708296,
author = {Qiu, Han and Chen, Hongyun and Zou, Baoyu and Dong, Zizheng},
title = {Intelligent Design and Implementation of Government Affairs Processes Driven by Large Language Models},
year = {2025},
isbn = {9798400709869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708282.3708296},
doi = {10.1145/3708282.3708296},
abstract = {This study aims to address the issues of low efficiency and high manual involvement in existing government affairs processes by proposing an automated system based on large model technology. The system integrates modules such as data collection, natural language processing, business rule engine, process generation, and intelligent applications. Innovations include using large models to transform unstructured government information into executable rules, automatically generating process models and documents, and implementing intelligent approval and automated verification at key nodes. Experimental results indicate that the system significantly enhances the automation level and execution efficiency of government affairs processes, providing a new solution for the intelligent transformation of government management.},
booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence of Things and Computing},
pages = {70–79},
numpages = {10},
keywords = {Government Affairs Process Automation, Intelligent Governance, Large Language Model, Natural Language Processing},
location = {
},
series = {AITC '24}
}

@article{10.1145/3632913,
author = {Ding, Yuantian and Qiu, Xiaokang},
title = {Enhanced Enumeration Techniques for Syntax-Guided Synthesis of Bit-Vector Manipulations},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632913},
doi = {10.1145/3632913},
abstract = {Syntax-guided synthesis has been a prevalent theme in various computer-aided programming systems. However, the domain of bit-vector synthesis poses several unique challenges that have not yet been sufficiently addressed and resolved. In this paper, we propose a novel synthesis approach that incorporates a distinct enumeration strategy based on various factors. Technically, this approach weighs in subexpression recurrence by term-graph-based enumeration, avoids useless candidates by example-guided filtration, prioritizes valuable components identified by large language models. This approach also incorporates a bottom-up deduction step to enhance the enumeration algorithm by considering subproblems that contribute to the deductive resolution. We implement all the enhanced enumeration techniques in our SyGuS solver DryadSynth, which outperforms state-of-the-art solvers in terms of the number of solved problems, execution time, and solution size. Notably, DryadSynth successfully solved 31 synthesis problems for the first time, including 5 renowned Hacker’s Delight problems.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {71},
numpages = {31},
keywords = {Syntax-guided synthesis, Bit vector, Term graph, Enumeration, Large language model}
}

@inbook{10.1145/3712256.3726430,
author = {Vella Zarb, David and Parks, Geoff and Kipouros, Timoleon},
title = {Program Synthesis with LLM-Predicted Minimal Specialized Grammars},
year = {2025},
isbn = {9798400714658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712256.3726430},
abstract = {Grammatical evolution (GE) has proven effective at program synthesis. However, its performance is hindered by the exponential growth of search spaces as grammar complexity increases. Methods utilizing recent advances in large language models and genetic programming have outperformed GE on some benchmarks, affirming the need for scalable approaches. This paper addresses the scalability challenge by proposing an in-context learning method to automatically generate minimal specialized grammars (MSGs), which are problem-specific subsets of a larger grammar designed to reduce search space complexity. To the best of our knowledge, this represents the first use of in-context learning for program synthesis within GE. Our approach conditions a language model on examples of Backus-Naur Form grammars to generate MSGs tailored to individual synthesis tasks. We evaluate this framework on a benchmark suite widely used in GE research. Experimental results show our method almost always outperforms the baseline GE approach, improving both the number and frequency of problems solved while reducing computational cost as measured by fitness evaluations.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1072–1080},
numpages = {9}
}

@inproceedings{10.1145/3689031.3696095,
author = {Liu, Hanzhi and Jiang, Yanyan and Xu, Chang},
title = {Understanding the Linux Kernel, Visually},
year = {2025},
isbn = {9798400711961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689031.3696095},
doi = {10.1145/3689031.3696095},
abstract = {Understanding the Linux kernel is challenging due to its large and complex program state. While existing kernel debugging tools provide full access to kernel states at arbitrary levels of detail, developers often spend a significant amount of time sifting through redundant information to find what is truly useful. Additionally, the textual results provided by traditional debuggers are often insufficient for expressing high-dimensional information in a readable manner.This paper presents Visualinux, the first debugging framework that can simplify the program state of the Linux kernel to a level that can be visually understood with low programming complexity and effort. Visualinux includes a domain-specific language for specifying simplifications of a kernel object graph, an SQL-like domain-specific language for customizing the simplified object graph, and a panel-based interactive debugger. Evaluation results show that Visualinux can visualize various complex kernel components and efficiently assist developers in diagnosing sophisticated kernel bugs.},
booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
pages = {1044–1060},
numpages = {17},
keywords = {Debugging, Linux Kernel, Software Visualization},
location = {Rotterdam, Netherlands},
series = {EuroSys '25}
}

@inproceedings{10.1109/ICSE-Companion66252.2025.00010,
author = {Kolthoff, Kristian and Kretzer, Felix and Bartelt, Christian and Maedche, Alexander and Ponzetto, Simone Paolo},
title = {GUIDE: LLM-Driven GUI Generation Decomposition for Automated Prototyping},
year = {2025},
isbn = {9798331536831},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion66252.2025.00010},
doi = {10.1109/ICSE-Companion66252.2025.00010},
abstract = {Graphical user interface (GUI) prototyping serves as one of the most valuable techniques for enhancing the elicitation of requirements, facilitating the visualization and refinement of customer needs and closely integrating the customer into the development activities. While GUI prototyping has a positive impact on the software development process, it simultaneously demands significant effort and resources. The emergence of Large Language Models (LLMs) with their impressive code generation capabilities offers a promising approach for automating GUI prototyping. Despite their potential, there is a gap between current LLM-based prototyping solutions and traditional user-based GUI prototyping approaches which provide visual representations of the GUI prototypes and direct editing functionality. In contrast, LLMs and related generative approaches merely produce text sequences or non-editable image output, which lacks both mentioned aspects and therefore impede supporting GUI prototyping. Moreover, minor changes requested by the user typically lead to an inefficient regeneration of the entire GUI prototype when using LLMs directly. In this work, we propose GUIDE, a novel LLM-driven GUI generation decomposition approach seamlessly integrated into the popular prototyping framework Figma. Our approach initially decomposes high-level GUI descriptions into fine-granular GUI requirements, which are subsequently translated into Material Design GUI prototypes, enabling higher controllability and more efficient adaption of changes. To efficiently conduct prompting-based generation of Material Design GUI prototypes, we propose a retrieval-augmented generation (RAG) approach to integrate the component library. Our preliminary evaluation demonstrates the effectiveness of GUIDE in bridging the gap between LLM generation capabilities and traditional GUI prototyping workflows, offering a more effective and controlled user-based approach to LLM-driven GUI prototyping. Video presentation of GUIDE is available at: https://youtu.be/C9RbhMxqpTU},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings},
pages = {1–4},
numpages = {4},
keywords = {automated gui prototyping, retrieval-augmented generation (RAG), prompt decomposition, LLM},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3715335.3735479,
author = {Behmanush, Hamayoon and Akhtari, Freshta and Nooripour, Roghieh and Weber, Ingmar and Cannanure, Vikram Kamath},
title = {Online Learning and GenAI: Supporting Women’s Aspirations Amid Socio-Political Instability in Afghanistan},
year = {2025},
isbn = {9798400714849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715335.3735479},
doi = {10.1145/3715335.3735479},
abstract = {Women in Afghanistan encounter substantial challenges in accessing education and achieving their career aspirations, mainly stemming from cultural norms and ongoing socio-political instability. Since 2021, women have been banned from employment opportunities and formal education beyond the sixth grade. As a result, many women have turned to digital platforms like online courses and utilizing AI tools as an alternative avenue to continue their learning. While these approaches have evidence for impact, their role and utility in unstable socio-political contexts remain unexplored. This mixed-methods study examined how online learning and Generative AI (GenAI) might support Afghanistani women learning introductory programming. Our findings reveal that while online learning was a viable option for participants, learners faced numerous challenges accessing and completing online courses. However, they benefited from GenAI through tutoring, assistance with debugging, and motivational support, which helped them develop programming skills. These skills expanded their career aspirations and improved employment prospects despite ongoing socio-political instability. Based on these insights, we offer initial recommendations for designing GenAI tools to further support women’s career aspirations.},
booktitle = {Proceedings of the 2025 ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies},
pages = {401–417},
numpages = {17},
keywords = {Online Learning, GenAI for Education, LLMs, Programming Education, Women, Socio-Political Instability, Aspiration},
location = {
},
series = {COMPASS '25}
}

@inproceedings{10.1145/3652620.3687805,
author = {Netz, Lukas and Reimer, Jan and Rumpe, Bernhard},
title = {Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687805},
doi = {10.1145/3652620.3687805},
abstract = {Low-code development platforms (LCDPs) are becoming increasingly important in industry, which confronts us in academic teaching with the challenge of educating students in the basic principles, critical engagement, and evaluation of LCDPs. This leads us to the question, how to teach the usage of different LCDPs during an university course. The short time frame of university-level courses makes it challenging to teach more than only one LCDP. In our teaching approach, students use two different LCDPs and create a web-application with both of them. Firstly, we require the students to define a target application with common modeling languages, next they use the first LCDP, at about half the time they switch to the second LCDP and present their findings of the differences in methodology and development processes at the end. We discuss this approach, show survey results from the participants, and explain lessons learned. This concept allows students critical engagement with LCDPs and model-driven software engineering. Supervisors get an insight into the learnability of each LCDP and how novices adapt to different domain-specific languages and their notations.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {115–122},
numpages = {8},
keywords = {low-code development platforms, education, university-level courses, model-driven software engineering, problem-based learning},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3726302.3730326,
author = {Nguyen, Le and Jain, Preet and Panchal, Krutik and Alam, Md Tanvirul and Rastogi, Nidhi},
title = {Assessing Effective Token Length of Multimodal Models for Text-to-Image Retrieval},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730326},
doi = {10.1145/3726302.3730326},
abstract = {Multimodal embedding models have been widely adopted in text-to-image retrieval, enabling direct comparison between text and image modalities. However, how well they handle long text is poorly understood. For instance, Long-CLIP found that OpenAI's CLIP model, despite having a 77-token input limit, maintains optimal performance for only 20 tokens- its effective token length. In this paper, we build on the Long-CLIP study, and extend the analysis to other widely used multimodal models and find their effective token length. Unlike Long-CLIP, we examine how domain-specific language influences changes in effective token length and explore its implications on different domains. Based on our findings, we create a comprehensive reference of various models' effective token length across different domains; offering deeper insights into the true limitations of multimodal models used in text-to-image retrieval. Finally, we introduce a systematic benchmark that determines the effective token length of any multimodal model using a given dataset. Our results show that the effective token length is consistently lower than the input token limit for all models, meaning that these models cannot utilize all the text that can be given to them. We also find that the effective token length varies by dataset, with domain-specific language influencing how much text a model can use before retrieval performance plateaus. Our code is available for reproducibility at https://github.com/aiforsec/EffectiveTokenLength-MModels},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3173–3182},
numpages = {10},
keywords = {benchmarking, clip, image retrieval, long text, multimodal, text-to-image},
location = {Padua, Italy},
series = {SIGIR '25}
}

@article{10.1145/3732790,
author = {Taipalus, Toni and Grahn, Hilkka and Ritonummi, Saima and Siitonen, Valtteri and Vartiainen, Tero and Zhidkikh, Denis},
title = {Novice Perceptions on Effective Elements of PostgreSQL Error Messages},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
url = {https://doi.org/10.1145/3732790},
doi = {10.1145/3732790},
abstract = {SQL compiler error messages are the primary way users receive feedback when they encounter syntax errors or other issues in their SQL queries. Effective error messages can enhance the user experience by providing clear, informative, and actionable feedback. Despite the age of SQL compilers, it still remains largely unclear what contributes to an effective SQL error message. With 2,052 answers yielded by 165 participants for qualitative analysis, this study is an attempt to understand what novices perceive as effective elements in SQL error messages. The results uniformly indicate that communicating the precise error position, articulating what is wrong in the query with clear natural language, and showing hints on how to fix the error are perceived as the most effective elements for error recovery. These insights have potential to be utilized in providing more effective error messages in SQL compilers and SQL learning environments, and for guiding generative AI for enhanced error messages in order to minimize frustration caused by cryptic error messages, improving learning and adoption, and reducing debugging time.},
journal = {ACM Trans. Comput. Educ.},
month = jun,
articleno = {24},
numpages = {19},
keywords = {SQL, error, error message, effective, relational database, novice, software development, PostgreSQL}
}

@inproceedings{10.1145/3677389.3702523,
author = {Datta, Priyangshu and Datta, Suchana and Roy, Dwaipayan},
title = {RAGing Against the Literature: LLM-Powered Dataset Mention Extraction},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702523},
doi = {10.1145/3677389.3702523},
abstract = {Dataset Mention Extraction (DME) is a critical task in the field of scientific information extraction, aiming to identify references to datasets within research papers. In this paper, we explore two advanced methods for DME from research papers, utilizing the capabilities of Large Language Models (LLMs). The first method employs a language model with a prompt-based framework to extract dataset names from text chunks, utilizing patterns of dataset mentions as guidance. The second method integrates the Retrieval-Augmented Generation (RAG) framework, which enhances dataset extraction through a combination of keyword-based filtering, semantic retrieval, and iterative refinement. We observe that both of the proposed methods achieve more than a 25\% improvement in recall compared to the baselines. Further, the RAG-based model achieves an extensive 26\% improvement over the baselines. We also propose exData, a web-based tool for extracting dataset name mentions from a given article.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {29},
numpages = {12},
keywords = {bibliometrics, dataset mention extraction, LLM, RAG},
location = {Hong Kong, China},
series = {JCDL '24}
}

@inproceedings{10.1145/3706598.3713913,
author = {Wang, Huichen Will and Birnbaum, Larry and Setlur, Vidya},
title = {Jupybara: Operationalizing a Design Space for Actionable Data Analysis and Storytelling with LLMs},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713913},
doi = {10.1145/3706598.3713913},
abstract = {Mining and conveying actionable insights from complex data is a key challenge of exploratory data analysis (EDA) and storytelling. To address this challenge, we present a design space for actionable EDA and storytelling. Synthesizing theory and expert interviews, we highlight how semantic precision, rhetorical persuasion, and pragmatic relevance underpin effective EDA and storytelling. We also show how this design space subsumes common challenges in actionable EDA and storytelling, such as identifying appropriate analytical strategies and leveraging relevant domain knowledge. Building on the potential of LLMs to generate coherent narratives with commonsense reasoning, we contribute Jupybara, an AI-enabled assistant for actionable EDA and storytelling implemented as a Jupyter Notebook extension. Jupybara employs two strategies—design-space-aware prompting and multi-agent architectures—to operationalize our design space. An expert evaluation confirms Jupybara’s usability, steerability, explainability, and reparability, as well as the effectiveness of our strategies in operationalizing the design space framework with LLMs.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1005},
numpages = {24},
keywords = {Actionable Insights, Human-AI Collaboration, Multi-Agent System, Large Language Model, Exploratory Data Analysis, Data Storytelling, Data Science, Semantics, Rhetoric, Pragmatics.},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3643661.3643953,
author = {Alshahwan, Nadia and Harman, Mark and Harper, Inna and Marginean, Alexandru and Sengupta, Shubho and Wang, Eddy},
title = {Assured Offline LLM-Based Software Engineering},
year = {2024},
isbn = {9798400705649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643661.3643953},
doi = {10.1145/3643661.3643953},
abstract = {In this paper we address the following question: How can we use Large Language Models (LLMs) to improve code independently of a human, while ensuring that the improved code(1) does not regress the properties of the original code ?(2) improves the original in a verifiable and measurable way ?To address this question, we advocate Assured LLM-Based Software Engineering; a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees. This overcomes the potential problem of LLM's propensity to hallucinate. It allows us to generate code using LLMs, independently of any human. The human plays the role only of final code reviewer, as they would do with code generated by other human engineers.This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.},
booktitle = {Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering},
pages = {7–12},
numpages = {6},
keywords = {large language models (LLMs), genetic improvement (GI), search based software engineering (SBSE), llama, codellama, automated code generation},
location = {Lisbon, Portugal},
series = {InteNSE '24}
}

@inproceedings{10.1145/3643795.3648384,
author = {Koziolek, Heiko and Gr\"{u}ner, Sten and Hark, Rhaban and Ashiwal, Virendra and Linsbauer, Sofia and Eskandani, Nafise},
title = {LLM-based and Retrieval-Augmented Control Code Generation},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648384},
doi = {10.1145/3643795.3648384},
abstract = {Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {22–29},
numpages = {8},
keywords = {large language models, code generation, IEC 61131-3, industrial automation, PLC, DCS, ChatGPT, GPT-4},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@article{10.14778/3750601.3750625,
author = {Sun, Zhaoyan and Zhou, Xuanhe and Li, Guoliang and Yu, Xiang and Feng, Jianhua and Zhang, Yong},
title = {R-Bot: An LLM-Based Query Rewrite System},
year = {2025},
issue_date = {August 2025},
publisher = {VLDB Endowment},
volume = {18},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3750601.3750625},
doi = {10.14778/3750601.3750625},
abstract = {Query rewrite is essential for optimizing SQL queries to improve their execution efficiency without changing their results. Traditionally, this task has been tackled through heuristic and learning-based methods, each with its limitations in terms of inferior quality and low robustness. Recent advancements in LLMs offer a new paradigm by leveraging their superior natural language and code comprehension abilities. Despite their potential, directly applying LLMs like GPT-4 has faced challenges due to problems such as hallucinations, where the model might generate inaccurate or irrelevant results. To address this, we propose R-Bot, an LLM-based query rewrite system with a systematic approach. We first design a multi-source rewrite evidence preparation pipeline to generate query rewrite evidences for guiding LLMs to avoid hallucinations. We then propose a hybrid structure-semantics retrieval method that combines structural and semantic analysis to retrieve the most relevant rewrite evidences for effectively answering an online query. We next propose a step-by-step LLM rewrite method that iteratively leverages the retrieved evidences to select and arrange rewrite rules with self-reflection. We conduct comprehensive experiments on real-world datasets and widely used benchmarks, and demonstrate the superior performance of our system, R-Bot, surpassing state-of-the-art query rewrite methods. The R-Bot system has been deployed at Huawei and with real customers, and the results show that the proposed R-Bot system achieves lower query latency.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {5031–5044},
numpages = {14}
}

@inproceedings{10.1145/3691620.3695058,
author = {Muttillo, Vittoriano and Di Sipio, Claudio and Rubei, Riccardo and Berardinelli, Luca and Dehghani, MohammadHadi},
title = {Towards Synthetic Trace Generation of Modeling Operations using In-Context Learning Approach},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695058},
doi = {10.1145/3691620.3695058},
abstract = {Producing accurate software models is crucial in model-driven software engineering (MDE). However, modeling complex systems is an error-prone task that requires deep application domain knowledge. In the past decade, several automated techniques have been proposed to support academic and industrial practitioners by providing relevant modeling operations. Nevertheless, those techniques require a huge amount of training data that cannot be available due to several factors, e.g., privacy issues. The advent of large language models (LLMs) can support the generation of synthetic data although state-of-the-art approaches are not yet supporting the generation of modeling operations. To fill the gap, we propose a conceptual framework that combines modeling event logs, intelligent modeling assistants, and the generation of modeling operations using LLMs. In particular, the architecture comprises modeling components that help the designer specify the system, record its operation within a graphical modeling environment, and automatically recommend relevant operations. In addition, we generate a completely new dataset of modeling events by telling on the most prominent LLMs currently available. As a proof of concept, we instantiate the proposed framework using a set of existing modeling tools employed in industrial use cases within different European projects. To assess the proposed methodology, we first evaluate the capability of the examined LLMs to generate realistic modeling operations by relying on well-founded distance metrics. Then, we evaluate the recommended operations by considering real-world industrial modeling artifacts. Our findings demonstrate that LLMs can generate modeling events even though the overall accuracy is higher when considering human-based operations. In this respect, we see generative AI tools as an alternative when the modeling operations are not available to train traditional IMAs specifically conceived to support industrial practitioners.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {619–630},
numpages = {12},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3734523,
author = {Reddy, E Bhawani Eswar and Bhattacharyya, Sutirtha and Sarmah, Ankur and Nongpoh, Fedrick and Maddala, Karthik and Karfa, Chandan},
title = {LHS: LLM Assisted Efficient High-level Synthesis of Deep Learning Tasks},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3734523},
doi = {10.1145/3734523},
abstract = {Deep learning tasks, especially those involving complex convolution neural networks (CNNs), are computationally intensive and pose significant challenges when implemented on hardware. Accelerating these tasks is critical for improving performance. High-level Synthesis (HLS) has the potential to automate the efficient hardware accelerator designs directly from high-level C/C++ specification of trained machine learning (ML) models. Traditional HLS tools cannot synthesize certain high-level constructs, which require manual intervention. Many source code optimizations and the selection of pragmas for HLS optimizations are crucial for generating efficient hardware accelerators with HLS. However, both of these tasks are mostly manual efforts. Recently, Large Language Models (LLMs) have shown remarkable capabilities in various generative tasks. In this work, we explore the application of LLMs to remove these manual efforts in adapting HLS for ML accelerator designs. Our framework called LLM-assisted HLS, i.e., LHS, uses LLMs to automate the resolution of synthesis issues, ensuring compatibility with HLS tools. Furthermore, our framework automates the source code modification and optimization selection through pragma insertion steps, which are crucial for optimizing the synthesized design. Our experimental results with LHS demonstrate a significant improvement in latency for deep learning tasks with underlying complex CNN models without much area overhead. Our LHS allows us to achieve up to 2690 \texttimes{} latency improvement. Promisingly, LHS performs better than the state-of-the-art ML accelerator design tool hls4ml in 4 out of 6 cases in the context of latency improvement at the expense of area overhead (i.e., performance to hardware gain). This work highlights the potential of LLMs to assist and accelerate the HLS process, thereby creating more efficient hardware implementation for deep learning models.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = may,
keywords = {LLM, High-level synthesis, CNN}
}

@article{10.1145/3715317,
author = {Taleby Ahvanooey, Milad and Mazurczyk, Wojciech and Lee, Dongwon},
title = {Socioeconomic Threats of Deepfakes and the Role of Cyber-Wellness Education in Defense},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/3715317},
doi = {10.1145/3715317},
abstract = {In recent years, society has witnessed accelerated advancement in generative artificial intelligence (GenAI) technologies, which may be viewed as a double-edged sword. On one hand, GenAI tools can be used to create synthetic content legitimately. For example, advertising agencies may, with permission, generate celebrities’ images or videos using GenAI tools without putting them in front of cameras and thus reducing the overall cost of media construction. On the other hand, scammers may utilize GenAI tools to craft or edit artificial content (for example, texts, images, videos, or audio), so-called deepfakes, to mislead or deceive netizens with robocalls or voice cloning phishing, potentially causing detrimental consequences for society. This article briefly debates emerging socioeconomic threats of deepfakes in today’s society and how cyber-wellness (or digital media literacy) education can help netizens mitigate their risks.To the mitigate the risks of deepfakes, enhanced cyber-wellness programs are needed that empower both producers and consumers of generative AI–based content.},
journal = {Commun. ACM},
month = aug,
pages = {70–79},
numpages = {10}
}

@article{10.14778/3742728.3742734,
author = {Li, Changlun and Yang, Chenyu and Luo, Yuyu and Fan, Ju and Tang, Nan},
title = {Weak-to-Strong Prompts with Lightweight-to-Powerful LLMs for High-Accuracy, Low-Cost, and Explainable Data Transformation},
year = {2025},
issue_date = {April 2025},
publisher = {VLDB Endowment},
volume = {18},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3742728.3742734},
doi = {10.14778/3742728.3742734},
abstract = {Data transformation poses significant challenges due to the wide diversity in input data formats and different requirements. Existing approaches—including human-driven, algorithmic, and large language model (LLM)-based solutions—each exhibits trade-offs in terms of cost, accuracy, and the range of supported transformations. To address these limitations, we propose MegaTran, a novel framework for generating accurate and cost-effective data transformation code. MegaTran employs a two-stage process: Weak2StrongPrompt, which converts a user's weak prompt (a loosely specified user input) into a strong, structured prompt, and Prompt2Code, which generates transformation code based on this refined prompt. In Weak2StrongPrompt, a fine-tuned lightweight LLM predicts the transformation type and generates a detailed task description from the user's input. In Prompt2Code, a powerful LLM generates the corresponding transformation code, guided by two key optimizations: (1) Sanity-check Reflection with checklist, which iteratively debugs and refines the code by addressing errors; and (2) Lazy-RAG, a retrieval-augmented generation technique that retrieves relevant code snippets or documentation from external resources (e.g., GitHub, DataPrep) to enhance code quality. Extensive experiments show that MegaTran achieves results varying from +2.2\% to +26.1\% accuracy improvement compared with SoTA methods.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {2371–2384},
numpages = {14}
}

@inproceedings{10.1145/3678719.3685691,
author = {Garc\'{\i}a, Boni and Leotta, Maurizio and Ricca, Filippo and Whitehead, Jim},
title = {Use of ChatGPT as an Assistant in the End-to-End Test Script Generation for Android Apps},
year = {2024},
isbn = {9798400711091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678719.3685691},
doi = {10.1145/3678719.3685691},
abstract = {Automated testing is crucial in software development to ensure that applications perform as intended. However, generating automated End-to-End (E2E) tests can be time-consuming and challenging, especially for junior developers. This study investigates the use of ChatGPT, a popular Generative Artificial Intelligence (GenAI) model, as an assistant in developing automated E2E test scripts for Android apps. We present an empirical study that compares the effort required to create E2E test scripts and the resulting reliability of these tests using two treatments: manually and assisted by ChatGPT. We used Gherkin, a domain-specific language that allows non-technical practitioners to define test scenarios using a human-readable syntax. Our findings indicate that using ChatGPT significantly reduces the time required to develop automated test scripts without compromising the reliability of the scripts. Statistical analysis shows a notable reduction in development time for the ChatGPT-assisted group compared to the manual group, with a large effect size. While the reliability of the tests did not show a significant difference between the two groups, the results suggest practical benefits in terms of efficiency.},
booktitle = {Proceedings of the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation},
pages = {5–11},
numpages = {7},
keywords = {Android, E2E Automated Testing, Empirical Study, GenAI},
location = {Vienna, Austria},
series = {A-TEST 2024}
}

@article{10.1145/3746060,
author = {Berger, Thorsten and Mahmood, Wardah and Abu Zahra, Ramzi and Vassilevski, Igor and Burger, Andreas and Ji, Wenbin and Antkiewicz, Michal and Czarnecki, Krzysztof},
title = {Cost and Benefit of Tracing Features with Embedded Annotations},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3746060},
doi = {10.1145/3746060},
abstract = {Features are commonly used to describe the functional and non-functional characteristics of software. Especially agile development methods, such as SCRUM, FDD or XP, use features to plan and manage software development. Features are often the main units of software reuse, communication, and configuration, abstracting over code details. Especially in the age of generative AI, where feature requirements are specified as prompts and substantial code is cloned, codebases are becoming increasingly complex and redundant. This requires raising the level of abstraction at which we manage and evolve software systems. However, effectively using features requires knowing their precise locations within codebases, which is especially challenging when they are scattered across the codebase. Once implemented, the knowledge about a feature’s location quickly deteriorates when the software evolves or development teams change, requiring expensive recovery of features. This decades-old problem is known as the feature-location or concept assignment problem in software engineering, which researchers have—unsuccessfully over decades—tried to address with automated feature-location recovery techniques.The problem lies in the common belief that recording and maintaining feature locations during development is laborious and error-prone. In this study, we argue to the contrary. We hypothesize that such information can be effectively embedded into codebases, and that the arising costs will be amortized by the benefits of this information. We validated this hypothesis in a simulation study with three subjects systems: a smaller open-source system, a large commercial firmware system, and an open-source mobile app. We designed a lightweight code annotation technique and simulated its use as if annotations had been added, maintained, and exploited during the original development. We identified evolution patterns and measured the cost and benefit of these annotations. Our results show that not only the cost of adding annotations, but also that of maintaining them is negligible compared to the development and maintenance costs of the actual code. Embedding the annotations into the codebase significantly reduced their maintenance effort, because they naturally co-evolved with the code. The annotations provided a benefit for feature-related maintenance tasks, such as feature cloning or merging the clones into an integrated codebase, that exceeded the costs of using them.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
keywords = {features, feature location, software evolution, traceability, annotation system, simulation study}
}

@inproceedings{10.1109/ICSE55347.2025.00152,
author = {Liu, Qikang and He, Yang and Cai, Yanwen and Kwak, Byeongguk and Wang, Yuepeng},
title = {Synthesizing Document Database Queries Using Collection Abstractions},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00152},
doi = {10.1109/ICSE55347.2025.00152},
abstract = {Document databases are increasingly popular in various applications, but their queries are challenging to write due to the flexible and complex data model underlying document databases. This paper presents a synthesis technique that aims to generate document database queries from input-output examples automatically. A new domain-specific language is designed to express a representative set of document database queries in an algebraic style. Furthermore, the synthesis technique leverages a novel abstraction of collections for deduction to efficiently prune the search space and quickly generate the target query. An evaluation of 110 benchmarks from various sources shows that the proposed technique can synthesize 108 benchmarks successfully. On average, the synthesizer can generate document database queries from a small number of input-output examples within tens of seconds.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {476–488},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3726302.3730117,
author = {Hadad, Guy and Roitman, Haggai and Eshel, Yotam and Shapira, Bracha and Rokach, Lior},
title = {X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730117},
doi = {10.1145/3726302.3730117},
abstract = {As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ''X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25\% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50\%-75\% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1497–1507},
numpages = {11},
keywords = {cross-domain recommendation, dynamic integration, language models, lora, natural language processing, parameter and data efficiency},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3635059.3635062,
author = {Giarelis, Nikolaos and Mastrokostas, Charalampos and Siachos, Ilias and Karacapilidis, Nikos},
title = {A Review of Greek NLP Technologies for Chatbot Development},
year = {2024},
isbn = {9798400716263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635059.3635062},
doi = {10.1145/3635059.3635062},
abstract = {The advent of Generative AI has certainly boosted the interest in developing innovative chatbot applications. Despite a vast amount of machine learning (ML) and natural language processing (NLP) research and English language resources that greatly improve chatbot technology, the corresponding research and resources for the Greek language are limited. The contribution of this paper is twofold: (i) it reports on the state-of-the-art research in Greek NLP, as far as language resources, embeddings-based techniques, deep learning models, and existing chatbot applications are concerned; (ii) it offers a set of insights on current NLP models and chatbot implementation methodologies, and outlines a set of pending issues and future research directions.},
booktitle = {Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics},
pages = {15–20},
numpages = {6},
keywords = {Deep Learning, Greek Language, Large Language Models, Review, Text Classification, Text Summarization, Word Embeddings},
location = {Lamia, Greece},
series = {PCI '23}
}

@inproceedings{10.1145/3711896.3736838,
author = {Tang, Ziyi and Chen, Zechuan and Yang, Jiarui and Mai, Jiayao and Zheng, Yongsen and Wang, Keze and Chen, Jinrui and Lin, Liang},
title = {AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3736838},
doi = {10.1145/3711896.3736838},
abstract = {Alpha mining, a critical component in quantitative investment, focuses on discovering predictive signals for future asset returns in increasingly complex financial markets. However, the pervasive issue of alpha decay-where factors lose their predictive power over time-poses a significant challenge for alpha mining. Traditional methods such as genetic programming are prone to rapid alpha decay, primarily due to their susceptibility to overfitting. At the same time, approaches driven by Large Language Models (LLMs), despite their promise, often fail to impose regularization against factor homogenization-resulting in crowded signals and accelerated decay. To address this challenge, we propose AlphaAgent, an autonomous framework that effectively integrates LLM-driven agents with ad hoc regularization for mining decay-resistant alpha factors. AlphaAgent employs three key mechanisms: (i) originality enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alphas(ii) hypothesis-factor alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (iii) complexity control via AST-based structural constraints, preventing over-engineered constructions that are prone to overfitting. These mechanisms collectively guide the alpha generation process to balance originality, financial rationale, and adaptability to evolving market conditions, mitigating the risk of alpha decay. Extensive evaluations show that AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay across bull and bear markets, consistently delivering significant alpha in Chinese CSI 500 and U.S. S&amp;P 500 markets over the past four years. Notably, AlphaAgent showcases remarkable resistance to alpha decay, elevating the potential for yielding powerful factors.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {2813–2822},
numpages = {10},
keywords = {alpha mining, autonomous agents, large language models, quantitative investment},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@article{10.1145/3712008,
author = {Burgue\~{n}o, Lola and Di Ruscio, Davide and Sahraoui, Houari and Wimmer, Manuel},
title = {Automation in Model-Driven Engineering: A Look Back, and Ahead},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712008},
doi = {10.1145/3712008},
abstract = {Model-Driven Engineering (MDE) provides a huge body of knowledge of automation for many different engineering tasks, especially those involving transitioning from design to implementation. With the huge progress made in AI, questions arise about the future of MDE, such as how existing MDE techniques and technologies can be improved or how other activities that currently lack dedicated support can also be automated. However, at the same time, it has to be revisited where and how models should be used to keep the engineers in the loop for creating, operating, and maintaining complex systems. To trigger dedicated research on these open points, we discuss the history of automation in MDE and present perspectives on how automation in MDE can be further improved and which obstacles have to be overcome in both the medium and long-term.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {122},
numpages = {25},
keywords = {Model-Driven Engineering (MDE), automation}
}

@inproceedings{10.1145/3532213.3532254,
author = {Zhang, Xiangliang and Jia, Yangli and Zhang, Zhenling and Kang, Qi and Zhang, Yongchen and Jia, Hongling},
title = {Improving End-to-End Biomedical Question Answering System},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532254},
doi = {10.1145/3532213.3532254},
abstract = {Biomedical question answering refers to extracting an answer based on given questions and related documents. Existing biomedical question answering research either focuses on a specific stage, such as machine reading comprehension, or uses traditional rule-based methods and ontology with complex construction processes. In this paper, we demonstrate the application of simple but powerful neural-based approaches in improving the end-to-end biomedical question answering system. We employ the BM25-based documents retriever, BERT-based neural ranker, and an answer extraction stage using the BioBERT pre-trained language model. In view of the lack of sufficient training data in the biomedical domain, domain adaptation and data augmentation are adopted to address the question answering task, so as to further reinforce the system performance. Based on our self-built standard large-volume retrieve corpus and neural ranker corpus, we get competitive results on BioASQ8b.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {274–279},
numpages = {6},
keywords = {Biomedical question answering, Data augmentation, Document re-ranking},
location = {Tianjin, China},
series = {ICCAI '22}
}

@inproceedings{10.1109/ICSE55347.2025.00197,
author = {Mao, Ziyu and Wang, Jingyi and Sun, Jun and Qin, Shengchao and Xiong, Jiawen},
title = {LLM-Aided Automatic Modeling for Security Protocol Verification},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00197},
doi = {10.1109/ICSE55347.2025.00197},
abstract = {Symbolic protocol analysis serves as a pivotal technique for protocol design, security analysis, and the safeguarding of information assets. Several modern tools such as Tamarin and ProVerif have been proven successful in modeling and verifying real-world protocols, including complex protocols like TLS 1.3 and 5G AKA. However, developing formal models for protocol verification is a non-trivial task, which hinders the wide adoption of these powerful tools in practical protocol analysis.In this work, we aim to bridge the gap by developing an automatic method for generating symbolic protocol models using Large Language Models (LLMs) from protocol descriptions in natural language document. Although LLMs are powerful in various code generation tasks, it is shown to be ineffective in generating symbolic models (according to our empirical study). Therefore, rather than applying LLMs naively, we carefully decompose the symbolic protocol modeling task into several stages so that a series of formal models are incrementally developed towards generating the final correct symbolic model. Specifically, we apply LLMs for semantic parsing, enable lightweight manual interaction for disambiguation, and develop algorithms to transform the intermediate models for final symbolic model generation. To ensure the correctness of the generated symbolic model, each stage is designed based on a formal execution model and the model transformations are proven sound. To the best of our knowledge, this is the first work aiming to generate symbolic models for protocol verification from natural language documents. We also introduce a benchmark for symbolic protocol model generation, with 18 real-world security protocol's text description and their corresponding symbolic models. We then demonstrate the potential of our tool, which successfully generated correct models of moderate scale in 10 out of 18 cases. Our tool is released at [1].},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {642–654},
numpages = {13},
keywords = {automatic modeling, symbolic analysis, LLMs},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3706598.3713932,
author = {Kretzer, Felix and Kolthoff, Kristian and Bartelt, Christian and Ponzetto, Simone Paolo and Maedche, Alexander},
title = {Closing the Loop between User Stories and GUI Prototypes: An LLM-Based Assistant for Cross-Functional Integration in Software Development},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713932},
doi = {10.1145/3706598.3713932},
abstract = {Graphical user interfaces (GUIs) are at the heart of almost every software we encounter. GUIs are often created through a collaborative effort involving UX designers, product owners, and software developers, constantly facing changing requirements. Historically, problems in GUI development include a fragmented, poorly integrated tool landscape and high synchronization efforts between stakeholders. Recent approaches suggest using large language models (LLMs) to recognize requirements fulfillment in GUIs and automatically propose new GUI components. Based on ten interviews with practitioners, this paper proposes an LLM-based assistant as a Figma plug-in that bridges the gap between user stories and GUI prototyping. We evaluated the prototype with 40 users and 40 crowd-workers, showing that the effectiveness of GUI creation is improved by using LLMs to detect requirements’ completion and generate new GUI components. We derive design rationales to support cross-functional integration in software development, ensuring that our plug-in integrates well into established processes.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {879},
numpages = {19},
keywords = {GUI Prototypes; User Stories; Requirements; Assistance},
location = {
},
series = {CHI '25}
}

@article{10.1145/3712006,
author = {Autili, Marco and De Sanctis, Martina and Inverardi, Paola and Pelliccione, Patrizio},
title = {Engineering Digital Systems for Humanity: A Research Roadmap},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712006},
doi = {10.1145/3712006},
abstract = {As testified by new regulations like the European AI Act, worries about the human and societal impact of (autonomous) software technologies are becoming of public concern. Human, societal, and environmental values, alongside traditional software quality, are increasingly recognized as essential for sustainability and long-term well-being. Traditionally, systems are engineered taking into account business goals and technology drivers. Considering the growing awareness in the community, in this article, we argue that engineering of systems should also consider human, societal, and environmental drivers. Then, we identify the macro and technological challenges by focusing on humans and their role while co-existing with digital systems. The first challenge considers humans in a proactive role when interacting with digital systems, i.e., taking initiative in making things happen instead of reacting to events. The second concerns humans having a reactive role in interacting with digital systems, i.e., humans interacting with digital systems as a reaction to events. The third challenge focuses on humans with a passive role, i.e., they experience, enjoy or even suffer the decisions and/or actions of digital systems. The fourth challenge concerns the duality of trust and trustworthiness, with humans playing any role. Building on the new human, societal, and environmental drivers and the macro and technological challenges, we identify a research roadmap of digital systems for humanity. The research roadmap is concretized in a number of research directions organized into four groups: development process, requirements engineering, software architecture and design, and verification and validation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {130},
numpages = {33},
keywords = {Human values, Societal values, Environmental values, Research directions, Research roadmap, Software engineering}
}

@inproceedings{10.1145/3605098.3635889,
author = {Arrieta, Kutz and Fillottrani, Pablo R and Keet, C. Maria},
title = {CoSMo: A multilingual modular language for Content Selection Modelling},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605098.3635889},
doi = {10.1145/3605098.3635889},
abstract = {Representing snippets of information abstractly is a task that needs to be performed for various purposes, such as database view specification and the first stage in the natural language generation pipeline for generative AI from structured input, i.e., the content selection stage to determine what needs to be verbalised. For the Abstract Wikipedia project, requirements analysis revealed that such an abstract representation requires multilingual modelling, content selection covering declarative content and functions, and both classes and instances. There is no modelling language that meets either of the three features, let alone a combination. Following a rigorous language design process inclusive of broad stakeholder consultation, we created CoSMo, a novel Content Selection Modeling language that meets these and other requirements so that it may be useful both in Abstract Wikipedia as well as other contexts. We describe the design process, rationale and choices, the specification, and preliminary evaluation of the language.},
booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
pages = {706–713},
numpages = {8},
keywords = {modeling language, query language, wikidata, multilingualism},
location = {Avila, Spain},
series = {SAC '24}
}

@inproceedings{10.1109/ICSE55347.2025.00173,
author = {Fakhoury, Sarah and Kuppe, Markus and Lahiri, Shuvendu K. and Ramananandro, Tahina and Swamy, Nikhil},
title = {3DGen: AI-Assisted Generation of Provably Correct Binary Format Parsers},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00173},
doi = {10.1109/ICSE55347.2025.00173},
abstract = {Improper parsing of attacker-controlled input is a leading source of software security vulnerabilities, especially when programmers transcribe informal format descriptions in RFCs into efficient parsing logic in low-level, memory unsafe languages. Several researchers have proposed formal specification languages for data formats from which efficient code can be extracted. However, distilling informal requirements into formal specifications is challenging and, despite their benefits, new, formal languages are hard for people to learn and use.In this work, we present 3DGen, a framework that makes use of AI agents to transform mixed informal input, including natural language documents (i.e., RFCs) and example inputs into format specifications in a language called 3D. To support humans in understanding and trusting the generated specifications, 3DGen uses symbolic methods to also synthesize test inputs that can be validated against an external oracle. Symbolic test generation also helps in distinguishing multiple plausible solutions. Through a process of repeated refinement, 3DGen produces a 3D specification that conforms to a test suite, and which yields safe, efficient, provably correct, parsing code in C.We have evaluated 3DGen on 20 Internet standard formats, demonstrating the potential for AI-agents to produce formally verified C code at a non-trivial scale. A key enabler is the use of a domain-specific language to limit AI outputs to a class for which automated, symbolic analysis is tractable.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2535–2547},
numpages = {13},
keywords = {code generation, agentic ai systems, trustworthy AI programming},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3650212.3680314,
author = {Nguyen, Thanh-Dat and Do-Viet, Tung and Nguyen-Duy, Hung and Luu, Tuan-Hai and Le, Hung and Le, Bach and Thongtanunam, Patanamon},
title = {VRDSynth: Synthesizing Programs for Multilingual Visually Rich Document Information Extraction},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680314},
doi = {10.1145/3650212.3680314},
abstract = {Businesses often need to query visually rich documents (VRDs), e.g., purchase receipts, medical records, and insurance forms, among many other forms from multiple vendors, to make informed decisions. As such, several techniques have been proposed to automatically extract independent entities of interest from VRDs such as extracting price tags from purchase receipts, etc. However, for extracting semantically linked entities, such as finding corresponding price tags for each item, these techniques either have limited capability in handling new layouts, e.g., template-based approaches, or require extensive amounts of pre-training data and do not perform well, e.g., deep-learning approaches.                                                                In this work, we introduce a program synthesis method, namely VRDSynth, to automatically generate programs to extract entity relations from multilingual VRDs. Two key novelties, which empower VRDSynth to tackle flexible layouts while requiring no pre-training data for extracting entity relations, include: (1) a new domain-specific language (DSL) to effectively capture the spatial and textual relations between document entities, and (2) a novel synthesis algorithm that makes use of frequent spatial relations between entities to construct initial programs, equivalent reduction to prune the search space, and a combination of positive, negative, and mutually exclusive programs to improve the coverage of programs.                                                                We evaluate our method on two popular VRD understanding benchmarks, namely FUNSD and XFUND, on the semantic entity linking task, consisting of 1,600 forms in 8 different languages. Experiments show that VRDSynth, despite having no prior pre-training data, outperforms the state-of-the-art pre-trained deep-learning approach, namely LayoutXLM, in 5 out of 8 languages. Noticeably, VRDSynth achieved an improvement of 42\% over LayoutXLM in terms of F1 score on FUNSD while being complementary to LayoutXLM in 7/8 languages. Regarding efficiency, VRDSynth significantly improves the memory footprint required for storage and inference over LayoutXLM (1M and 380MB versus that of 1.48GB and 3GB required by LayoutXLM), while maintaining similar time efficiency despite the speed differences between the languages used for implementation (Python vs C++).},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {704–716},
numpages = {13},
keywords = {Automatic Programming, Information Extraction, Program Synthesis, Programming By Example, Visually-Rich Document},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3613904.3642319,
author = {Barnaby, Celeste and Chen, Qiaochu and Wang, Chenglong and Dillig, Isil},
title = {PhotoScout: Synthesis-Powered Multi-Modal Image Search},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642319},
doi = {10.1145/3613904.3642319},
abstract = {Due to the availability of increasingly large amounts of visual data, there is a growing need for tools that can help users find relevant images. While existing tools can perform image retrieval based on similarity or metadata, they fall short in scenarios that necessitate semantic reasoning about the content of the image. This paper explores a new multi-modal image search approach that allows users to conveniently specify and perform semantic image search tasks. With our tool, PhotoScout, the user interactively provides natural language descriptions, positive and negative examples, and object tags to specify their search tasks. Under the hood, PhotoScout is powered by a program synthesis engine that generates visual queries in a domain-specific language and executes the synthesized program to retrieve the desired images. In a study with 25 participants, we observed that PhotoScout allows users to perform image retrieval tasks more accurately and with less manual effort.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {896},
numpages = {15},
keywords = {Interface design, multi-modal interfaces, program synthesis},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3534678.3539460,
author = {Yang, Qingping and Cao, Yixuan and Luo, Ping},
title = {Numerical Tuple Extraction from Tables with Pre-training},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539460},
doi = {10.1145/3534678.3539460},
abstract = {Tables are omnipresent on the web and in various vertical domains, storing massive amounts of valuable data. However, the great flexibility in the table layout hinders the machine from understanding this valuable data. In order to unlock and utilize knowledge from tables, extracting data as numerical tuples is the first and critical step. As a form of relational data, numerical tuples have direct and transparent relationships between their elements and are therefore easy for machines to use. Extracting numerical tuples requires a deep understanding of intricate correlations between cells. The correlations are presented implicitly in texts and visual appearances of tables, which can be roughly classified into Hierarchy and Juxtaposition. Although many studies have made considerable progress in data extraction from tables, most of them only consider hierarchical relationships but neglect the juxtapositions. Meanwhile, they only evaluate their methods on relatively small corpora. This paper proposes a new framework to extract numerical tuples from tables and evaluate it on a large test set. Specifically, we convert this task into a relation extraction problem between cells. To represent cells with their intricate correlations in tables, we propose a BERT-based pre-trained language model, TableLM, to encode tables with diverse layouts. To evaluate the framework, we collect a large finance dataset that includes 19,264 tables and 604K tuples. Extensive experiments on the dataset are conducted to demonstrate the superiority of our framework compared to a well-designed baseline.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2233–2241},
numpages = {9},
keywords = {pre-training, tabular representation, tuple extraction},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1145/3746226,
author = {Chafik, Salmane and Ezzini, Saad and Berrada, Ismail},
title = {Towards Automating Domain-Specific Data Generation for Text-to-SQL: A Comprehensive Approach},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3746226},
doi = {10.1145/3746226},
abstract = {As software systems increasingly rely on natural language interfaces, ensuring the reliability of these systems is crucial. One critical component is the ability to accurately translate natural language queries into corresponding SQL queries, a field known as Text-to-SQL. However, the scarcity of high-quality, large-scale, and domain-specific Text-to-SQL datasets hinders the development of reliable and robust models. To tackle these challenges, we propose SelectCraft, a novel automatic generation approach designed to create realistic Text-to-SQL datasets tailored to specific domains. Our method leverages existing databases and their structures to generate complex text-SQL pairs that mirror real-world usage scenarios. As a proof of concept, we have successfully generated a substantial financial Text-to-SQL dataset, denominated as BanQies, encompassing over 1 million samples utilizing our proposed approach. Moreover, we introduce BanQL, a new large language model (LLM) based on StarCoder2, a state-of-the-art code-based LLM, and fine-tuned on our newly created dataset. We evaluate BanQL performance against several state-of-the-art models, demonstrating significant enhancements in accuracy and generalizability, highlighting the advantages of incorporating domain-specific data in Text-to-SQL tasks. We firmly believe that our contributions have the potential to improve the overall reliability of Text-to-SQL software systems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
keywords = {Text-to-SQL, SQL-to-Text, Code Generation, Data Generation, Synthetic Data}
}

@inproceedings{10.1145/3613904.3642016,
author = {Arawjo, Ian and Swoopes, Chelse and Vaithilingam, Priyan and Wattenberg, Martin and Glassman, Elena L.},
title = {ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642016},
doi = {10.1145/3613904.3642016},
abstract = {Evaluating outputs of large language models (LLMs) is challenging, requiring making—and making sense of—many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {304},
numpages = {18},
keywords = {auditing, language models, prompt engineering, toolkits, visual programming environments},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3728913,
author = {Zhang, Zhiyu and Li, Longxing and Liang, Ruigang and Chen, Kai},
title = {Unlocking Low Frequency Syscalls in Kernel Fuzzing with Dependency-Based RAG},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728913},
doi = {10.1145/3728913},
abstract = {Most coverage-guided kernel fuzzers test operating system kernels based on syscall sequence synthesis. However, there are still syscalls rarely or not covered (called low frequency syscalls, LFS) in a period of fuzzing, meaning the relevant code branches remain unexplored. This is due to the complex dependencies of the LFS and mutation uncertainty, which makes it difficult for fuzzers to generate corresponding syscall sequences. Since many kernel fuzzers can dynamically learn syscall dependencies from the current corpus based on the choice table mechanism, providing comprehensive and high-quality seeds could help fuzzers cover LFS. However, constructing such seeds relies heavily on expert experience to resolve the syscall dependencies. In this paper, we propose SyzGPT, the first kernel fuzzing framework to automatically generate effective seeds for LFS via Large Language Model (LLM). We leverage a dependency-based retrieval-augmented generation (DRAG) method to unlock the potential of LLM and design a series of steps to improve the effectiveness of the generated seeds. First, SyzGPT automatically extracts syscall dependencies from the existing documentation via LLM. Second, SyzGPT retrieves programs from the fuzzing corpus based on the dependencies to construct adaptive context for LLM. Last, SyzGPT periodically generates and repairs seeds with feedback to enrich the fuzzing corpus for LFS. We propose a novel set of evaluation metrics for seed generation in kernel domain. Our evaluation shows that SyzGPT can generate seeds with a high valid rate of 87.84\% and can be extended to offline and fine-tuned LLMs. Compared to seven state-of-the-art kernel fuzzers, SyzGPT improves code coverage by 17.73\%, LFS coverage by 58.00\%, and vulnerability detection by 323.22\% on average. Besides, SyzGPT independently discovered 26 unknown kernel bugs (10 are LFS-related), with 11 confirmed.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA038},
numpages = {23},
keywords = {Kernel Fuzzing, RAG, Seed Generation, Syscall Dependency}
}

@article{10.1145/3709155,
author = {Villena, Fabi\'{a}n and Quiroga, Tamara and Dunstan, Jocelyn},
title = {Clinical Analogy Resolution Performance for Foundation Language Models},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3709155},
doi = {10.1145/3709155},
abstract = {Using extensive data sources to create foundation language models has revolutionized the performance of deep learning-based architectures. This remarkable improvement has led to state-of-the-art results for various downstream NLP tasks, including clinical tasks. However, more research is needed to measure model performance intrinsically, especially in the clinical domain. We revisit the use of analogy questions as an effective method to measure the intrinsic performance of language models for the clinical domain in English. We tested multiple Transformer-based language models s over analogy questions constructed from the Unified Medical Language System (UMLS), a massive knowledge graph of clinical concepts. Our results show that large language models are significantly more performant for analogy resolution than small language models. Similarly, domain-specific language models perform better than general domain language models. We also found a correlation between intrinsic and extrinsic performance, validated through PubMedQA extrinsic task. Creating clinical-specific and language-specific language models is essential for advancing biomedical and clinical NLP and will ensure a valid application in clinical practice. Finally, given that our proposed intrinsic test is based on a term graph available in multiple languages, the dataset can be built to measure the performance of models in languages other than English.},
journal = {ACM Trans. Comput. Healthcare},
month = apr,
articleno = {25},
numpages = {12},
keywords = {Foundation Models, Clinical NLP, Intrinsic Tests}
}

@inproceedings{10.1145/3638530.3664121,
author = {Guo, Ping and Liu, Fei and Lin, Xi and Zhao, Qingchuan and Zhang, Qingfu},
title = {L-AutoDA: Large Language Models for Automatically Evolving Decision-based Adversarial Attacks},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664121},
doi = {10.1145/3638530.3664121},
abstract = {In the rapidly evolving field of machine learning, adversarial attacks pose a significant threat to the robustness and security of models. Amongst these, decision-based attacks are particularly insidious due to their nature of requiring only the model's decision output, which makes them notably challenging to counteract. This paper presents L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), an innovative methodology that harnesses the generative capabilities of large language models (LLMs) to streamline the creation of such attacks. L-AutoDA employs an evolutionary strategy, where iterative interactions with LLMs lead to the autonomous generation of potent attack algorithms, thereby reducing human intervention. The performance of L-AutoDA was evaluated on the CIFAR-10 dataset, where it demonstrated substantial superiority over existing baseline methods in terms of success rate and computational efficiency. Ultimately, our results highlight the formidable utility of language models in crafting adversarial attacks and reveal promising directions for constructing more resilient AI systems.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1846–1854},
numpages = {9},
keywords = {large language models, adversarial attacks, automated algorithm design, evolutionary algorithms},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3711896.3736879,
author = {Zhang, Yiqing and Liu, Xiaozhong and Murai, Fabricio},
title = {CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3736879},
doi = {10.1145/3711896.3736879},
abstract = {Many existing models for clinical trial outcome prediction are optimized using task-specific loss functions on trial phase-specific data. While this scheme may boost prediction for common diseases and drugs, it can hinder the learning of generalizable representations, leading to more false positives/negatives. To address this limitation, we introduce CLaDMoP, a new pre-training approach for clinical trial outcome prediction, alongside the Successful Clinical Trials dataset (SCT), specifically designed for this task. CLaDMoP leverages a Large Language Model-to encode trials' eligibility criteria-linked to a lightweight Drug-Molecule branch through a novel multi-level fusion technique. To efficiently fuse long embeddings across levels, we incorporate a grouping block, drastically reducing computational overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training on a ''pair matching'' proxy task. Compared to established zero-shot and few-shot baselines, our method significantly improves both PR-AUC and ROC-AUC, especially for phase I and phase II trials. We further evaluate and perform ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome Prediction (TOP) benchmark. CLaDMoP achieves up to 10.5\% improvement in PR-AUC and 3.6\% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP, highlighting its potential for clinical trial outcome prediction. Code and SCT dataset can be downloaded from https://github.com/murai-lab/CLaDMoP.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {3901–3911},
numpages = {11},
keywords = {clinical trial outcome prediction, llms, multi-modal data fusion, representation learning, self-supervised pre-training},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3696410.3714790,
author = {Chen, Eason and Tang, Xinyi and Xiao, Zimo and Li, Chuangji and Li, Shizhuo and Wu, Tingguan and Wang, Siyun and Chalkias, Kostas Kryptos},
title = {SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability on Non-Open-Source Blockchain Smart Contract},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714790},
doi = {10.1145/3696410.3714790},
abstract = {The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering. Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33\% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance. In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD. MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization. Moreover, MAD's methodology could potentially extend to other smart contract languages, like Solidity, further enhancing Web3 transparency.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {1567–1576},
numpages = {10},
keywords = {auditing tools, large language models, move, prompt engineering, smart contract, sui, transparency, web applications, web3},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3670474.3685948,
author = {Batten, Christopher and Pinckney, Nathaniel and Liu, Mingjie and Ren, Haoxing and Khailany, Brucek},
title = {PyHDL-Eval: An LLM Evaluation Framework for Hardware Design Using Python-Embedded DSLs},
year = {2024},
isbn = {9798400706998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670474.3685948},
doi = {10.1145/3670474.3685948},
abstract = {Embedding hardware design frameworks within Python is a promising technique to improve the productivity of hardware engineers. At the same time, there is significant interest in using large-language models (LLMs) to improve key chip design tasks. This paper describes PyHDL-Eval, a new framework for evaluating LLMs on specification-to-RTL tasks in the context of Python-embedded domain-specific languages (DSLs). The framework includes 168 problems, Verilog reference solutions, Verilog test benches, Python test scripts, and workflow orchestration scripts. We use the framework to conduct a detailed case study comparing five LLMs (CodeGemma 7B, Llama3 8B/70B, GPT4, and GPT4 Turbo) targeting Verilog and five Python-embedded DSLs (PyMTL3, PyRTL, MyHDL, Migen, and Amaranth). Our results demonstrate the promise of in-context learning when applied to smaller models (e.g., pass rate for CodeGemma 7B improves from 14.9\% to 32.7\% on Verilog) and Python-embedded DSLs (e.g., pass rate for LLama3 70B improves from 0.6\% to 33.0\% on PyMTL3). We find LLMs perform better when targeting Verilog as compared to Python-embedded DSLs (e.g., pass rate for GPT4 Turbo is 72.2\% on Verilog and 29.8-62.0\% on the Python-embedded DSLs) despite using a popular general-purpose host language. PyHDL-Eval will serve as a useful framework for future research at the intersection of Python-embedded DSLs and LLMs.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
articleno = {10},
numpages = {17},
keywords = {Python-embedded domain-specific languages, hardware description languages, large language models},
location = {Salt Lake City, UT, USA},
series = {MLCAD '24}
}

@inproceedings{10.1145/3652620.3688557,
author = {Manellanga, Rajitha and David, Istvan},
title = {Participatory and Collaborative Modeling of Sustainable Systems: A Systematic Review},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688557},
doi = {10.1145/3652620.3688557},
abstract = {Sustainability has become a key characteristic of modern systems. Unfortunately, the convoluted nature of sustainability limits its understanding and hinders the design of sustainable systems. Thus, cooperation among a diverse set of stakeholders is paramount to sound sustainability-related decisions. Collaborative modeling has demonstrated benefits in facilitating cooperation between technical experts in engineering problems; but fails to include non-technical stakeholders in the modeling endeavor. In contrast, participatory modeling excels in facilitating high-level modeling among a diverse set of stakeholders, often of non-technical profiles; but fails to generate actionable engineering models. To instigate a convergence between the two disciplines, we systematically survey the field of collaborative and participatory modeling for sustainable systems. By analyzing 24 primary studies (published until June 2024), we identify common challenges, cooperation models, modeling formalisms and tools; and recommend future avenues of research.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {645–654},
numpages = {10},
keywords = {collaboration, MDE, model-driven, model-based, participatory modeling, survey, sustainability, systematic literture review},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3510003.3510125,
author = {Chai, Yitian and Zhang, Hongyu and Shen, Beijun and Gu, Xiaodong},
title = {Cross-domain deep code search with meta learning},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510125},
doi = {10.1145/3510003.3510125},
abstract = {Recently, pre-trained programming language models such as CodeBERT have demonstrated substantial gains in code search. Despite their success, they rely on the availability of large amounts of parallel data to fine-tune the semantic mappings between queries and code. This restricts their practicality in domain-specific languages with relatively scarce and expensive data. In this paper, we propose CDCS, a novel approach for domain-specific code search. CDCS employs a transfer learning framework where an initial program representation model is pre-trained on a large corpus of common programming languages (such as Java and Python), and is further adapted to domain-specific languages such as Solidity and SQL. Unlike cross-language CodeBERT, which is directly fine-tuned in the target language, CDCS adapts a few-shot meta-learning algorithm called MAML to learn the good initialization of model parameters, which can be best reused in a domain-specific language. We evaluate the proposed approach on two domain-specific languages, namely Solidity and SQL, with model transferred from two widely used languages (Python and Java). Experimental results show that CDCS significantly outperforms conventional pre-trained code models that are directly fine-tuned in domain-specific languages, and it is particularly effective for scarce data.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {487–498},
numpages = {12},
keywords = {code search, deep learning, few-shot learning, meta learning, pre-trained code models},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.5555/3643142.3643346,
author = {Bellamy, Elijah and Beskow, David M.},
title = {Using Simulated Narratives to Understand Attribution in the Information Dimension},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {Conducting a measured response to cyber or information attack is predicated on attribution. When these operations are conducted covertly or through proxies, uncertainty in attribution limits response options. To increase attribution certainty in the information dimension, the authors have developed a suite of supervised machine learning models that attribute an emerging narrative to historical narratives from known actors. These models were first developed on simulated narratives produced with a Large Language Model. Once the supervised classification models were developed and tested on the simulated narratives, they are evaluated on known actor social media narratives from three known actors. The attribution models are language agnostic and offer one-vs-rest and multi-class options. All models performed at relatively high accuracy and can provide decision support for cyber response decisions.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2460–2469},
numpages = {10},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@inbook{10.1145/3728725.3728757,
author = {Xia, Weiyi and Zhang, Yinggang and Zhao, Ben and Liu, Wei and Han, Linjie and Ye, Qifu},
title = {Intelligent PLC Code Generation in HCPS 2.0: A Multi-dimensional Taxonomy and Evolutionary Framework},
year = {2025},
isbn = {9798400713453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3728725.3728757},
abstract = {The manufacturing industry is accelerating towards the New-Generation Intelligent Manufacturing, where the automatic generation of Programmable Logic Controllers (PLCs) code serves as a key technology, crucial for enhancing the intelligence and efficiency of manufacturing systems. Traditional manual programming has become increasingly inadequate to meet the demands for flexibility and real-time performance in intelligent manufacturing. This paper reviews the integration of intelligent manufacturing and PLC technologies, highlighting the core role of Human-Cyber-Physical Systems (HCPS) and demonstrating how it effectively guides theoretical research and engineering practices in new-generation intelligent manufacturing. We innovatively classify automatic PLC code generation methods through multiple dimensions and propose a new framework for Structured Text (ST) code generation. Finally, this paper discusses future trends in controller code generation, identifying multimodal-driven intelligent generation, adaptive learning and real-time optimization, and cloud-edge collaborative intelligent generation as significant directions, forming a technical roadmap of "parallel promotion and integrated development" to support comprehensive intelligent transformation and upgrading of manufacturing.},
booktitle = {Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security},
pages = {202–212},
numpages = {11}
}

@article{10.1145/3658147,
author = {He, Kai and Yao, Kaixin and Zhang, Qixuan and Yu, Jingyi and Liu, Lingjie and Xu, Lan},
title = {DressCode: Autoregressively Sewing and Generating Garments from Text Guidance},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3658147},
doi = {10.1145/3658147},
abstract = {Apparel's significant role in human appearance underscores the importance of garment digitalization for digital human creation. Recent advances in 3D content creation are pivotal for digital human creation. Nonetheless, garment generation from text guidance is still nascent. We introduce a text-driven 3D garment generation framework, DressCode, which aims to democratize design for novices and offer immense potential in fashion design, virtual try-on, and digital human creation. We first introduce SewingGPT, a GPT-based architecture integrating cross-attention with text-conditioned embedding to generate sewing patterns with text guidance. We then tailor a pre-trained Stable Diffusion to generate tile-based Physically-based Rendering (PBR) textures for the garments. By leveraging a large language model, our framework generates CG-friendly garments through natural language interaction. It also facilitates pattern completion and texture editing, streamlining the design process through user-friendly interaction. This framework fosters innovation by allowing creators to freely experiment with designs and incorporate unique elements into their work. With comprehensive evaluations and comparisons with other state-of-the-art methods, our method showcases superior quality and alignment with input prompts. User studies further validate our high-quality rendering results, highlighting its practical utility and potential in production settings. Our project page is https://IHe-KaiI.github.io/DressCode/.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {72},
numpages = {13},
keywords = {garment generation, sewing patterns, autoregressive model}
}

@article{10.1145/3357334,
author = {Saini, Naveen and Saha, Sriparna and Bhattacharyya, Pushpak and Tuteja, Himanshu},
title = {Textual Entailment--Based Figure Summarization for Biomedical Articles},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3357334},
doi = {10.1145/3357334},
abstract = {This article proposes a novel unsupervised approach (FigSum++) for automatic figure summarization in biomedical scientific articles using a multi-objective evolutionary algorithm. The problem is treated as an optimization problem where relevant sentences in the summary for a given figure are selected based on various sentence scoring features (or objective functions), such as the textual entailment score between sentences in the summary and a figure’s caption, the number of sentences referring to that figure, semantic similarity between sentences and a figure’s caption, and the number of overlapping words between sentences and a figure’s caption. These objective functions are optimized simultaneously using multi-objective binary differential evolution (MBDE). MBDE consists of a set of solutions, and each solution represents a subset of sentences to be selected in the summary. MBDE generally uses a single differential evolution variant, but in the current study, an ensemble of two different differential evolution variants measuring diversity among solutions and convergence toward global optimal solution, respectively, is employed for efficient search. Usually, in any summarization system, diversity among sentences (called anti-redundancy) in the summary is a very critical feature, and it is calculated in terms of similarity (like cosine similarity) among sentences. In this article, a new way of measuring diversity in terms of textual entailment is proposed. To represent the sentences of the article in the form of numeric vectors, the recently proposed BioBERT pre-trained language model in biomedical text mining is utilized. An ablation study has also been presented to determine the importance of different objective functions. For evaluation of the proposed technique, two benchmark biomedical datasets containing 91 and 84 figures are considered. Our proposed system obtains 5\% and 11\% improvements in terms of the F-measure metric over two datasets, compared to the state-of-the-art unsupervised methods.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {35},
numpages = {24},
keywords = {Figure-assisted text summarization, evolutionary computing, multi-objective optimization (MOO), textual entailment}
}

@article{10.1145/3571226,
author = {Cambronero, Jos\'{e} and Gulwani, Sumit and Le, Vu and Perelman, Daniel and Radhakrishna, Arjun and Simon, Clint and Tiwari, Ashish},
title = {FlashFill++: Scaling Programming by Example by Cutting to the Chase},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {POPL},
url = {https://doi.org/10.1145/3571226},
doi = {10.1145/3571226},
abstract = {Programming-by-Examples (PBE) involves synthesizing an "intended program" from a small set of user-provided input-output examples. A key PBE strategy has been to restrict the search to a carefully designed small domain-specific language (DSL) with "effectively-invertible" (EI) operators at the top and "effectively-enumerable" (EE) operators at the bottom. This facilitates an effective combination of top-down synthesis strategy (which backpropagates outputs over various paths in the DSL using inverse functions) with a bottom-up synthesis strategy (which propagates inputs over various paths in the DSL). We address the problem of scaling synthesis to large DSLs with several non-EI/EE operators. This is motivated by the need to support a richer class of transformations and the need for readable code generation. We propose a novel solution strategy that relies on propagating fewer values and over fewer paths.    Our first key idea is that of "cut functions" that prune the set of values being propagated by using knowledge of the sub-DSL on the other side. Cuts can be designed to preserve completeness of synthesis; however, DSL designers may use incomplete cuts to have finer control over the kind of programs synthesized. In either case, cuts make search feasible for non-EI/EE operators and efficient for deep DSLs. Our second key idea is that of "guarded DSLs" that allow a precedence on DSL operators, which dynamically controls exploration of various paths in the DSL. This makes search efficient over grammars with large fanouts without losing recall. It also makes ranking simpler yet more effective in learning an intended program from very few examples. Both cuts and precedence provide a mechanism to the DSL designer to restrict search to a reasonable, and possibly incomplete, space of programs.    Using cuts and gDSLs, we have built FlashFill++, an industrial-strength PBE engine for performing rich string transformations, including datetime and number manipulations. The FlashFill++ gDSL is designed to enable readable code generation in different target languages including Excel's formula language, PowerFx, and Python. We show FlashFill++ is more expressive, more performant, and generates better quality code than comparable existing PBE systems. FlashFill++ is being deployed in several mass-market products ranging from spreadsheet software to notebooks and business intelligence applications, each with millions of users.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {33},
numpages = {30},
keywords = {domain-specific languages, programming by example, string transformations}
}

@inproceedings{10.1145/3652620.3686244,
author = {Wiesmayr, Bianca and Zoitl, Alois and H\"{a}stbacka, David},
title = {Modeling Service Choreographies and Collaborative Tasks for Autonomous Mixed-Fleet Systems},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3686244},
doi = {10.1145/3652620.3686244},
abstract = {Complex processes require the cooperation of a variety of subsystems, such as robots, autonomous vehicles, and human-operated devices. These so-called mixed-fleet systems are found in logistics and production use cases, which also demand a high flexibility. Hence, the choreography that orchestrates the involved systems must be adaptable and reconfigurable. Enabling to add or remove subsystems flexibly during runtime requires a strong decoupling, which is found in multi-agent systems. In this paper, we explore a model-driven engineering process for service choreographies of flexible, heterogeneous, and autonomous mixed-fleet systems. Each complex process is decomposed into services and tasks, which are flexibly assigned to resources. The resulting layered service-oriented architecture is realized as an event-based system. We define requirements for modeling services, tasks, and events and evaluate different modeling language based on their applicability for each layer, i.e., BPMN, SysML/UML, and IEC 61499. We demonstrate and evaluate our architecture using a logistics use case scenario. The results show that these languages are suitable candidates for modeling event-based process models and that the diagrams can be used to capture service choreography models for decentralized systems. Future work will investigate how these models can be validated comprehensively and used for system implementation.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {234–244},
numpages = {11},
keywords = {model-driven engineering, services, event-based architecture},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3658271.3658342,
author = {Albuquerque, Beatriz Ventorini Lins de and Cunha, Antonio Fernando Souza da and Souza, Leonardo and Siqueira, Sean Wolfgand Matsui and Santos, Rodrigo Pereira dos},
title = {Generating and Reviewing Programming Codes with Large Language Models: A Systematic Mapping Study},
year = {2024},
isbn = {9798400709968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658271.3658342},
doi = {10.1145/3658271.3658342},
abstract = {Context: The proliferation of technologies based on Large Language Models (LLM) is reshaping various domains, also impacting on programming code creation and review. Problem: The decision-making process in adopting LLM in software development demands an understanding of associated challenges and diverse application possibilities. Solution: This study addresses the identified challenges linked to LLM utilization in programming code processes. It explores models, utilization strategies, challenges, and coping mechanisms, focusing on the perspectives of researchers in software development. IS Theory: Drawing on Task-Technology Fit (TTF) theory, the research examines the alignment between task characteristics in code generation and review, and LLM technology attributes to discern performance impacts and utilization patterns. Method: Employing the Systematic Mapping of the Literature method, the research analyzes 19 selected studies from digital databases—IEEE Digital Library, Compendex Engineering Village, and Scopus—out of 1,257 retrieved results. Summary of Results: The research reveals 23 models, 13 utilization strategies, 15 challenges, and 14 coping mechanisms associated with LLM in programming code processes, offering a comprehensive understanding of the application landscape. Contributions to IS: Contributing to the Information Systems (IS) field, This study provides valuable insights into the utilization of LLM in programming code generation and review. The identified models, strategies, challenges, and coping mechanisms offer practical guidance for decision-making processes related to LLM technology adoption. The research aims to support the IS community in effectively navigating the complexities of integrating large language models into the dynamic software development lifecycle.},
booktitle = {Proceedings of the 20th Brazilian Symposium on Information Systems},
articleno = {70},
numpages = {10},
keywords = {Code Generation, LLM, automatic refactoring, code auto-suggestion, code completion, natural language models, neural network, systematic mapping study, transformer architecture},
location = {Juiz de Fora, Brazil},
series = {SBSI '24}
}

@inproceedings{10.1145/3715336.3735798,
author = {Boecking, Nils Wakan and Azari Gargari, Parastou and Reichel, Sarah and Hoffmann, Sven and Richter, Tobias and Wulf, Volker},
title = {Chat with Standards: An Assistant for the Provision of Normative Knowledge for Practical Use in Welding},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735798},
doi = {10.1145/3715336.3735798},
abstract = {Standards form an important basis for the manufacture of high-quality and safe products. As the standards landscape becomes more complex over time, the mandatory interpretation is becoming increasingly challenging and knowledge gained from experience in dealing with the standards plays a key role. This paper uses welding—as a manufacturing process that is highly regulated by standards—to demonstrate the possibilities that large language models (LLMs) offer to assist people and shows how knowledge management can be supported in applying these standards. Therefore, a chatbot prototype specialising in the specific requirements of welding standards was developed and evaluated on the methodological framework of a design case study. The results show that LLMs have the potential to improve access to complex standards beyond simple databases and document searches and facilitate compliance with these requirements. However, there are certain limitations regarding normative language and the need for referencing.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {2171–2188},
numpages = {18},
keywords = {Chatbot, Standards, Welding, Knowledge Management, User Study},
location = {
},
series = {DIS '25}
}

@inproceedings{10.1109/ICSE55347.2025.00112,
author = {Tinnes, Christof and Welter, Alisa and Apel, Sven},
title = {Software Model Evolution with Large Language Models: Experiments on Simulated, Public, and Industrial Datasets},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00112},
doi = {10.1109/ICSE55347.2025.00112},
abstract = {Modeling structure and behavior of software systems plays a crucial role in the industrial practice of software engineering. As with other software engineering artifacts, software models are subject to evolution. Supporting modelers in evolving software models with recommendations for model completions is still an open problem, though. In this paper, we explore the potential of large language models for this task. In particular, we propose an approach, RaMc, leveraging large language models, model histories, and retrieval-augmented generation for model completion. Through experiments on three datasets, including an industrial application, one public open-source community dataset, and one controlled collection of simulated model repositories, we evaluate the potential of large language models for model completion with RaMc. We found that large language models are indeed a promising technology for supporting software model evolution (62.30\% semantically correct completions on real-world industrial data and up to 86.19\% type-correct completions). The general inference capabilities of large language models are particularly useful when dealing with concepts for which there are few, noisy, or no examples at all.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {950–962},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@article{10.1145/3731209,
author = {Ma, Jiaju and Agrawala, Maneesh},
title = {MoVer: Motion Verification for Motion Graphics Animations},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3731209},
doi = {10.1145/3731209},
abstract = {While large vision-language models can generate motion graphics animations from text prompts, they regularly fail to include all spatio-temporal properties described in the prompt. We introduce MoVer, a motion verification DSL based on first-order logic that can check spatio-temporal properties of a motion graphics animation. We identify a general set of such properties that people commonly use to describe animations (e.g., the direction and timing of motions, the relative positioning of objects, etc.). We implement these properties as predicates in MoVer and provide an execution engine that can apply a MoVer program to any input SVG-based motion graphics animation. We then demonstrate how MoVer can be used in an LLM-based synthesis and verification pipeline for iteratively refining motion graphics animations. Given a text prompt, our pipeline synthesizes a motion graphics animation and a corresponding MoVer program. Executing the verification program on the animation yields a report of the predicates that failed and the report can be automatically fed back to LLM to iteratively correct the animation. To evaluate our pipeline, we build a synthetic dataset of 5600 text prompts paired with ground truth MoVer verification programs. We find that while our LLM-based pipeline is able to automatically generate a correct motion graphics animation for 58.8\% of the test prompts without any iteration, this number raises to 93.6\% with up to 50 correction iterations. Our code and dataset are at https://mover-dsl.github.io.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {33},
numpages = {17},
keywords = {verification, iterative refinement, large language models, motion graphics, visual programs, SVG}
}

@inproceedings{10.1145/3664647.3681405,
author = {Liu, Yihao and Xue, Feng and Ming, Anlong and Zhao, Mingshuai and Ma, Huadong and Sebe, Nicu},
title = {SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple Cameras and Scenes by One Model},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681405},
doi = {10.1145/3664647.3681405},
abstract = {In the last year, universal monocular metric depth estimation (universal MMDE) has gained considerable attention, serving as the foundation model for various multimedia tasks, such as video and image editing. Nonetheless, current approaches face challenges in maintaining consistent accuracy across diverse scenes without scene-specific parameters and pre-training, hindering the practicality of MMDE. Furthermore, these methods rely on extensive datasets comprising millions, if not tens of millions, of data for training, leading to significant time and hardware expenses. This paper presents SM4Depth, a model that seamlessly works for both indoor and outdoor scenes, without needing extensive training data and GPU clusters. Firstly, to obtain consistent depth across diverse scenes, we propose a novel metric scale modeling, i.e., variation- based unnormalized depth bins. It reduces the ambiguity of the conventional metric bins and enables better adaptation to large depth gaps of scenes during training. Secondly, we propose a ''divide and conquer'' solution to reduce reliance on massive training data. Instead of estimating directly from the vast solution space, the metric bins are estimated from multiple solution sub-spaces to reduce complexity. Additionally, we introduce an uncut depth dataset, BUPT Depth, to evaluate the depth accuracy and consistency across various indoor and outdoor scenes. Trained on a consumer-grade GPU using just 150K RGB-D pairs, SM4Depth achieves outstanding performance on the most never-before-seen datasets, especially maintaining consistent accuracy across indoors and outdoors. The code can be found here.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {3469–3478},
numpages = {10},
keywords = {domain-aware bin estimation, seamless monocular metric depth estimation},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1145/3712311.3712323,
author = {Bonifati, Angela and Ozsu, M. Tamer and Tian, Yuanyuan and Voigt, Hannes and Yu, Wenyuan and Zhang, enjie},
title = {A Roadmap to Graph Analytics},
year = {2025},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/3712311.3712323},
doi = {10.1145/3712311.3712323},
abstract = {Graphs are ubiquitous data structures used in a large spectrum of applications, spanning from transportation networks, financial networks, social networks, product-order transactions and biomedical applications [33]. A recent survey on the usage of graph applications from real users has highlighted the fact that analytics is the most time-consuming task as opposed to testing, cleaning and ETL [32].},
journal = {SIGMOD Rec.},
month = jan,
pages = {43–51},
numpages = {9}
}

@inproceedings{10.1109/ICSE-Companion66252.2025.00012,
author = {Karre, Sai Anirudh and Halhalli, Amogha A and Reddy, Y. Raghu},
title = {VReqST: A Requirement Specification Tool for Virtual Reality Software Products},
year = {2025},
isbn = {9798331536831},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion66252.2025.00012},
doi = {10.1109/ICSE-Companion66252.2025.00012},
abstract = {Developing Virtual Reality (VR) software products with discrete and incremental requirements is a challenging task for VR practitioners. A domain expert's assistance plays a key role in VR product completeness, as most VR requirements are abstract or under-specified during the early stages. Slight changes to the requirements can significantly impact the overall flow of the VR software development process. In this paper, we introduce VReqST, a tool for VR requirement analysts to specify requirements for building effective VR software products. It is developed based on a Role-based model template for specifying virtual environments, custom behaviors, VR-specific algorithms, user-flows, action responses, timeline of events, etc. The tool has been developed after several interactions with VR practitioners from industry \&amp; academia. We share our insights on the effectiveness of this tool in practice.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings},
pages = {9–12},
numpages = {4},
keywords = {software requirement specification, requirement elicitation, virtual reality, industrial practices},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@article{10.1145/3715750,
author = {Gao, Yi and Hu, Xing and Yang, Xiaohu and Xia, Xin},
title = {Automated Unit Test Refactoring},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715750},
doi = {10.1145/3715750},
abstract = {Test smells arise from poor design practices and insufficient domain knowledge, which can lower the quality of test code and make it harder to maintain and update. Manually refactoring of test smells is time-consuming and error-prone, highlighting the necessity for automated approaches. Current rule-based refactoring methods often struggle in scenarios not covered by predefined rules and lack the flexibility needed to handle diverse cases effectively. In this paper, we propose a novel approach called UTRefactor, a context-enhanced, LLM-based framework for automatic test refactoring in Java projects. UTRefactor extracts relevant context from test code and leverages an external knowledge base that includes test smell definitions, descriptions, and DSL-based refactoring rules. By simulating the manual refactoring process through a chain-of-thought approach, UTRefactor guides the LLM to eliminate test smells in a step-by-step process, ensuring both accuracy and consistency throughout the refactoring. Additionally, we implement a checkpoint mechanism to facilitate comprehensive refactoring, particularly when multiple smells are present. We evaluate UTRefactor on 879 tests from six open-source Java projects, reducing the number of test smells from 2,375 to 265, achieving an 89\% reduction. UTRefactor outperforms direct LLM-based refactoring methods by 61.82\% in smell elimination and significantly surpasses the performance of a rule-based test smell refactoring tool. Our results demonstrate the effectiveness of UTRefactor in enhancing test code quality while minimizing manual involvement.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE033},
numpages = {21},
keywords = {Large Language Models, Test Refactoring, Test Smells}
}

@inproceedings{10.1145/3640310.3674085,
author = {Ben Chaaben, Meriem and Ben Sghaier, Oussama and Dhaouadi, Mouna and Elrasheed, Nafisa and Darif, Ikram and Jaoua, Imen and Oakes, Bentley and Syriani, Eugene and Hamdaqa, Mohammad},
title = {Toward Intelligent Generation of Tailored Graphical Concrete Syntax},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674085},
doi = {10.1145/3640310.3674085},
abstract = {In model-driven engineering, the concrete syntax of a domain-specific modeling language (DSML) is fundamental as it constitutes the primary point of interaction between the user and the DSML. Nevertheless, the conventional one-size-fits-all approach to concrete syntax often undermines the effectiveness of DSMLs, as it fails to accommodate the diverse constraints and specific requirements inherent to diverse users and usage contexts. Such shortcomings can lead to a significant decline in the performance, usability, and efficiency of DSMLs. This vision paper proposes a conceptual framework to generate concrete syntax intelligently. Our framework considers multiple concerns of users and aims to align the concrete syntax with the context of the DSML usage. Additionally, we detail a baseline process to employ our framework in practice, leveraging large language models to expedite the generation of tailored concrete syntax. We illustrate the potential of our vision with two concrete examples and discuss the shortcomings and research challenges of current intelligent generation techniques.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {160–171},
numpages = {12},
keywords = {Artificial Intelligence, Concrete Syntax, Domain-specific Modeling Languages, Large Language Models},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3652620.3687807,
author = {Yang, Yujing and Chen, Boqi and Chen, Kua and Mussbacher, Gunter and Varr\'{o}, D\'{a}niel},
title = {Multi-step Iterative Automated Domain Modeling with  Large Language Models},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687807},
doi = {10.1145/3652620.3687807},
abstract = {Domain modeling, which represents the concepts and relationships in a problem domain, is an essential part of software engineering. As large language models (LLMs) have recently exhibited remarkable ability in language understanding and generation, many approaches are designed to automate domain modeling with LLMs. However, these approaches usually formulate all input information to the LLM in a single step. Our previous single-step approach resulted in many missing modeling elements and advanced patterns. This paper introduces a novel framework designed to enhance fully automated domain model generation. The proposed multi-step automated domain modeling approach extracts model elements (e.g., classes, attributes, and relationships) from problem descriptions. The approach includes instructions and human knowledge in each step and uses an iterative process to identify complex patterns, repeatedly extracting the pattern from various instances and then synthesizing these extractions into a summarized overview. Furthermore, the framework incorporates a self-reflection mechanism. This mechanism assesses each generated model element, offering self-feedback for necessary modifications or removals, and integrates the domain model with the generated self-feedback. The proposed approach is assessed in experiments, comparing it with a baseline single-step approach from our earlier work. Experiments demonstrate a significant improvement over our earlier work, with a 22.71\% increase in the F1-score for identifying classes, 75.18\% for relationships, and a 10.39\% improvement for identifying the player-role pattern, with comparable performance for attributes. Our approach, dataset, and evaluation provide valuable insight for future research in automated LLM-based domain modeling.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {587–595},
numpages = {9},
keywords = {domain modeling, large language models, few-shot learning, prompt engineering},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3691620.3695277,
author = {Sahoo, Priyam and Pujar, Saurabh and Nalawade, Ganesh and Genhardt, Richard and Mandel, Louis and Buratti, Luca},
title = {Ansible Lightspeed: A Code Generation Service for IT Automation},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695277},
doi = {10.1145/3691620.3695277},
abstract = {The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for Information Technology (IT) automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Ansible Lightspeed is an LLM-based service designed explicitly to generate Ansible YAML, given natural language prompt.In this paper, we present the design and implementation of the Ansible Lightspeed service. We then evaluate its utility to developers using diverse indicators, including extended utilization, analysis of user edited suggestions, as well as user sentiments analysis. The evaluation is based on data collected for 10,696 real users including 3,910 returning users. The code for Ansible Lightspeed service and the analysis framework is made available for others to use.To our knowledge, our study is the first to involve thousands of users of code assistants for domain-specific languages. We are also the first code completion tool to present N-Day user retention figures, which is 13.66\% on Day 30. We propose an improved version of user acceptance rate, called Strong Acceptance rate, where a suggestion is considered accepted only if less than 50\% of it is edited and these edits do not change critical parts of the suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong acceptance rate of 49.08\% for multi-line Ansible task suggestions. With our findings we provide insights into the effectiveness of small, dedicated models in a domain-specific context. We hope this work serves as a reference for software engineering and machine learning researchers exploring code completion.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2148–2158},
numpages = {11},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3757062,
author = {Yang, Fu-Chia and Guo, Siqi and Mousas, Christos},
title = {Exploring Familiarity and Knowledgeability in Conversational Virtual Agents},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1544-3558},
url = {https://doi.org/10.1145/3757062},
doi = {10.1145/3757062},
abstract = {In this study, we examined the impact of agent familiarity and knowledgeability on several variables spanning agent perceptions (i.e., perceived knowledge, familiarity, trust, anthropomorphism, uncanny valley effect, and likability), social and emotional experiences (i.e., co-presence, rapport, cognitive process expectations, and willingness for future interaction), and conversation dynamics (i.e., conversation transcript, participants’ response word count, and response time). We created two virtual agents for the study: a digital replica of a professor from our department (i.e., familiar agent) and an agent with similar demographic variables (i.e., age, gender, and ethnicity) but with a fabricated appearance and voice (i.e., unfamiliar agent). We implemented both agents to exhibit two levels of knowledgeability (i.e., low and high) in the domain of game development and course-specific information. We used large language models (LLMs) to provide the agents with persona information and domain knowledge through prompt engineering. For our user study, we followed a 2 (familiarity: unfamiliar vs. familiar agent)  (times)  2 (knowledgeability: low vs. high knowledgeability) within-group study design and recruited 32 participants who engaged in a five-minute, conversation-based virtual reality (VR) interaction with all four experimental conditions: unfamiliar agent with low knowledgeability (ULK), unfamiliar agent with high knowledgeability (UHK), familiar agent with low knowledgeability (FLK), and familiar agent with high knowledgeability (FHK). The findings demonstrated a significant main effect of agent familiarity on perceived knowledge, suggesting that familiarity plays a crucial role in shaping users’ perception of the agent's knowledgeability level. Besides perceived knowledge, familiarity also affected all other variables, apart from co-presence. Conversely, agent knowledgeability affected perceived familiarity, trust, anthropomorphism, cognitive process expectations, willingness for future interaction, conversation content, and participants’ response word count. Finally, we found an interaction effect between agent familiarity and perceived knowledge, indicating that familiarity has a significant influence on users’ perceptions of the agent's knowledgeability. This study contributes to the field of conversational human-agent interaction in VR by providing empirical evidence on how adapting both familiarity and knowledgeability of virtual agents can significantly enhance user experience, offering valuable insights into designing more engaging, trustworthy, and effective embodied conversational agents.},
note = {Just Accepted},
journal = {ACM Trans. Appl. Percept.},
month = jul,
keywords = {Virtual Reality, Familiarity, Knowledgeability, Embodied Conversational Agents, Large Language Models}
}

@article{10.1145/3580480,
author = {Du, Kelvin and Xing, Frank and Cambria, Erik},
title = {Incorporating Multiple Knowledge Sources for Targeted Aspect-based Financial Sentiment Analysis},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3580480},
doi = {10.1145/3580480},
abstract = {Combining symbolic and subsymbolic methods has become a promising strategy as research tasks in AI grow increasingly complicated and require higher levels of understanding. Targeted Aspect-based Financial Sentiment Analysis (TABFSA) is an example of such complicated tasks, as it involves processes like information extraction, information specification, and domain adaptation. However, little is known about the design principles of such hybrid models leveraging external lexical knowledge. To fill this gap, we define anterior, parallel, and posterior knowledge integration and propose incorporating multiple lexical knowledge sources strategically into the fine-tuning process of pre-trained transformer models for TABFSA. Experiments on the Financial Opinion mining and Question Answering challenge (FiQA) Task 1 and SemEval 2017 Task 5 datasets show that the knowledge-enabled models systematically improve upon their plain deep learning counterparts, and some outperform state-of-the-art results reported in terms of aspect sentiment analysis error. We discover that parallel knowledge integration is the most effective and domain-specific lexical knowledge is more important according to our ablation analysis.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {23},
numpages = {24},
keywords = {Financial sentiment analysis, neural networks, knowledge enabled system, deep learning, transformer models}
}

@inproceedings{10.1145/3652620.3688224,
author = {Khalilipour, Alireza and Challenger, Moharram},
title = {Towards Intelligent Model Management: An Exploratory Study and Road-mapping},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688224},
doi = {10.1145/3652620.3688224},
abstract = {With data science entering various domains, new branches are emerging due to the extraction of latent knowledge from each domain's data. Model-based engineering and modeling are no exceptions. Now is the time to open a new chapter in this field by leveraging advanced artificial intelligence techniques. As the number and complexity of models increase, NP-complete problems arise that cannot be effectively addressed through deterministic management solutions. An effective way to address these challenges is by applying non-deterministic intelligent methodologies and data science-derived solutions. The increasing number of models and the formation of large model repositories necessitate intelligent model management, which aims to recognize hidden patterns and knowledge within these repositories using data science, machine learning techniques, and statistical and probabilistic methods for reuse. Despite the progress made in this area, both theoretically and practically, intelligent model management has not yet secured a prominent place in the body of knowledge of model-driven engineering. In this paper, we aim to clarify the exact position of intelligent model management by providing precise definitions, distinguishing it from conventional management, and identifying associated challenges. The above objective outlines the research approach in this area, making it easy for researchers to comprehend the procedures they employ for conducting their investigations.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1015–1024},
numpages = {10},
keywords = {intelligent model management, model repositories, machine learning, model-driven engineering, model reuse},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3723498.3723726,
author = {van Rozen, Riemer},
title = {Live Game Design: Prototyping at the Speed of Play},
year = {2025},
isbn = {9798400718564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723498.3723726},
doi = {10.1145/3723498.3723726},
abstract = {Automated Game Design empowers game designers with languages, techniques and tools that automate iterative design processes. However, these tools currently lack suitable input and feedback mechanisms for creating rules and perceiving how changes affect running game prototypes. As a result, iterating takes too long, forming mental models about cause-and-effect relationships is difficult, and learning how to program can be tedious and frustrating. We investigate how Live Programming can accelerate game design iterations, make visual tools more accessible and engaging, and provide immediate feedback that brings code to life. We propose Live Game Design, a novel approach for rapid game prototyping that introduces mini-cycles to help designers of all skill levels explore, learn, and see a prototype come alive. We introduce Vie (pronounced /vi/), a game-making game for simultaneously prototyping and playtesting simple 2D games using Machinations. In an observational study, we evaluate the app during a Game-Based Learning tutorial for children aged 8 to 14. Our results show Vie is accessible to novices and Live Game Design enables prototyping at the speed of play.},
booktitle = {Proceedings of the 20th International Conference on the Foundations of Digital Games},
articleno = {26},
numpages = {12},
keywords = {automated game design, live programming, game mechanics, mixed-initiative design, prototyping, playtesting, game-based learning},
location = {
},
series = {FDG '25}
}

@inproceedings{10.1145/3659677.3659742,
author = {Babaalla, Zakaria and Jakimi, Abdeslam and Oualla, Mohamed and Saadane, Rachid and Chehri, Abdellah},
title = {Towards an Automatic Extracting UML Class Diagram from System's Textual Specification},
year = {2024},
isbn = {9798400709296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659677.3659742},
doi = {10.1145/3659677.3659742},
abstract = {Developing a software system from natural language requirements is a complex and delicate task that requires a high level of design and programming expertise. Increasing the level of abstraction used to describe these requirements is the most natural solution. Model-Driven Engineering (MDE) also takes this route, using abstract models as primary entities to generate source code automatically or semi-automatically. Among these models, the UML class diagram occupies a privileged place in object-oriented systems because it not only serves as a basis for communication between developers but also provides a closely aligned static representation of the system implementation. However, creating a UML class diagram from a textual system specification poses a significant challenge due to the inherent imprecision and ambiguity commonly found in natural language expressions. In this paper, we propose a model-centric approach based on deep learning for the automatic extraction of UML class diagrams from textual requirements.},
booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
articleno = {36},
numpages = {5},
keywords = {Class diagram, Deep learning, Machine learning, Natural Language Processing, UML},
location = {Meknes, AA, Morocco},
series = {NISS '24}
}

@article{10.1145/3632860,
author = {Patton, Noah and Rahmani, Kia and Missula, Meghana and Biswas, Joydeep and Dillig, I\c{s}\i{}l},
title = {Programming-by-Demonstration for Long-Horizon Robot Tasks},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632860},
doi = {10.1145/3632860},
abstract = {The goal of programmatic Learning from Demonstration (LfD) is to learn a policy in a programming language that can be used to control a robot’s behavior from a set of user demonstrations. This paper presents a new programmatic LfD algorithm that targets long-horizon robot tasks which require synthesizing programs with complex control flow structures, including nested loops with multiple conditionals. Our proposed method first learns a program sketch that captures the target program’s control flow and then completes this sketch using an LLM-guided search procedure that incorporates a novel technique for proving unrealizability of programming-by-demonstration problems. We have implemented our approach in a new tool called prolex and present the results of a comprehensive experimental evaluation on 120 benchmarks involving complex tasks and environments. We show that, given a 120 second time limit, prolex can find a program consistent with the demonstrations in 80\% of the cases. Furthermore, for 81\% of the tasks for which a solution is returned, prolex is able to find the ground truth program with just one demonstration. In comparison, CVC5, a syntaxguided synthesis tool, is only able to solve 25\% of the cases even when given the ground truth program sketch, and an LLM-based approach, GPT-Synth, is unable to solve any of the tasks due to the environment complexity.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {18},
numpages = {34},
keywords = {Abstract Interpretation, Program Synthesis, Learning from Demonstrations}
}

@inproceedings{10.1145/3718958.3750503,
author = {Yan, Zihan and Li, Dan and Chen, Li and Xiong, Dian and Gao, Kaihui and Zhang, Yiwei and Yan, Rui and Zhang, Menglei and Zhang, Bochun and Jiang, Zhuo and Ye, Jianxi and Lin, Haibin},
title = {From ATOP to ZCube: Automated Topology Optimization Pipeline and A Highly Cost-Effective Network Topology for Large Model Training},
year = {2025},
isbn = {9798400715242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3718958.3750503},
doi = {10.1145/3718958.3750503},
abstract = {The development of large language models (LLMs) poses new challenges in data center network topology design. To assist in exploring topology design, we propose ATOP, an Automated Topology Optimization Pipeline, which models network topology as a set of hyperparameters, enabling the discovery of potential topologies. With various optimization algorithms and customizable optimization objectives, ATOP achieves automated topology optimization on a scale of tens of thousands of GPUs. We apply ATOP on network topologies for 256, 1024, 4096, and 16384 GPUs, optimizing performance under LLMs training traffic patterns, collective communication performance, fault tolerance, and network cost. We also evaluate ATOP in different scenarios: building, optimizing, and expanding a data center. From ATOP's results, we discover a new topology — ZCube, which reaches the highest cost-effectiveness across various GPU scales. Simulation results show that ZCube, compared to the previous state-of-the-art topologies, including Rail-optimized Fat-tree (ROFT), Rail-only, and HPN, improves end-to-end LLM training speed by 3\% to 7\% and reduces network hardware costs by 26\% to 46\%. We also construct ZCube on a real-world testbed. Results show that ZCube reduces hardware costs by 25\% compared to Rail-Optimized Topology while maintaining the same all-reduce and all-to-all performance.},
booktitle = {Proceedings of the ACM SIGCOMM 2025 Conference},
pages = {861–881},
numpages = {21},
keywords = {data center networks, network topology, AI infrastructure},
location = {S\~{a}o Francisco Convent, Coimbra, Portugal},
series = {SIGCOMM '25}
}

@inbook{10.1145/3676536.3689923,
author = {Bohm Agostini, Nicolas and Gozzi, Giovanni and Fiorito, Michele and Barone, Claudio and Curzel, Serena and Limaye, Ankur and Minutoli, Marco and Castellana, Vito Giovanni and Manzano, Joseph and Ferrandi, Fabrizio and Tumeo, Antonino},
title = {Extending High-Level Synthesis with AI/ML Methods},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3689923},
abstract = {Artificial Intelligence (AI) and Machine Learning (ML) methods offer significant opportunities to improve the quality of results in high-level synthesis (HLS). For instance, they can be used to model and predict metrics of the final design (e.g., area, considering aspects such as interconnect overhead for different device technologies), thereby facilitating exploration when searching for the best design trade-offs. Additionally, they can help identify hidden correlations across various phases of synthesis and the optimizations performed, enabling the identification of the most effective pipelines. Furthermore, these methods can greatly facilitate and enhance the design space exploration for the synthesis process in terms of both time and quality of results. This paper discusses the opportunities and challenges of augmenting HLS with AI/ML, using as an example the SODA Synthesizer, an open-source hardware generation toolchain that includes SODA-OPT, a hardware/software partitioning and pre-optimization tool developed with the MLIR framework, and PandA-Bambu, a state-of-the-art HLS tool. SODA interfaces with OpenROAD to provide a complete end-to-end toolchain.},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {28},
numpages = {6}
}

@inproceedings{10.1145/3706598.3713613,
author = {Li, Brenna and Tauseef, Saba and Truong, Khai N. and Mariakakis, Alex},
title = {A Comparative Analysis of Information Gathering by Chatbots, Questionnaires, and Humans in Clinical Pre-Consultation},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713613},
doi = {10.1145/3706598.3713613},
abstract = {Information gathering is an important capability that allows chatbots to understand and respond to users’ needs, yet the effectiveness of LLM-powered chatbots at this task remains underexplored. Our work investigates this question in the context of clinical pre-consultation, wherein patients provide information to an intermediary before meeting with a physician to facilitate communication and reduce consultation inefficiencies. We conducted a study at a walk-in clinic with 45 patients who interacted with one of three conversational agents: a chatbot, a questionnaire, and a Wizard-of-Oz. We analyzed patients’ messages using metrics adapted from Grice’s maxims to assess the quality of information gathered at each conversation turn. We found that the Wizard and LLM were more successful than the questionnaire because they modified questions and asked follow-ups when participants provided unsatisfactory answers. However, the LLM did not ask nearly as many follow-up questions as the Wizard, particularly when participants provided unclear answers.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {639},
numpages = {17},
keywords = {Pre-consultation, chatbot, LLM, primary care, walk-in clinic},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3632971.3632981,
author = {Ji, Xiaoliu and Cao, Yang and Ni, Xuanfan and Wang, Xi and Mao, Xueying and Li, Piji and Guo, Chunjie},
title = {Neural Machine Translation for Chinese Patent Medicine Instructions},
year = {2024},
isbn = {9798400707704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632971.3632981},
doi = {10.1145/3632971.3632981},
abstract = {Abstract. Insufficient research has been conducted on the validity of datasets pertaining to the translation of Chinese Patent Medicine Instructions into English. Upon analyzing the Chinese and English texts generated by prominent translation engines, we observe that the readability of translation is a sore point and the English translation standards lack consistency. There exists a restricted range of internet search platforms that are specifically designed for the purpose of Chinese Patent Medicine (CPM). The focus of these platforms centers on the domain of specialized terminology related to Chinese herbal medicine. To address these problems, we initially develop a Chinese Patent Medicine Instruction Dataset (CPMID) for Chinese-English translation. This dataset comprises 11,695 Chinese-English entries to be meticulously annotated and validated. We benchmark the task by training and testing multiple baselines including traditional models Seq2Seq+Attention (LSTM) and Transformer, pre-trained and released translation models SMaLL-100, NLLB-200, mBART-50, and ChatGPT. The dataset demonstrates the accuracy and effectiveness with improvement of 42.5 BLEU, surpassing prior state-of-the-art by over 54.7\%. The primary objective of utilizing this dataset in future R&amp;D is to provide a reliable retrieval system for foreign users of Chinese Patent Medicine (CPM). We believe that the implementation of CPMID has the potential to facilitate the modernization of Traditional Chinese Medicine (TCM) and significantly contribute to the field of Modern Medicine (MM).},
booktitle = {Proceedings of the 2023 International Joint Conference on Robotics and Artificial Intelligence},
pages = {155–160},
numpages = {6},
keywords = {Chinese Patent Medicine Instructions, Large Language Models, Neural Machine Translation, Transformer},
location = {Shanghai, China},
series = {JCRAI '23}
}

@inproceedings{10.1145/3706598.3714327,
author = {Yang, Jackie (Junrui) and Shi, Yingtian and Gu, Chris and Zheng, Zhang and Jain, Anisha and Li, Tianshi and Lam, Monica S. and Landay, James A.},
title = {GenieWizard: Multimodal App Feature Discovery with Large Language Models},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714327},
doi = {10.1145/3706598.3714327},
abstract = {Multimodal interactions are more flexible, efficient, and adaptable than graphical interactions, allowing users to execute commands beyond simply tapping GUI buttons. However, the flexibility of multimodal commands makes it hard for designers to prototype and provide design specifications for developers. It is also hard for developers to anticipate what actions users may want. We present GenieWizard, a tool to aid developers in discovering potential features to implement in multimodal interfaces. GenieWizard supports user-desired command discovery early in the implementation process, streamlining the development process. GenieWizard uses an LLM to generate potential user interactions and parse these interactions into a form that can be used to discover the missing features for developers. Our evaluations showed that GenieWizard can reliably simulate user interactions and identify missing features. Also, in a study (N = 12), we demonstrated that developers using GenieWizard can identify and implement 42\% of the missing features of multimodal apps compared to only 10\% without GenieWizard.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {936},
numpages = {17},
keywords = {Multimodal interfaces, developer tools, large language models, feature discovery, interaction simulation, voice interfaces, touch interfaces, semantic parsing, multimodal app development},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3637528.3671622,
author = {Fischer, Sophie and Gemmell, Carlos and Tecklenburg, Niklas and Mackie, Iain and Rossetto, Federico and Dalton, Jeffrey},
title = {GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671622},
doi = {10.1145/3637528.3671622},
abstract = {We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84\% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge. These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4951–4961},
numpages = {11},
keywords = {conversational task assistants, large language models},
location = {Barcelona, Spain},
series = {KDD '24}
}

@article{10.1145/3696379,
author = {Bui, Minh-Thanh and Boffa, Matteo and Valentim, Rodolfo Vieira and Navarro, Jose Manuel and Chen, Fuxing and Bao, Xiaosheng and Houidi, Zied Ben and Rossi, Dario},
title = {A Systematic Comparison of Large Language Models Performance for Intrusion Detection},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CoNEXT4},
url = {https://doi.org/10.1145/3696379},
doi = {10.1145/3696379},
abstract = {We explore the capabilities of Large Language Models (LLMs) to assist or substitute devices (i.e., firewalls) and humans (i.e., security experts) respectively in the detection and analysis of security incidents. We leverage transformer-based technologies, from relatively small to foundational sizes, to address the problem of correctly identifying the attack severity (and accessorily identifying and explaining the attack type). We contrast a broad range of LLM techniques (prompting, retrieval augmented generation, and fine-tuning of several models) using state-of-the-art machine learning models as a baseline. Using proprietary data from commercial deployment, our study provides an unbiased picture of the strengths and weaknesses of LLM for intrusion detection.},
journal = {Proc. ACM Netw.},
month = nov,
articleno = {22},
numpages = {23},
keywords = {computing methodologies, firewalls, intrusion detection systems, machine learning, natural language processing, security and privacy}
}

@inproceedings{10.1145/3652620.3688206,
author = {Rabbi, Fazle},
title = {A Model-Based Framework for Exploring Conflict Dynamics},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688206},
doi = {10.1145/3652620.3688206},
abstract = {This paper introduces a novel framework for conflict analysis that leverages advanced visual modeling techniques. By employing comparative analysis, key variables influencing armed conflicts are identified and analyzed. The framework includes a meta-model representing domain concepts such as the goals and strategies of conflicting parties, escalating stages, and impacts of conflicts.Conflict escalation is a complex process characterized by interactions between opposing parties. This paper presents a structured model that outlines how conflicts evolve and intensify over time. We adapt a meta-modeling framework called the Diagram Predicate Framework (DPF) to represent conflict-related concepts and extend it to support abstract view generation. This framework facilitates the analysis of conflict trends and the study of dynamics across various levels of abstraction.A computational model based on category theory is proposed for trend analysis, enabling the extraction of patterns of conflict evolution and the comparison of strategies and goals at different escalation stages. Categorical operations such as pullback and limit construction are employed to compute conflict evolution and identify common structures among conflict instances, providing insights into conflict dynamics across diverse zones.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {745–754},
numpages = {10},
keywords = {conflict analysis, computational journalism, category theory, metamodeling},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3677052.3698651,
author = {Chen, Zekai and Chen, Po-Yu and Buet-Golfouse, Francois},
title = {Online Personalizing White-box LLMs Generation with Neural Bandits},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698651},
doi = {10.1145/3677052.3698651},
abstract = {Personalized content generation by Large Language Models (LLMs) in finance presents a challenge: efficiently adapting text to individual preferences without creating unique models for each user. This study introduces an innovative online method for financial applications, employing neural bandit algorithms to dynamically optimize soft instruction embeddings based on user feedback, enhancing personalization in white-box LLMs. Through experiments on public generation tasks, we demonstrate significant performance improvements. Notably, our NeuralTS implementation achieves up to a 62.9\% improvement in ROUGE scores and a 2.76\% increase in LLM-agent evaluation for personalized content generation. This research showcases the efficacy of neural bandits in refining LLM outputs to align with client-specific needs and regulatory requirements, marking a pivotal step towards feasible and effective adaptive text generation in finance. Our method offers a promising and scalable solution for financial institutions to enhance client engagement, improve risk assessment, and streamline regulatory reporting.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {711–718},
numpages = {8},
keywords = {Large language models, multi-armed bandits, personalization},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@inproceedings{10.1145/3747912.3747966,
author = {Zhu, Linkai and Zhu, Xiaolian and Liu, Lu and Yin, Bin and Li, Xiaojuan},
title = {From Regulation to Execution: Generating GDPR-Compliant Smart Contracts for Cross-Border Data Transfer},
year = {2025},
isbn = {9798400715136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3747912.3747966},
doi = {10.1145/3747912.3747966},
abstract = {Privacy policy about sharing data in different countries make it hard to create smart contracts on blockchain that follow the rules. This research suggests a new way to automatically turn legal rules into smart contracts. The method uses a special language for law to change regulations into Solidity code. We make a tool that changes legal rules into code with conditions, time limits, and permissions. We find a mechanism to check the logic of the rules. And integrate a system that puts everything together into a full smart contract with checks and alerts.  This system will check if the contract works right and follows the privacy policy in GDPR.},
booktitle = {Proceedings of the 2025 International Conference on Software Engineering and Computer Applications},
pages = {49–52},
numpages = {4},
keywords = {Cross-border Data Transfer, Privacy Regulation Compliance, Smart Legal Contracts, Solidity Code Generation},
location = {
},
series = {SECA '25}
}

@article{10.1145/3709616.3709617,
author = {Neumann, Peter G.},
title = {ACM Risks Forum Quarterly Summary},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3709616.3709617},
doi = {10.1145/3709616.3709617},
abstract = {This is an annotated summary of the main items contributed to the ACM Risks Forum in the most recent quarter, orgnanized categorically. References (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Marshall at Newcastle: http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: https://www.csl.sri.com/users/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {4–7},
numpages = {4}
}

@inproceedings{10.1145/3640310.3674089,
author = {Costa, Carlos Dur\'{a} and L\'{o}pez, Jos\'{e} Antonio Hern\'{a}ndez and Cuadrado, Jes\'{u}s S\'{a}nchez},
title = {ModelMate: A recommender for textual modeling languages based on pre-trained language models},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674089},
doi = {10.1145/3640310.3674089},
abstract = {Current DSL environments lack smart editing facilities intended to enhance modeler productivity and cannot keep pace of current developments of integrated development environments based on AI. In this paper, we propose an approach to address this shortcoming through a recommender system specifically tailored for textual DSLs based on the fine-tuning of pre-trained language models. We identify three main tasks: identifier suggestion, line completion, and block completion, which we implement over the same fine-tuned model and we propose a workflow to apply these tasks to any textual DSL. We have evaluated our approach with different pre-trained models for three DSLs: Emfatic, Xtext and a DSL to specify domain entities, showing that the system performs well and provides accurate suggestions. We compare it against existing approaches in the feature name recommendation task showing that our system outperforms the alternatives. Moreover, we evaluate the inference time of our approach obtaining low latencies, which makes the system adequate for live assistance. Finally, we contribute a concrete recommender, named ModelMate, which implements the training, evaluation and inference steps of the workflow as well as providing integration into Eclipse-based textual editors.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {183–194},
numpages = {12},
keywords = {Machine learning, Meta-modeling, Model-Driven Engineering, Recommendation},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.5555/3712729.3712978,
author = {Barat, Souvik and Yadav, Abhishek and Thogaru, Himabindu and Kulkarni, Vinay and Bhattacharya, Kaustav},
title = {Imparting Adaptiveness and Resilience to Parcel Delivery Networks: A Digital Twin Centric Simulation Based Approach},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {Contemporary parcel delivery companies face a significant surge in demand, along with increased customer expectations for flawless and timely delivery. They must meet these expectations within a shrinking window of opportunity in an increasingly competitive world while dealing with various micro and macro level uncertainties. Current industry practice relying on localized analysis to meet these expectations has turned out ineffective. This paper argues that imparting adaptiveness and resilience to parcel delivery network is the key to a pragmatic solution. It presents a holistic approach based on simulatable digital twins and composable agents to enable "in silico" business experimentation wherein a set of what-if scenarios are simulated to help evaluate efficacy of current strategy and identify suitable modifications to the strategy if necessary. The paper illustrates the proposed approach on a case study from the parcel industry and demonstrates its utility and efficacy on a set of real-life scenarios.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2999–3010},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@article{10.1145/3762990,
author = {Meijer, Erik},
title = {Guardians of the Agents: Formal verification of AI workflows},
year = {2025},
issue_date = {July/August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {1542-7730},
url = {https://doi.org/10.1145/3762990},
doi = {10.1145/3762990},
abstract = {To mitigate against models going off the rails during inference, people often use so-called guardrails to dynamically monitor, filter, and control model responses for problematic content. Guardrails, however, come with their own set of problems such as false positives caused by pattern matching against a fixed set of forbidden words. This mathematical proof-based approach addresses these limitations by providing deterministic and verifiable assurances of safety without the need to trust the AI nor any of the artifacts it produces.},
journal = {Queue},
month = sep,
pages = {66–85},
numpages = {20}
}

@article{10.1145/3712002,
author = {Murillo, Juan Manuel and Garcia-Alonso, Jose and Moguel, Enrique and Barzen, Johanna and Leymann, Frank and Ali, Shaukat and Yue, Tao and Arcaini, Paolo and P\'{e}rez-Castillo, Ricardo and Garc\'{\i}a-Rodr\'{\i}guez de Guzm\'{a}n, Ignacio and Piattini, Mario and Ruiz-Cort\'{e}s, Antonio and Brogi, Antonio and Zhao, Jianjun and Miranskyy, Andriy and Wimmer, Manuel},
title = {Quantum Software Engineering: Roadmap and Challenges Ahead},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712002},
doi = {10.1145/3712002},
abstract = {As quantum computers advance, the complexity of the software they can execute increases as well. To ensure this software is efficient, maintainable, reusable, and cost-effective—key qualities of any industry-grade software—mature software engineering practices must be applied throughout its design, development, and operation. However, the significant differences between classical and quantum software make it challenging to directly apply classical software engineering methods to quantum systems. This challenge has led to the emergence of Quantum Software Engineering (QSE) as a distinct field within the broader software engineering landscape. In this work, a group of active researchers analyze in depth the current state of QSE research. From this analysis, the key areas of QSE are identified and explored in order to determine the most relevant open challenges that should be addressed in the next years. These challenges help identify necessary breakthroughs and future research directions for advancing QSE.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {154},
numpages = {48},
keywords = {Quantum Software Engineering, open challenges, Quantum Computing, QSE}
}

@article{10.1145/3702987,
author = {Zhang, Sai and Xing, Zhenchang and Guo, Ronghui and Xu, Fangzhou and Chen, Lei and Zhang, Zhaoyuan and Zhang, Xiaowang and Feng, Zhiyong and Zhuang, Zhiqiang},
title = {Empowering Agile-Based Generative Software Development through Human-AI Teamwork},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3702987},
doi = {10.1145/3702987},
abstract = {In software development, the raw requirements proposed by users are frequently incomplete, which impedes the complete implementation of software functionalities. With the emergence of large language models, the exploration of generating software through user requirements has attracted attention. Recent methods with the top-down waterfall model employ a questioning approach for requirement completion, attempting to explore further user requirements. However, users, constrained by their domain knowledge, result in a lack of effective acceptance criteria during the requirement completion, failing to fully capture the implicit needs of the user. Moreover, the cumulative errors of the waterfall model can lead to discrepancies between the generated code and user requirements. The Agile methodologies reduce cumulative errors of the waterfall model through lightweight iteration and collaboration with users, but the challenge lies in ensuring semantic consistency between user requirements and the code generated by the agent. To address these challenges, we propose AgileGen, an agile-based generative software development through human-AI teamwork. Unlike existing questioning agents, AgileGen adopts a novel collaborative approach that breaks free from the constraints of domain knowledge by initiating the end-user perspective to complete the acceptance criteria. By introducing the Gherkin language, AgileGen attempts for the first time to use testable requirement descriptions as a bridge for semantic consistency between requirements and code, aiming to ensure that software products meet actual user requirements by defining user scenarios that include acceptance criteria. Additionally, we innovate in the human-AI teamwork model, allowing users to participate in decision-making processes they do well and significantly enhancing the completeness of software functionality. To ensure semantic consistency between requirements and generated code, we derive consistency factors from Gherkin to drive the subsequent software code generation. Finally, to improve the reliability of user scenarios, we also introduce a memory pool mechanism, collecting user decision-making scenarios and recommending them to new users with similar requirements. AgileGen, as a user-friendly interactive system, significantly outperformed existing best methods by 16.4\% and garnered higher user satisfaction.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {156},
numpages = {46},
keywords = {Agile, Human-AI Teamwork, Generative Software Development, User Requirement, Gherkin}
}

@inproceedings{10.1145/3643660.3643945,
author = {Lethbridge, Timothy},
title = {TAMVE: Properties of Design Technologies to Address Challenges to Software Design in the Era of Agility and Frameworks},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643660.3643945},
doi = {10.1145/3643660.3643945},
abstract = {It has been over 20 years since software design started the transition from the era of big design documents to the era where agility and the use of sophisticated frameworks has become standard. Design in both eras faced challenges: Big documents are rarely maintained properly; agility results in design either being skipped, lost, or dispersed into multiple small files; frameworks such as Ruby on Rails tend to impose a design on the system, encouraging developers to jump into coding. In this position paper, we suggest how design technologies and notations should have five properties that would help overcome many of the challenges to design in the current era. They should be Textual, Analysable, Multi-technology, Visualizable and Example-rich; we use the acronym TAMVE as a mnemonic for this. We explain how the Umple technology goes a considerable distance towards this achieving the TAMVE vision.},
booktitle = {Proceedings of the 1st International Workshop on Designing Software},
pages = {56–59},
numpages = {4},
keywords = {software design, frameworks, agility, umple, modeling},
location = {Lisbon, Portugal},
series = {Designing '24}
}

@inproceedings{10.1145/3652620.3686246,
author = {Siddeshwar, Vaishali and Alwidian, Sanaa and Makrehchi, Masoud},
title = {A Comparative Study of Large Language Models for Goal Model Extraction},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3686246},
doi = {10.1145/3652620.3686246},
abstract = {User stories, expressed in snippets of natural language text, are commonly used to elicit stakeholder's needs in agile software development. Requirement engineers model user stories to interpret the relations among goals and requirements. Manual transformation of goal models has challenges such as, difficulty of converting lower-abstraction user stories into higher-level goals, and extraction of goals embedded in user stories depends on the skill of requirements engineers. In this paper we introduce a technique that leverages Large Language Models (LLMs) to automatically generate goal models from user stories. The approach uses Iterative Prompt Engineering that guides LLM to extract intentional elements and generate its XML-compatible representation in Goal-oriented Requirements Language (GRL). The generated models can be visualized using jUCMNav tool. We evaluated our approach using three LLMs: GPT-4, Llama and Cohere. Our qualitative evaluation indicates that GPT-4 or Llama can be used to assist requirements engineers in modeling as they can produce GRL goal models that are understandable. Additionally, these LLMs are capable of exposing soft goals that are not apparent to stakeholders who are new to the domain.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {253–263},
numpages = {11},
keywords = {goal-oriented requirement language (GRL), goal modeling, user story, agile development, requirements engineering, large language models (LLMS), GPT-4, llama, cohere},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1109/TCBB.2024.3412174,
author = {Dai, Yuanfei and Zhang, Bin and Wang, Shiping},
title = {Distantly Supervised Biomedical Relation Extraction via Negative Learning and Noisy Student Self-Training},
year = {2024},
issue_date = {Nov.-Dec. 2024},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {21},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2024.3412174},
doi = {10.1109/TCBB.2024.3412174},
abstract = {Biomedical relation extraction aims to identify underlying relationships among entities, such as gene associations and drug interactions, within biomedical texts. Despite advancements in relation extraction in general knowledge domains, the scarcity of labeled training data remains a significant challenge in the biomedical field. This paper provides a novel approach for biomedical relation extraction that leverages a noisy student self-training strategy combined with negative learning. This method addresses the challenge of data insufficiency by utilizing distantly supervised data to generate high-quality labeled samples. Negative learning, as opposed to traditional positive learning, offers a more robust mechanism to discern and relabel noisy samples, preventing model overfitting. The integration of these techniques ensures enhanced noise reduction and relabeling capabilities, leading to improved performance even with noisy datasets. Experimental results demonstrate the effectiveness of the proposed framework in mitigating the impact of noisy data and outperforming existing benchmarks.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jun,
pages = {1697–1708},
numpages = {12}
}

@inproceedings{10.1145/3696410.3714901,
author = {Wang, Xin and Feng, Ling and Zhang, Huijun and Cao, Lei and Zeng, Kaisheng and Li, Qi and Ding, Yang and Dai, Yi and Clifton, David},
title = {MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714901},
doi = {10.1145/3696410.3714901},
abstract = {Stress haunts people in modern society, which may cause severe health issues if left unattended. With social media becoming an integral part of daily life, leveraging social media to detect stress has gained increasing attention. While the majority of the work focuses on classifying stress states and stress categories, this study introduce a new task aimed at estimating more specific stressors (like exam, writing paper, etc.) through users' posts on social media. Unfortunately, the diversity of stressors with many different classes but a few examples per class, combined with the consistent arising of new stressors over time, hinders the machine understanding of stressors. To this end, we cast the stressor estimation problem within a practical scenario few-shot learning setting, and propose a novel meta-learning based stressor estimation framework that is enhanced by a meta-knowledge inheritance mechanism. This model can not only learn generic stressor context through meta-learning, but also has a good generalization ability to estimate new stressors with little labeled data. A fundamental breakthrough in our approach lies in the inclusion of the meta-knowledge inheritance mechanism, which equips our model with the ability to prevent catastrophic forgetting when adapting to new stressors. The experimental results show that our model achieves state-of-the-art performance compared with the baselines. Additionally, we construct a social media-based stressor estimation dataset that can help train artificial intelligence models to facilitate human well-being.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {1866–1876},
numpages = {11},
keywords = {meta-knowledge inheritance, social media, stressor estimation},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3652620.3688335,
author = {Sedrakyan, Gayane and Iacob, Maria-Eugenia and Van Hillegersberg, Jos},
title = {Towards LowDevSecOps Framework for Low-Code Development: Integrating Process-Oriented Recommendations for Security Risk Management},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688335},
doi = {10.1145/3652620.3688335},
abstract = {The increasing demand for software solutions in the coming years will surpass the availability of IT talent, driving interest in citizen development and low-code approaches. However, the lack of technical insight among citizen developers poses potential security risks. This research aims to support businesses adopting citizen development by providing a framework that helps to proactively identify security risks by also linking them to specific actors and tools needed during the system design and development process to mitigate those risks. Additionally, this framework helps to address knowledge gaps by outlining actionable steps to ensure secure low-code development practices. The research aims to answer the question: "How can contextual information be modeled in low-code platforms to proactively identify and address security-related issues, acting as a virtual mentor for citizen / low-code developers?". To answer this question, our research conceptualizes security risks from established frameworks and operational security methodologies into a practical framework that allows mapping security risks to the context of low-code development. This framework serves as a foundational platform for designing and integrating active process-oriented guidance within low-code platforms using model-based automated prompts. This approach additionally aligns with DevSecOps principles that allows enhancing the capacity for low-code approach and citizen development in areas that currently may include manual coding and integrations.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {886–894},
numpages = {9},
keywords = {low code development, modeling, recommenders, security, devops, devsecops},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3656296,
author = {Wang, Changjie and Scazzariello, Mariano and Farshin, Alireza and Ferlin, Simone and Kosti\'{c}, Dejan and Chiesa, Marco},
title = {NetConfEval: Can LLMs Facilitate Network Configuration?},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CoNEXT2},
url = {https://doi.org/10.1145/3656296},
doi = {10.1145/3656296},
abstract = {This paper explores opportunities to utilize Large Language Models (LLMs) to make network configuration human-friendly, simplifying the configuration of network devices \&amp; development of routing algorithms and minimizing errors. We design a set of benchmarks (NetConfEval) to examine the effectiveness of different models in facilitating and automating network configuration. More specifically, we focus on the scenarios where LLMs translate high-level policies, requirements, and descriptions (i.e., specified in natural language) into low-level network configurations \&amp; Python code. NetConfEval considers four tasks that could potentially facilitate network configuration, such as (i) generating high-level requirements into a formal specification format, (ii) generating API/function calls from high-level requirements, (iii) developing routing algorithms based on high-level descriptions, and (iv) generating low-level configuration for existing and new protocols based on input documentation. Learning from the results of our study, we propose a set of principles to design LLM-based systems to configure networks. Finally, we present two GPT-4-based prototypes to (i) automatically configure P4-enabled devices from a set of high-level requirements and (ii) integrate LLMs into existing network synthesizers.},
journal = {Proc. ACM Netw.},
month = jun,
articleno = {7},
numpages = {25},
keywords = {benchmark, code generation, function calling, large language models (llms), network configuration, network synthesizer, p4, rag, routing algorithms}
}

@inproceedings{10.1145/3675888.3676107,
author = {Kumar, Pratiksh and Gupta, Rishik and Kumar, Bagesh and Kumar, Aman},
title = {Bridging the Gap: Leveraging Textual and Visual Contexts for PreciseMedical Visual Question Answering},
year = {2024},
isbn = {9798400709722},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675888.3676107},
doi = {10.1145/3675888.3676107},
abstract = {The advent of Visual Question Answering (VQA) technology has brought significant advancements in the medical field, offering transformative potential in clinical diagnostics and patient care. This research explores the application of VQA within the medical domain, highlighting its critical role in interpreting complex visual data, such as radiological images, pathology slides, and other diagnostic visuals. Traditional diagnostic processes often rely heavily on human expertise, which can be time-consuming and prone to variability. VQA systems, powered by sophisticated machine learning models, provide consistent and accurate interpretations, thus enhancing diagnostic accuracy and efficiency. Visual Question Answering (VQA) in the medical field necessitates extracting information from both textual and visual inputs to provide accurate answers, a critical requirement for supporting medical decision-making. This research introduces a novel approach to address VQA challenges in the medical domain using Bi-Directional Layout with Positional Encoding (BLIP) models. Our methodology seamlessly integrates text and image processing within a unified framework, enabling precise interactions between textual queries and medical imaging data. We commence with textual inputs, encoded by BLIP processors, and medical images, encoded by BLIP image processors. A custom VQA dataset, specifically designed for the medical field, includes textual questions and their corresponding medical image features. We employ a BLIP-based Question Answering architecture, fine-tuned on our medical VQA dataset, and optimized using the AdamW optimizer with a learning rate of 0.00005, ensuring efficient convergence. Additionally, we introduce attention mechanisms using Coarse and Fine Attention blocks for enhanced feature fusion and accurate answer prediction. Our results are highly encouraging, demonstrating competitive metrics in extensive VQA task experiments on both training and validation datasets. Qualitative analysis of sample predictions indicates the model’s capability to provide accurate answers for diverse visual and textual medical inputs. This work holds significant promise for improving automated medical image analysis and supporting clinical decision-making.},
booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing},
pages = {519–526},
numpages = {8},
keywords = {Attention Model, BLIP, Medical Visual Question Answering, PathVQA},
location = {Noida, India},
series = {IC3-2024}
}

@inproceedings{10.1145/3695053.3731409,
author = {Coburn, Joel and Tang, Chunqiang and Asal, Sameer Abu and Agrawal, Neeraj and Chinta, Raviteja and Dixit, Harish and Dodds, Brian and Dwarakapuram, Saritha and Firoozshahian, Amin and Gao, Cao and Gondkar, Kaustubh and Graf, Tyler and Hu, Junhan and Huang, Jian and Hughes, Sterling and Hutchin, Adam and Jakka, Bhasker and Chen, Guoqiang Jerry and Kalyanaraman, Indu and Kamath, Ashwin and Kansal, Pankaj and Kazi, Erum and Levenstein, Roman and Maddury, Mahesh and Mastro, Alex and Medaiyese, Siji and Modi, Pritesh and Montgomery, Jack and Nadathur, Satish and Nagpal, Amit and Narasimha, Ashwin and Naumov, Maxim and Ozer, Eleanor and Park, Jongsoo and Ramani, Poorvaja and Reddy, Harikrishna and Reiss, David and Roy, Deboleena and Sekar, Sathish and Sharma, Arushi and Shetty, Pavan and Sukumaran-Rajam, Aravind and Tal, Eran and Tsai, Mike and Varshini, Shreya and Wareing, Richard and Wu, Olivia and Xie, Xiaolong and Yang, Jinghan and Yu, Hangchen and Zargar, Tanmay and Zeng, Zitong and Zhang, Feixiong and Matthews, Ajit and Jiao, Xun and Zhang, Jiyuan and Menage, Emmanuel and Stokke, Truls Edvard and Sourouri, Mohammed},
title = {Meta's Second Generation AI Chip: Model-Chip Co-Design and Productionization Experiences},
year = {2025},
isbn = {9798400712616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3695053.3731409},
doi = {10.1145/3695053.3731409},
abstract = {The rapid growth of AI workloads at Meta has motivated our in-house development of AI chips, aiming to significantly reduce the total cost of ownership and mitigate risks posed by unpredictable GPU supplies. At ISCA’23, we presented Meta’s first-generation AI chip, MTIA 1. This paper describes its successor, MTIA 2i, now deployed at scale and serving billions of users. MTIA 2i significantly improves upon MTIA 1, reducing total cost of ownership by 44\% compared to GPUs while delivering competitive performance per watt. A key differentiator is its memory hierarchy: instead of costly HBM, it uses large SRAM alongside LPDDR. Although there has been a proliferation of publications on AI chips, they often focus on architectural design and overlook three critical aspects: (1)&nbsp;co-designing and optimizing ML models to work effectively with the AI chip; (2) demonstrating sufficient flexibility to support a wide range of models; and (3)&nbsp;during the productionization process, addressing challenges unanticipated or decisions deferred at design time, such as dealing with memory errors, safe overclocking, reducing provisioned power, and implementing real-time firmware updates to mitigate silicon design defects. A key contribution of this paper is sharing our experience with these aspects, based on our journey of productionizing MTIA 2i at scale.},
booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture},
pages = {1689–1702},
numpages = {14},
keywords = {Accelerators, Artificial intelligence, Inference, Machine learning, Memory hierarchy, Productionization, Deep learning recommendation models, Total Cost of Ownership},
location = {
},
series = {ISCA '25}
}

@inproceedings{10.1145/3658644.3690327,
author = {Wang, Peiran and Li, Qiyu and Yu, Longxuan and Wang, Ziyao and Li, Ang and Jin, Haojian},
title = {Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3690327},
doi = {10.1145/3658644.3690327},
abstract = {We present Moderator, a policy-based model management system that allows administrators to specify fine-grained content moderation policies and modify the weights of a text-to-image (TTI) model to make it significantly more challenging for users to produce images that violate the policies. In contrast to existing general-purpose model editing techniques, which unlearn concepts without considering the associated contexts, Moderator allows admins to specify what content should be moderated, under which context, how it should be moderated, and why moderation is necessary. Given a set of policies, Moderator first prompts the original model to generate images that need to be moderated, then uses these self-generated images to reverse fine-tune the model to compute task vectors for moderation and finally negates the original model with the task vectors to decrease its performance in generating moderated content. We evaluated Moderator with 14 participants to play the role of admins and found they could quickly learn and author policies to pass unit tests in approximately 2.29 policy iterations. Our experiment with 32 stable diffusion users suggested that Moderator can prevent 65\% of users from generating moderated content under 15 attempts and require the remaining users an average of 8.3 times more attempts to generate undesired content.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {1181–1195},
numpages = {15},
keywords = {content moderation, policy language, text-to-image model},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3721145.3725774,
author = {Guan, Jiexiong and Hu, Zhenqing and Antonopoulos, Christos D. and Bellas, Nikolaos and Lalis, Spyros and Smirni, Evgenia and Zhou, Gang and Agrawal, Gagan and Ren, Bin},
title = {TMModel: Modeling Texture Memory and Mobile GPU Performance to Accelerate DNN Computations},
year = {2025},
isbn = {9798400715372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721145.3725774},
doi = {10.1145/3721145.3725774},
abstract = {The demand for Deep Neural Network (DNN) execution (including both inference and training) on mobile system-on-a-chip (SoCs) has surged, driven by factors like the need for real-time latency, privacy, and reducing vendors’ costs. Mainstream mobile GPUs (e.g., Qualcomm Adreno GPUs) usually have a 2.5D L1 texture cache that offers throughput superior to that of on-chip memory. However, to date, there is limited understanding of the performance features of such a 2.5D cache, which limits the optimization potential. This paper introduces TMModel, a framework with three components: 1) a set of micro-benchmarks and a novel performance assessment methodology to characterize a non-well-documented architecture with 2D memory, 2) a complete analytical performance model configurable for different data access pattern(s), tiling size(s), and other GPU execution parameters for a given operator (and associated size and shape), and 3) a compilation framework incorporating this model and generating optimized code with low overhead. TMModel is validated both on a set of DNN kernels and for training complete models on mobile GPU, and compared against both popular mobile DNN frameworks and another GPU performance model. Evaluation results demonstrate that TMModel outperforms all baselines, achieving 1.48 − 3.61 \texttimes{} speedup on individual kernels and 1.83 − 66.1 \texttimes{} speedup for end-to-end on-device training with only (0.25\%-18.5\%) the tuning cost of the baselines.},
booktitle = {Proceedings of the 39th ACM International Conference on Supercomputing},
pages = {205–220},
numpages = {16},
keywords = {Texture Memory, Performance Modeling, Architecture Profiling, Automatic Code Generation, On-device Training},
location = {
},
series = {ICS '25}
}

@article{10.1145/3720449,
author = {Li, Zongjie and Wu, Daoyuan and Wang, Shuai and Su, Zhendong},
title = {API-Guided Dataset Synthesis to Finetune Large Code Models},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720449},
doi = {10.1145/3720449},
abstract = {Large code models (LCMs), pre-trained on vast code corpora, have demonstrated remarkable performance        across a wide array of code-related tasks. Supervised fine-tuning (SFT) plays a vital role in aligning these models        with specific requirements and enhancing their performance in particular domains. However, synthesizing        high-quality SFT datasets poses a significant challenge due to the uneven quality of datasets and the scarcity        of domain-specific datasets.        Inspired by APIs as high-level abstractions of code that encapsulate rich semantic information in a concise        structure, we propose DataScope, an API-guided dataset synthesis framework designed to enhance the SFT        process for LCMs in both general and domain-specific scenarios. DataScope comprises two main components:        Dslt and Dgen. On the one hand, Dslt employs API coverage as a core metric, enabling efficient dataset        synthesis in general scenarios by selecting subsets of existing (uneven-quality) datasets with higher API        coverage. On the other hand, Dgen recasts domain dataset synthesis as a process of using API-specified        high-level functionality and deliberately constituted code skeletons to synthesize concrete code.        Extensive experiments demonstrate DataScope’s effectiveness, with models fine-tuned on its synthesized        datasets outperforming those tuned on unoptimized datasets five times larger. Furthermore, a series of analyses        on model internals, relevant hyperparameters, and case studies provide additional evidence for the efficacy        of our proposed methods. These findings underscore the significance of dataset quality in SFT and advance        the field of LCMs by providing an efficient, cost-effective framework for constructing high-quality datasets,        which in turn lead to more powerful and tailored LCMs for both general and domain-specific scenarios.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {108},
numpages = {30},
keywords = {API coverage, Code synthesis, Large code models}
}

@inproceedings{10.1109/ASE56229.2023.00064,
author = {Shahariar, G. M. and Hasant, Tahmid and Iqbalt, Anindya and Uddin, Gias},
title = {Contrastive Learning for API Aspect Analysis},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00064},
doi = {10.1109/ASE56229.2023.00064},
abstract = {We present a novel approach - CLAA - for API aspect detection in API reviews that utilizes transformer models trained with a supervised contrastive loss objective function. We evaluate CLAA using performance and impact analysis. For performance analysis, we utilized a benchmark dataset on developer discussions collected from Stack Overflow and compare the results to those obtained using state-of-the-art transformer models. Our experiments show that contrastive learning can significantly improve the performance of transformer models in detecting aspects such as Performance, Security, Usability, and Documentation. For impact analysis, we performed empirical and developer study. On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92\% accuracy while the SOTA baseline achieved 81.5\%. According to our developer study involving 10 participants, the use of Stack Overflow + CLAA resulted in increased accuracy and confidence during API selection. Replication package: https://github.com/disa-lab/Contrastive-Learning-API-Aspect-ASE2023.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {637–648},
numpages = {12},
keywords = {API aspects, contrastive learning, transformers, API review, aspect detection, LIME},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1145/3652594,
author = {Bayer, Markus and Kuehn, Philipp and Shanehsaz, Ramin and Reuter, Christian},
title = {CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {2471-2566},
url = {https://doi.org/10.1145/3652594},
doi = {10.1145/3652594},
abstract = {The field of cysec is evolving fast. Security professionals are in need of intelligence on past, current and —ideally — upcoming threats, because attacks are becoming more advanced and are increasingly targeting larger and more complex systems. Since the processing and analysis of such large amounts of information cannot be addressed manually, cysec experts rely on machine learning techniques. In the textual domain, pre-trained language models such as Bidirectional Encoder Representations from Transformers (BERT) have proven to be helpful as they provide a good baseline for further fine-tuning. However, due to the domain-knowledge and the many technical terms in cysec, general language models might miss the gist of textual information. For this reason, we create a high-quality dataset1 and present a language model2 specifically tailored to the cysec domain that can serve as a basic building block for cybersecurity systems. The model is compared on 15 tasks: Domain-dependent extrinsic tasks for measuring the performance on specific problems, intrinsic tasks for measuring the performance of the internal representations of the model, as well as general tasks from the SuperGLUE benchmark. The results of the intrinsic tasks show that our model improves the internal representation space of domain words compared with the other models. The extrinsic, domain-dependent tasks, consisting of sequence tagging and classification, show that the model performs best in cybersecurity scenarios. In addition, we pay special attention to the choice of hyperparameters against catastrophic forgetting, as pre-trained models tend to forget the original knowledge during further training.},
journal = {ACM Trans. Priv. Secur.},
month = apr,
articleno = {18},
numpages = {20},
keywords = {Language model, cybersecurity BERT, cybersecurity dataset}
}

@inproceedings{10.1145/3706598.3713472,
author = {McNutt, Andrew M and Cohen, Sam and Chugh, Ravi},
title = {Slowness, Politics, and Joy: Values That Guide Technology Choices in Creative Coding Classrooms},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713472},
doi = {10.1145/3706598.3713472},
abstract = {There are many tools and technologies for making art with code, each embodying distinct values and affordances. Within this landscape, creative coding educators must evaluate how different tools map onto their own principles and examine the potential impacts of those choices on students’ learning and artistic development. Understanding the values guiding these decisions is critical, as they reflect insights about these contexts, communities, and pedagogies. We explore these values through semi-structured interviews with (N=12) creative coding educators and toolbuilders. We identify three major themes: slowness (how friction can make room for reflection), politics (including the lasting effects of particular technologies), and joy (or the capacity for playful engagement). The lessons and priorities voiced by our participants offer valuable, transferable perspectives—like preferring community building (such as through documentation) over techno-solutionism. We demonstrate application of these critical lenses to two tool design areas (accessibility and AI assistance).},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {54},
numpages = {16},
keywords = {Creative Coding, Interview Study, Power, Reflection, Arts},
location = {
},
series = {CHI '25}
}

@article{10.1145/3729362,
author = {Jang, Sujin and Ryou, Yeonhee and Lee, Heewon and Heo, Kihong},
title = {UnitCon: Synthesizing Targeted Unit Tests for Java Runtime Exceptions},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729362},
doi = {10.1145/3729362},
abstract = {We present UnitCon, a system for synthesizing targeted unit testsfor runtime exceptions in Java programs. Targeted unit tests aim to reveal a bug at a specific location in the program under test. This capability benefits various tasks in software development, such as patch testing, crash reproduction, or static analysis alarm inspection. However, conventional unit test generation tools are mainly designed for regression tests by maximizing code coverage; hence they are not effective at such target-specific tasks. In this paper, we propose a novel synthesis technique that effectively guides the search for targeted unit tests. The key idea is to use static analysis to prune and prioritize the search space by estimating the semantics of candidate test cases. This allows us to efficiently focus on promising unit tests that are likely to trigger runtime exceptions at the target location. According to our experiments on a suite of Java programs, our approach outperforms the state-of-the-art unit test generation tools. We also applied UnitCon for inspecting static analysis alarms for null pointer exceptions (NPEs) in 51 open-source projects and discovered 21 previously unknown NPE bugs.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE092},
numpages = {22},
keywords = {Program analysis, Program synthesis, Software testing}
}

@article{10.1145/3767739,
author = {Wang, Fanyu and Arora, Chetan and Tantithamthavorn, Chakkrit and Huang, Kaicheng and Aleti, Aldeida},
title = {Requirements-Driven Automated Software Testing: A Systematic Review},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3767739},
doi = {10.1145/3767739},
abstract = {Automated software testing has significant potential to enhance efficiency and reliability within software development processes. However, its broader adoption faces considerable challenges, particularly concerning alignment between test generation methodologies and software requirements. REquirements-Driven Automated Software Testing (REDAST) addresses this gap by systematically leveraging requirements as the foundation for automated test artifact generation. This systematic literature review (SLR) critically examines the REDAST landscape, analyzing the current state of requirements input formats, transformation techniques, generated test artifacts, evaluation methods, and prevailing limitations. We conducted a thorough analysis of 156 relevant studies selected through a rigorous multi-stage filtering process from an initial collection of 27,333 papers sourced from six major research databases. Our findings highlight the predominance of functional requirements, model-based specifications, and natural language formats. Rule-based techniques are extensively utilized, while machine learning-based approaches remain relatively underexplored. Furthermore, most existing frameworks are sequential and dependent on singular intermediate representations, and while test cases, structured textual formats, and requirements coverage are common, full automation remains rare. We identify significant gaps related to automation completeness, and dependency on input quality. This comprehensive synthesis provides a detailed overview of REDAST research and limitations, offering clear, evidence-based recommendations to guide future advancements in automated software testing.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
keywords = {Software Engineering, Requirements Engineering, Software Testing, Automated Test Generation, Systematic Literature Review}
}

@inproceedings{10.1145/3646548.3672584,
author = {Kogler, Philipp and Chen, Wei and Falkner, Andreas and Haselb\"{o}ck, Alois and Wallner, Stefan},
title = {Modelling Engineering Processes in Natural Language: A Case Study},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672584},
doi = {10.1145/3646548.3672584},
abstract = {Engineering process management aims to formally specify processes which are executable, measurable, and controllable. Common representations include text-based domain-specific languages (DSLs) or graphical notations such as the Business Process Modelling Notation (BPMN). The specification itself can be seen as a Software Product Line (SPL), building upon concepts such as tasks, UI forms, fields and actions. Domain experts provide requirements for processes but often lack the technical programming skills to formalize them in a process specification language. We present an interactive SPL application prototype that allows domain experts to model simple processes in natural language. Our framework for the reliable generation of formal specifications with Large Language Models (LLMs) supports the machine-translation from natural language to a JSON-based process DSL. In this case study, five domain experts were asked to model any process of their choice through natural-language interactions. As a result, the user interface corresponding to the process DSL was shown as immediate feedback. We documented their perceived translation quality and interviewed them on their impressions of this methodology. An average user-assessed performance rating of 68\% was achieved. Even though the modelling strategies differed greatly between individuals, the tool was able to adequately capture the majority of instructions, leaving an overall positive impression on the participants. More context awareness and additional conventional interaction elements were the main aspects found to be improved for a productive implementation.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {170–178},
numpages = {9},
keywords = {Domain-specific Languages, Generative Artificial Intelligence, Large Language Models, Process Management, Process Modelling, Reliable Code Generation},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3550356.3561542,
author = {Cabot, Jordi and Delgado, David and Burgue\~{n}o, Lola},
title = {Combining OCL and natural language: a call for a community effort},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561542},
doi = {10.1145/3550356.3561542},
abstract = {The growing popularity and availability of pretrained natural language models opens the door to many interesting applications combining natural language (NL) with software artefacts. A couple of examples are the generation of code excerpts from NL instructions or the verbalization of programs in NL to facilitate their comprehension.Many of these language models have been trained with open source software datasets and therefore "understand" a variety of programming languages, but not OCL.We argue that OCL needs to jump into the machine learning bandwagon or it will risk losing its appeal as a constraint specification language. For that, the key first task is to create together an OCL corpus dataset amenable for natural language processing.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {908–912},
numpages = {5},
keywords = {OCL, community, corpus, dataset, natural language},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3652620.3687820,
author = {Moln\'{a}r, Vince and Graics, Bence and V\"{o}r\"{o}s, Andr\'{a}s and Tonetta, Stefano and Cristoforetti, Luca and Kimberly, Greg and Dyer, Pamela and Giammarco, Kristin and Koethe, Manfred and Hester, John and Smith, Jamie and Grimm, Christoph},
title = {Towards the Formal Verification of SysML v2 Models},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687820},
doi = {10.1145/3652620.3687820},
abstract = {Systems Modeling Language (SysML) is the de facto standard in the industry for modeling complex systems. SysML v2 is the new version of the language with reworked fundamentals. In this paper, we explore how the new formal semantics of SysML v2 can enable formal verification and various forms of automated reasoning. Formal verification involves mathematically proving the correctness of a system's design with respect to certain specifications or properties. This rigorous approach ensures that models behave as intended under all possible conditions. Through a detailed examination, we demonstrate how five specific tools - Gamma, MP-Firebird, Imandra, SAVVS, and SysMD - can formally analyze SysML v2 models. We show how these tools support the different concepts in the language, as well as the set of features and technologies they provide to users of SysML v2, such as model checking, theorem proving, contract-based design, or automatic fault injections. We propose a workflow for applying formal methods on SysML v2 models, illustrated by example models and artifacts generated by the above tools.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1086–1095},
numpages = {10},
keywords = {SysML V2, systems modeling, formal methods, verification and validation, automated reasoning, tools},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3587691,
author = {Hirzel, Martin},
title = {Low-Code Programming Models},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3587691},
doi = {10.1145/3587691},
abstract = {Low-code has the potential to empower more people to automate tasks by creating computer programs.},
journal = {Commun. ACM},
month = sep,
pages = {76–85},
numpages = {10}
}

@inproceedings{10.1145/3663529.3663779,
author = {Zheng, Xi and Mok, Aloysius K. and Piskac, Ruzica and Lee, Yong Jae and Krishnamachari, Bhaskar and Zhu, Dakai and Sokolsky, Oleg and Lee, Insup},
title = {Testing Learning-Enabled Cyber-Physical Systems with Large-Language Models: A Formal Approach},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663779},
doi = {10.1145/3663529.3663779},
abstract = {The integration of machine learning into cyber-physical systems (CPS) promises enhanced efficiency and autonomous capabilities, revolutionizing fields like autonomous vehicles and telemedicine. This evolution necessitates a shift in the software development life cycle, where data and learning are pivotal. Traditional verification and validation methods are inadequate for these AI-driven systems. This study focuses on the challenges in ensuring safety in learning-enabled CPS. It emphasizes the role of testing as a primary method for verification and validation, critiques current methodologies, and advocates for a more rigorous approach to assure formal safety.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {467–471},
numpages = {5},
keywords = {AI-based Systems, LLM-based Testing, automata-learning, model-based testing},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@article{10.1145/3747290,
author = {Dahou, Abdelghani and Dahou, Abdelhalim Hafedh and Cheragui, Mohamed Amine and Abdedaiem, Amin and Al-Qaness, Mohammed A. A. and Abd Elaziz, Mohamed and Ewees, Ahmed A. and Zheng, Zhonglong},
title = {A Survey on Dialect Arabic Processing and Analysis: Recent Advances and Future Trends},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3747290},
doi = {10.1145/3747290},
abstract = {Advances in language models have enabled significant strides in developing language technologies tailored for analyzing and processing Dialectical Arabic (DA), which exhibits unique linguistic features and variations compared to standard Arabic. This progress has sparked a surge of interest in various research tasks within the Arabic Natural Language Processing (ANLP) domain, encompassing areas such as sentiment analysis, dialect identification, normalization and classification, fake news detection, and part-of-speech tagging. The primary objective of this survey paper is to provide a comprehensive overview of the advancements made in dialectical ANLP from 2014 to 2024. A thorough analysis is undertaken, covering a corpus of approximately 200 research papers, to offer insights into the latest developments, resources, and applications concerning dialectical Arabic. By identifying and discussing the challenges and opportunities for future research, this study aspires to serve as a valuable reference for researchers, practitioners, and enthusiasts interested in the subject matter. Central to the investigation are the recent strides in natural language processing techniques that pertain to dialectical Arabic, namely DA sentiment analysis, DA identification, DA classification, DA normalization, DA part-of-speech tagging, and the role of DA in fake news detection, among other applications. Each research category is meticulously examined, providing a comprehensive understanding of their respective contributions, significance, encountered challenges, and the availability of pertinent datasets. This exhaustive survey paper encompasses existing studies within dialectical Arabic research categories. As a result, readers are presented with a detailed reference source in pursuing advancements and innovations within this field.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {84},
numpages = {45},
keywords = {Deep learning, arabic dialect, sentiment analysis, classification, POS tag, datasets, fake news}
}

@inproceedings{10.1145/3640310.3674079,
author = {Sultan, Bastien and Apvrille, Ludovic},
title = {AI-Driven Consistency of SysML Diagrams},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674079},
doi = {10.1145/3640310.3674079},
abstract = {Graphical modeling languages, expected to simplify systems analysis and design, present a challenge in maintaining consistency across their varied views. Traditional rule-based methods for ensuring consistency in languages like UML often fall short in addressing complex semantic dimensions. Moreover, the integration of Large Language Models (LLMs) into Model Driven Engineering (MDE) introduces additional consistency challenges, as LLM's limited output contexts requires the integration of responses. This paper presents a new framework that automates the detection and correction of inconsistencies across different views, leveraging formally defined rules and incorporating OpenAI's GPT, as implemented in TTool. Focusing on the consistency between use case and block diagrams, the framework is evaluated through its application to three case studies, highlighting its potential to significantly enhance consistency management in graphical modeling.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {149–159},
numpages = {11},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3608298.3608324,
author = {Schl\"{o}r, Daniel and Pfister, Jan and Hotho, Andreas},
title = {Optimizing Medical Service Request Processes through Language Modeling and Semantic Search},
year = {2023},
isbn = {9798400700712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3608298.3608324},
doi = {10.1145/3608298.3608324},
abstract = {Medical service requests are a crucial part of the workflow in hospitals and healthcare organizations. However, the process of requesting medical services can be time consuming and can require physicians and medical personnel to navigate complex interfaces and enter detailed information about the requested service. In this paper, we propose a system that uses machine learning techniques such as large language models and semantic search to optimize the process of requesting medical services. Our approach enables physicians to request medical services using natural language rather than navigating complex interfaces, allowing for more efficient and flexible interactions with hospital information systems. We evaluate our approach on real-world data and discuss the implications of our work for the future of digital health care. Our results suggest that our approach has the potential to streamline the process of requesting medical services and reduce the time and manual effort required in the daily hospital routine.},
booktitle = {Proceedings of the 2023 7th International Conference on Medical and Health Informatics},
pages = {136–141},
numpages = {6},
keywords = {language modeling, medical service optimization, semantic search},
location = {Kyoto, Japan},
series = {ICMHI '23}
}

@article{10.1145/3728947,
author = {Fu, Yingjie and Li, Bozhou and Li, Linyi and Zhang, Wentao and Xie, Tao},
title = {The First Prompt Counts the Most! An Evaluation of Large Language Models on Iterative Example-Based Code Generation},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728947},
doi = {10.1145/3728947},
abstract = {The capabilities of Large Language Models (LLMs) in code generation have been extensively studied, particularly for implementing target functionalities from natural-language descriptions. As an alternative to natural language, input-output (I/O) examples provide an accessible, unambiguous, and flexible way to describe functionalities. However, their inherent diversity, opaqueness, and incompleteness impose greater challenges for understanding and implementing the target requirements. Therefore, generating code from I/O examples (i.e., example-based code generation) provides a new perspective, allowing us to additionally evaluate LLMs’ capability to infer target functionalities from limited information and to process new-form requirements. However, related research about LLMs in example-based code generation remains largely unexplored. To fill this gap, this paper presents the first comprehensive study on example-based code generation using LLMs. To address the incorrectness caused by the incompleteness of I/O examples, we adopt an iterative evaluation framework and formalize the objective of example-based code generation as two sequential sub-objectives: generating code conforming to the given examples and generating code that successfully implements the target functionalities from (iteratively) given examples. We assess six state-of-the-art LLMs using a new benchmark of 172 diverse target functionalities (derived from HumanEval and CodeHunt). The results demonstrate that when requirements are described using iterative I/O examples rather than natural language, the LLMs’ score decreases by over 60\%, indicating that example-based code generation remains challenging for the evaluated LLMs. Notably, the vast majority (even over 95\%) of successfully implemented functionalities are achieved in the first round of the iterations, suggesting that the LLMs struggle to effectively utilize the iteratively supplemented requirements. Furthermore, we find that combining I/O examples with even imprecise and fragmental natural language descriptions greatly improves LLM performance, and the selection of initial I/O examples can also influence the score, suggesting opportunities for prompt optimization. These findings highlight the importance of early prompts during interactions and offer critical insights and implications for enhancing LLM-based code generation.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA070},
numpages = {24},
keywords = {Empirical Study, Example-Based Code Generation, Large Language Models, Multi-Turn Interaction, Prompt Engineering}
}

@inproceedings{10.1145/3533767.3534396,
author = {Zhang, Jialu and Mytkowicz, Todd and Kaufman, Mike and Piskac, Ruzica and Lahiri, Shuvendu K.},
title = {Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534396},
doi = {10.1145/3533767.3534396},
abstract = {Program merging is standard practice when developers integrate their individual changes to a common code base. When the merge algorithm fails, this is called a merge conflict. The conflict either manifests as a textual merge conflict where the merge fails to produce code, or as a semantic merge conflict where the merged code results in compiler errors or broken tests. Resolving these conflicts for large code projects is expensive because it requires developers to manually identify the sources of conflicts and correct them.   In this paper, we explore the feasibility of automatically repairing merge conflicts (both textual and semantic) using k-shot learning with pre-trained large neural language models (LM) such as GPT-3. One of the challenges in leveraging such language models is fitting the examples and the queries within a small prompt (2048 tokens). We evaluate LMs and k-shot learning for both textual and semantic merge conflicts for Microsoft Edge. Our results are mixed: on one-hand, LMs provide the state-of-the-art (SOTA) performance on semantic merge conflict resolution for Edge compared to earlier symbolic approaches; on the other hand, LMs do not yet obviate the benefits of special purpose domain-specific languages (DSL) for restricted patterns for program synthesis.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {77–88},
numpages = {12},
keywords = {GPT-3, Resolving merge conflicts, k-shot learning, language model},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@article{10.1145/3622830,
author = {Feser, Jack and Dillig, I\c{s}\i{}l and Solar-Lezama, Armando},
title = {Inductive Program Synthesis Guided by Observational Program Similarity},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622830},
doi = {10.1145/3622830},
abstract = {We present a new general-purpose synthesis technique for generating programs from input-output examples. Our method, called metric program synthesis, relaxes the observational equivalence idea (used widely in bottom-up enumerative synthesis) into a weaker notion of observational similarity, with the goal of reducing the search space that the synthesizer needs to explore. Our method clusters programs into equivalence classes based on an expert-provided distance metric and constructs a version space that compactly represents “approximately correct” programs. Then, given a “close enough” program sampled from this version space, our approach uses a distance-guided repair algorithm to find a program that exactly matches the given input-output examples. We have implemented our proposed metric program synthesis technique in a tool called SyMetric and evaluate it in three different domains considered in prior work. Our evaluation shows that SyMetric outperforms other domain-agnostic synthesizers that use observational equivalence and that it achieves results competitive with domain-specific synthesizers that are either designed for or trained on those domains.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {254},
numpages = {29},
keywords = {distance metric, inverse csg, program synthesis, regular expression inference}
}

@article{10.1109/TCBB.2023.3236477,
author = {Liang, Tingting and Xia, Congying and Zhao, Ziqiang and Jiang, Yixuan and Yin, Yuyu and Yu, Philip S.},
title = {Transferring From Textual Entailment to Biomedical Named Entity Recognition},
year = {2023},
issue_date = {July-Aug. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2023.3236477},
doi = {10.1109/TCBB.2023.3236477},
abstract = {Biomedical Named Entity Recognition (BioNER) aims at identifying biomedical entities such as genes, proteins, diseases, and chemical compounds in the given textual data. However, due to the issues of ethics, privacy, and high specialization of biomedical data, BioNER suffers from the more severe problem of lacking in quality labeled data than the general domain especially for the token-level. Facing the extremely limited labeled biomedical data, this work studies the problem of gazetteer-based BioNER, which aims at building a BioNER system from scratch. It needs to identify the entities in the given sentences when we have zero token-level annotations for training. Previous works usually use sequential labeling models to solve the NER or BioNER task and obtain weakly labeled data from gazetteers when we don't have full annotations. However, these labeled data are quite noisy since we need the labels for each token and the entity coverage of the gazetteers is limited. Here we propose to formulate the BioNER task as a Textual Entailment problem and solve the task via Textual Entailment with Dynamic Contrastive learning (TEDC). TEDC not only alleviates the noisy labeling issue, but also transfers the knowledge from pre-trained textual entailment models. Additionally, the dynamic contrastive learning framework contrasts the entities and non-entities in the same sentence and improves the model's discrimination ability. Experiments on two real-world biomedical datasets show that TEDC can achieve state-of-the-art performance for gazetteer-based BioNER.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {2577–2586},
numpages = {10}
}

@article{10.1145/3687922,
author = {Ganeshan, Aditya and Huang, Ryan and Xu, Xianghao and Jones, R. Kenny and Ritchie, Daniel},
title = {ParSEL: Parameterized Shape Editing with Language},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3687922},
doi = {10.1145/3687922},
abstract = {The ability to edit 3D assets with natural language presents a compelling paradigm to aid in the democratization of 3D content creation. However, while natural language is often effective at communicating general intent, it is poorly suited for specifying exact manipulation. To address this gap, we introduce ParSEL, a system that enables controllable editing of high-quality 3D assets with natural language. Given a segmented 3D mesh and an editing request, ParSEL produces a parameterized editing program. Adjusting these parameters allows users to explore shape variations with exact control over the magnitude of the edits. To infer editing programs which align with an input edit request, we leverage the abilities of large-language models (LLMs). However, we find that although LLMs excel at identifying the initial edit operations, they often fail to infer complete editing programs, resulting in outputs that violate shape semantics. To overcome this issue, we introduce Analytical Edit Propagation (AEP), an algorithm which extends a seed edit with additional operations until a complete editing program has been formed. Unlike prior methods, AEP searches for analytical editing operations compatible with a range of possible user edits through the integration of computer algebra systems for geometric analysis. Experimentally, we demonstrate ParSEL's effectiveness in enabling controllable editing of 3D objects through natural language requests over alternative system designs.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {197},
numpages = {14},
keywords = {shape editing, parametric editing, large language models, computer algebra systems, neuro-symbolic methods, program synthesis}
}

@article{10.1145/3731752,
author = {Liang, Qingyuan and Sun, Zeyu and Zhao, Yifan and Gong, Zhihao and Wang, Guoqing and Chen, Yizhou and Zhang, Lu and Liang, Guangtai and Wang, Qianxiang},
title = {Bipartite-Grammar Aware Pretraining for XML-SQL Code Updating},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3731752},
doi = {10.1145/3731752},
abstract = {The eXtensible Markup Language (XML) is a file format widely used for data transmission in modern software development. In recent years, embedding SQL statements in XML files (i.e., XML-SQL) has become a popular way for developing applications with database access capability. Typically, XML-SQL code snippets demonstrate similar functionalities and structures, leading to repetitive programming work. Therefore, leveraging pre-trained code models for automated code generation presents a promising way to alleviate duplicated efforts and enhance the efficiency of developing XML-SQL code. However, XML-SQL code has strong domain-specific characteristics that general pre-trained code models typically struggle to fully harness, thereby leading to limited overall performance of general pre-trained code models. In this paper, we aim to address the challenge of handling this domain-specific knowledge. First, we propose a code updating task and construct the corresponding TwinXSQL dataset to better evaluate the model’s code generation performance in the XML-SQL domain. Then, we leverage the common characteristics of XML-SQL and other programming languages (i.e., all programming languages impose grammar constraints on behaviour) to design a bipartite-grammar-aware training framework (named BGA) for unsupervised pre-training, thereby improving the transfer of general-purpose code models to the XML-SQL domain. Specifically, we divide the XML-SQL code into two types of grammatical components: structure components and value components. During pre-training, we undertake three tasks, each designed to learn the internal information of these grammatical components and the relationships between them, enabling the pre-training process to better incorporate previously unlearned domain-specific knowledge of XML-SQL code. Our experimental results show that our trained model XSQLT5-base (220M) improves accuracy by 13.8\% compared to the similarly sized CodeT5-base (220M). Additionally, our experiments reveal that ChatGPT, due to its inability to fully learn the XML-SQL domain knowledge, achieves a much lower generation accuracy even with few-shot samples compared to our XSQLT5-base (220M) model.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
keywords = {XML, Pre-training, Large Language Models, Code Generation, Software Evolution}
}

@article{10.1145/3708520,
author = {Cederbladh, Johan and Cicchetti, Antonio and Jongeling, Robbert},
title = {A Road-Map to Readily Available Early Validation and Verification of System Behaviour in Model-Based Systems Engineering using Software Engineering Best Practices},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708520},
doi = {10.1145/3708520},
abstract = {In this article, we discuss how we can facilitate the growing need for early validation and verification (V&amp;V) of system behaviour in Model-Based Systems Engineering (MBSyE). Several aspects, such as reducing cost and time to market, push companies towards integration of V&amp;V methods earlier in development to support effective decision-making. One foundational methodology seeing increased attention in industry is the use of MBSyE, which brings benefits of models with well-defined syntax and semantics to support V&amp;V activities, rather than relying on natural language text documentation. Despite their promise, industrial adoption of these practices is still challenging.This article presents a vision for readily available early V&amp;V. We present a summary of the literature on early V&amp;V in MBSyE and position existing challenges regarding potential solutions and future investigations towards this vision. We elaborate our vision by means of challenges with a specific emphasis on early V&amp;V of system behaviour. We identify three specific challenge areas: Creating and managing Models, Organisational systems engineering aspects, and early V&amp;V Methods. Finally, we outline a road-map to address these categories of challenges, in which we propose the transfer of established best practices from the software engineering domain to support emerging technologies in the systems engineering domain.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {151},
numpages = {30},
keywords = {Validation, Verification, Models, Early, Systems, Behaviour}
}

@inproceedings{10.1145/3580252.3586978,
author = {Rahman, M Arif and Preum, Sarah Masud and Williams, Ronald D. and Alemzadeh, Homa and Stankovic, John},
title = {EMS-BERT: A Pre-Trained Language Representation Model for the Emergency Medical Services (EMS) Domain},
year = {2024},
isbn = {9798400701023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580252.3586978},
doi = {10.1145/3580252.3586978},
abstract = {Emergency Medical Services (EMS) is an important domain of healthcare. First responders save millions of lives per year. Machine learning and sensing technologies are actively being developed to support first responders in their EMS activities. However, there are significant challenges to overcome in developing these new solutions. One of the main challenges is the limitations of existing methods for EMS text mining, and developing a highly accurate language model for the EMS domain. Several important Bidirectional Encoder Representations from Transformer (BERT) models for medical domains, i.e., BioBERT and ClinicalBERT, have significantly influenced biomedical text mining tasks. But extracting information from the EMS domain is a separate challenge due to the uniqueness of the EMS domain, and the significant scarcity of a high-quality EMS corpus. In this research, we propose EMS-BERT - a BERT model specifically developed for EMS text-mining tasks. For data augmentation on our small, classified EMS corpus which consists of nearly 2.4M words, we use a simultaneous pre-training method for transfer-learning relevant information from medical, bio-medical, and clinical domains; and train a high-performance BERT model. Our thorough evaluation shows at least 2\% to as much as 11\% improvement of F-1 scores for EMS-BERT on different classification tasks, i.e., entity recognition, relation extraction, and inferring missing information when compared both with existing state-of-the-art clinical entity recognition tools, and with various medical BERT models.},
booktitle = {Proceedings of the 8th ACM/IEEE International Conference on Connected Health: Applications, Systems and Engineering Technologies},
pages = {34–43},
numpages = {10},
keywords = {emergency medical services (EMS) data processing and analysis, EMS entity recognition, medicine and health, language model},
location = {Orlando, FL, USA},
series = {CHASE '23}
}

@inproceedings{10.1145/3707292.3707358,
author = {Liu, Fei and Kang, Zejun and Han, Xing},
title = {Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama ModelsOptimizing RAG Techniques Based on Locally Deployed Ollama ModelsA Case Study with Locally Deployed Ollama Models},
year = {2025},
isbn = {9798400707308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707292.3707358},
doi = {10.1145/3707292.3707358},
abstract = {With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using locally deployed Ollama models.Based on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding pipelines and an agent supporting self-RAG based on LangGraph best practices.To evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive industry dataset.Our optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing information processing and intelligent production in the automotive industry.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Intelligent Information Processing},
pages = {152–159},
numpages = {8},
keywords = {Automotive Industry, Langchain, Ollama, PDF Processing, RAG, self-rag},
location = {
},
series = {AIIIP '24}
}

@article{10.1145/3710904,
author = {Wu, Shiwei and Wang, Mingxiang and Shi, Chuhan and Peng, Zhenhui},
title = {ComViewer: An Interactive Visual Tool to Help Viewers Seek Social Support in Online Mental Health Communities},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3710904},
doi = {10.1145/3710904},
abstract = {Online mental health communities (OMHCs) offer rich posts and comments for viewers, who do not directly participate in the communications, to seek social support from others' experience. However, viewers could face challenges in finding helpful posts and comments and digesting the content to get needed support, as revealed in our formative study (N=10). In this work, we present an interactive visual tool named ComViewer to help viewers seek social support in OMHCs. With ComViewer, viewers can filter posts of different topics and find supportive comments via a zoomable circle packing visual component that adapts to searched keywords. Powered by LLM, ComViewer supports an interactive sensemaking process by enabling viewers to interactively highlight, summarize, and question any community content. A within-subjects study (N=20) demonstrates ComViewer's strengths in providing viewers with a more simplified, more fruitful, and more engaging support-seeking experience compared to a baseline OMHC interface without ComViewer. We further discuss design implications for facilitating information-seeking and sense making in online mental health communities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW006},
numpages = {31},
keywords = {information seeking \&amp; search, mental health, online community, social support, visualization}
}

@inproceedings{10.1145/3691620.3695015,
author = {Lahiri, Sumit and Kalita, Pankaj Kumar and Chittora, Akshay Kumar and Vankudre, Varun and Roy, Subhajit},
title = {Program Synthesis Meets Visual What-Comes-Next Puzzles},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695015},
doi = {10.1145/3691620.3695015},
abstract = {What-Comes-Next (WCN) puzzles challenge us to identify the next figure that "logically follows" a provided sequence of figures. WCN puzzles are a favorite of interviewers and examiners---there is hardly any aptitude test that misses WCN puzzles. In this work, we propose to automatically synthesize WCN puzzles. The key insight to our methodology is that generation of WCN problems can be posed as a program synthesis problem. We design a small yet expressive language, PuzzlerLang, to capture solutions to WCN puzzles. PuzzlerLang is expressive enough to explain almost all human generated WCN puzzles that we collected, and yet, small enough to allow synthesis in a reasonable time. To ensure that the generated puzzles are appealing to humans, we infer a machine learning model to approximate the appeal factor of given WCN puzzle to humans. We use this model within our puzzle synthesizer as an optimization function to generate highly appealing and correct-by-construction WCN puzzles. We implemented our ideas in a tool, PuzzleGen; we found that PuzzleGen is fast, clocking an average time of about 3.4s per puzzle. Further, statistical tests over the responses from a user-study supported that the PuzzleGen generated puzzles were indistinguishable from puzzles created by humans.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {418–429},
numpages = {12},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3698385.3699876,
author = {Jiang, Baozheng and Zhang, Haoxiang and Li, Yanxia and Zhou, Hexiao and Xiao, Zexiao and He, Sijia and Qiu, Wenying and Li, You},
title = {A Practical Investigation of the Accuracy of Large Language Models in Various Industrial Application Scenarios},
year = {2024},
isbn = {9798400712975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698385.3699876},
doi = {10.1145/3698385.3699876},
abstract = {Large language models have revolutionized natural language processing, facilitating a wide range of industrial applications. However, their precision and reliability in executing specific tasks across various industries remain incompletely understood. This study investigates the performance of these models across eight industrial scenarios, including code generation, safety monitoring, industrial prototype design, industrial knowledge inquiry, etc. By assessing their accuracy in these domains, the research highlights their practical utility, inherent limitations, and pinpoints areas for enhancement to optimize their deployment in industrial settings.},
booktitle = {Proceedings of the First International Workshop on IoT Datasets for Multi-Modal Large Model},
pages = {44–49},
numpages = {6},
keywords = {Industrial Applications, Large Language Models, Model Accuracy Evaluation},
location = {Hangzhou, China},
series = {IOTMMIM '24}
}

@inproceedings{10.1145/3640543.3645203,
author = {Rao, Nikitha and Tsay, Jason and Kate, Kiran and Hellendoorn, Vincent and Hirzel, Martin},
title = {AI for Low-Code for AI},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645203},
doi = {10.1145/3640543.3645203},
abstract = {Low-code programming allows citizen developers to create programs with minimal coding effort, typically via visual (e.g. drag-and-drop) interfaces. In parallel, recent AI-powered tools such as Copilot and ChatGPT generate programs from natural language instructions. We argue that these modalities are complementary: tools like ChatGPT greatly reduce the need to memorize large APIs but still require their users to read (and modify) textual programs, whereas visual tools abstract away most or all program text but struggle to provide easy access to large APIs. At their intersection, we propose LowCoder, the first low-code tool for developing AI pipelines that supports both a visual programming interface (LowCoderVP) and an AI-powered natural language interface (LowCoderNL). We leverage this tool to provide some of the first insights into whether and how these two modalities help programmers by conducting a user study. We task 20 developers with varying levels of AI expertise with implementing four ML pipelines using LowCoder, replacing the LowCoderNL component with a simple keyword search in half the tasks. Overall, we find that LowCoder is especially useful for (i)&nbsp;Discoverability: using LowCoderNL, participants discovered new operators in 75\% of the tasks, compared to just 32.5\% and 27.5\% using web search or scrolling through options respectively in the keyword-search condition, and (ii)&nbsp;Iterative Composition: 82.5\% of tasks were successfully completed and many initial pipelines were further successfully improved. Qualitative analysis shows that AI helps users discover how to implement constructs when they know what to do, but still fails to support novices when they lack clarity on what they want to accomplish. Overall, our work highlights the benefits of combining the power of AI with low-code programming.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {837–852},
numpages = {16},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{10.1145/3691620.3695550,
author = {Xia, Jingtao and Liu, Junrui and Brown, Nicholas and Chen, Yanju and Feng, Yu},
title = {Refinement Types for Visualization},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695550},
doi = {10.1145/3691620.3695550},
abstract = {Visualizations have become crucial in the contemporary data-driven world as they aid in exploring, verifying, and sharing insights obtained from data. In this paper, we propose a new paradigm of visualization synthesis based on refinement types. Besides input-output examples, users can optionally use refinement-type annotations to constrain the range of valid values in the example visualization or to express complex interactions between different visual components. Our system's outputs include both data transformation and visualization programs that are consistent with refinement-type specifications. To mitigate the scalability challenge during the synthesis process, we introduce a new visualization synthesis algorithm that uses lightweight bidirectional type checking to prune the search space. As we demonstrate experimentally, this new synthesis algorithm results in significant speed-up compared to prior work.We have implemented the proposed approach in a tool called Calico and evaluated it on 40 visualization tasks collected from online forums and tutorials. Our experiments show that Calico can solve 98\% of these benchmarks and, among those benchmarks that can be solved, the desired visualization is among the top-1 output generated by Calico. Furthermore, Calico takes an average of 1.56 seconds to generate the visualization, which is 50 times faster than Viser, a state-of-the-art synthesizer for data visualization.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1871–1881},
numpages = {11},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3718958.3750537,
author = {Wang, Zhaodong and Lin, Samuel and Yan, Guanqing and Ghorbani, Soudeh and Yu, Minlan and Zhou, Jiawei and Hu, Nathan and Baruah, Lopa and Peters, Sam and Kamath, Srikanth and Yang, Jerry and Zhang, Ying},
title = {Intent-Driven Network Management with Multi-Agent LLMs: The Confucius Framework},
year = {2025},
isbn = {9798400715242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3718958.3750537},
doi = {10.1145/3718958.3750537},
abstract = {Advancements in Large Language Models (LLMs) are significantly transforming network management practices. In this paper, we present our experience developing Confucius, a multi-agent framework for network management at Meta. We model network management workflows as directed acyclic graphs (DAGs) to aid planning. Our framework integrates LLMs with existing management tools to achieve seamless operational integration, employs retrieval-augmented generation (RAG) to improve long-term memory, and establishes a set of primitives to systematically support human/model interaction. To ensure the accuracy of critical network operations, Confucius closely integrates with existing network validation methods and incorporates its own validation framework to prevent regressions. Remarkably, Confucius is a production-ready LLM development framework that has been operational for two years, with over 60 applications onboarded. To our knowledge, this is the first report on employing multi-agent LLMs for hyper-scale networks.},
booktitle = {Proceedings of the ACM SIGCOMM 2025 Conference},
pages = {347–362},
numpages = {16},
keywords = {large language models (LLMs), RAG, network planning},
location = {S\~{a}o Francisco Convent, Coimbra, Portugal},
series = {SIGCOMM '25}
}

@inproceedings{10.1145/3594806.3596542,
author = {Othman, Achraf and Dhouib, Amira and Nasser Al Jabor, Aljazi},
title = {Fostering websites accessibility: A case study on the use of the Large Language Models ChatGPT for automatic remediation},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3596542},
doi = {10.1145/3594806.3596542},
abstract = {The use of automated accessibility testing tools remains a common practice for evaluating web accessibility. However, the results obtained from these tools may not always provide a comprehensive and complete view of a site's accessibility status. The main purpose of this study is to improve web accessibility by automatically remediating non-accessible ones using Large Language Models (LLM), particularly ChatGPT. The effectiveness of the used model in detecting and remediating accessibility issues to ensure compliance with the Web Content Accessibility Guidelines (WCAG 2.1) is also discussed. By using ChatGPT as a remediation tool, this study investigates the potential of LLM in improving web accessibility. In the case study, two websites that did not adhere to the WCAG 2.1 guidelines were selected as the primary experimental subjects for the study. These websites were assessed using the web accessibility evaluation tool, WAVE, to detect accessibility issues. The identified issues served then as the basis for remediation using ChatGPT. The effectiveness of the used advanced language model as a web accessibility remediation tool was evaluated by comparing its findings with those obtained from manual accessibility testing. The results of this comparison have significant implications for stakeholders involved in achieving WCAG compliance and contribute to the development of more accessible online platforms for individuals with disabilities.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {707–713},
numpages = {7},
location = {Corfu, Greece},
series = {PETRA '23}
}

@article{10.1145/3708521,
author = {Li, Rui and Liu, Huai and Poon, Pak-Lok and Towey, Dave and Sun, Chang-Ai and Zheng, Zheng and Zhou, Zhi Quan and Chen, Tsong Yueh},
title = {Metamorphic Relation Generation: State of the Art and Research Directions},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708521},
doi = {10.1145/3708521},
abstract = {Metamorphic testing has become one mainstream technique to address the notorious oracle problem in software testing, thanks to its great successes in revealing real-life bugs in a wide variety of software systems. Metamorphic relations, the core component of metamorphic testing, have continuously attracted research interests from both academia and industry. In the last decade, a rapidly increasing number of studies have been conducted to systematically generate metamorphic relations from various sources and for different application domains. In this article, based on the systematic review on the state of the art for metamorphic relations’ generation, we summarize and highlight visions for further advancing the theory and techniques for identifying and constructing metamorphic relations and discuss promising research directions in related areas.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {149},
numpages = {25},
keywords = {Metamorphic testing, Metamorphic relation, Metamorphic relation generation}
}

@inproceedings{10.5555/3643142.3643417,
author = {Taylor, Simon J. E. and Macal, Charles M. and Matta, Andrea and Rabe, Markus and Sanchez, Susan M. and Shao, Guodong},
title = {Enhancing Digital Twins with Advances in Simulation and Artificial Intelligence: Opportunities and Challenges},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {Simulations are used to investigate physical systems. A digital twin goes beyond this by connecting a simulation with the physical system with the purpose of analyzing and controlling that system in real-time. In the past 5 years there has been a substantial increase in research into Simulation and Artificial Intelligence (AI). The combination of Simulation with AI presents many possible innovations. Similarly, combining AI with Simulation presents further possibilities including approaches to developing trustworthy and explainable AI methods, solutions to problems arising from sparce or no data and better methods for time series analysis. Given the progress that has been made in Digital Twins and Simulation and AI, what opportunities are there from combining these two exciting research areas? What challenges need to be overcome to achieve these? This article discusses these from the perspectives of six leading members of the Modeling \&amp; Simulation community.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3296–3310},
numpages = {15},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@inproceedings{10.1145/3639233.3639353,
author = {Maceda, Lany Laguna and Llovido, Jennifer Laraya and Artiaga, Miles Biago and Abisado, Mideth Balawiswis},
title = {Classifying Sentiments on Social Media Texts: A GPT-4 Preliminary Study},
year = {2024},
isbn = {9798400709227},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639233.3639353},
doi = {10.1145/3639233.3639353},
abstract = {In today's digital age, social media has become a hub for people to express their thoughts and feelings. Sentiment classification discerns public opinions and trends to understand their sentiments towards a certain topic. Often, achieving accurate sentiment classifications in large datasets necessitate the use of human-annotated training data which can be costly and time-consuming. Large Language Models (LLMs) like the Generative Pre-trained models by OpenAI have surged in popularity due to its capabilities in understanding the given tasks. In this preliminary study, we report the performance of the latest OpenAI GPT-4 using zero- and one-shot learning approaches on classifying sentiments when fed with social media dataset. Notably, the latter approach written in English which mimics the instructions designed for human annotators, achieved a substantial agreement (k = 0.77) with human annotations, displaying high accuracy, precision, and recall accordingly even without explicit training data. Meanwhile, the fine-tuned mBERT resulted to lower evaluation scores than the GPT-4. Our findings provide foundational insights into the strengths and limitations of GPT-4 for sentiment classification in a social media dataset, setting the groundwork for broad future research in this field.},
booktitle = {Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval},
pages = {19–24},
numpages = {6},
keywords = {GPT-4, LLM Prompting, Sentiment Annotation, Social Media Data},
location = {Seoul, Republic of Korea},
series = {NLPIR '23}
}

@article{10.1145/3729348,
author = {Tu, Zhi and Niu, Liangkun and Fan, Wei and Zhang, Tianyi},
title = {Multi-modal Traffic Scenario Generation for Autonomous Driving System Testing},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729348},
doi = {10.1145/3729348},
abstract = {Autonomous driving systems (ADS) require extensive testing and validation before deployment. However, it is tedious and time-consuming to construct traffic scenarios for ADS testing. In this paper, we propose TrafficComposer, a multi-modal traffic scenario construction approach for ADS testing. TrafficComposer takes as input a natural language (NL) description of a desired traffic scenario and a complementary traffic scene image. Then, it generates the corresponding traffic scenario in a simulator, such as CARLA and LGSVL. Specifically, TrafficComposer integrates high-level dynamic information about the traffic scenario from the NL description and intricate details about the surrounding vehicles, pedestrians, and the road network from the image. The information from the two modalities is complementary to each other and helps generate high-quality traffic scenarios for ADS testing. On a benchmark of 120 traffic scenarios, TrafficComposer achieves 97.0\% accuracy, outperforming the best-performing baseline by 7.3\%. Both direct testing and fuzz testing experiments on six ADSs prove the bug detection capabilities of the traffic scenarios generated by TrafficComposer. These scenarios can directly discover 37 bugs and help two fuzzing methods find 33\%–124\% more bugs serving as initial seeds.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE078},
numpages = {24},
keywords = {Autonomous Driving System, Software Testing, Traffic Scenario Generation}
}

@inproceedings{10.1145/3643661.3643951,
author = {Darnell, Benjamin and Chopra, Hetarth and Councilman, Aaron and Grove, David and Wang, Yu-Xiong and Adve, Vikram},
title = {An Empirical Comparison of Code Generation Approaches for Ansible},
year = {2024},
isbn = {9798400705649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643661.3643951},
doi = {10.1145/3643661.3643951},
abstract = {The rapid proliferation of LLM-based programming assistants has enabled fast and accurate automatic code generation for general purpose programming languages. Domain-specific languages like Ansible, a DSL for IT Automation, have seen a lack of support despite being critical to many fields, due to limited public-domain code for training models and a lack of interest from tool developers. To address this issue, we collect a novel dataset of permissively licensed Ansible code, and use it to create Warp, an LLM for code fine-tuned to produce Ansible tasks from a natural language prompt. We evaluate state-of-the-art tools for LLM-based code generation models, comparing multiple common strategies, including fine-tuning base models on Ansible code and retrieval-augmented-generation using documentation, in order to understand challenges with existing methodology and identify future research directions to enable better code generation for DSLs.},
booktitle = {Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {large language models, code generation, domain specific languages, ansible},
location = {Lisbon, Portugal},
series = {InteNSE '24}
}

@inproceedings{10.1145/3624007.3624053,
author = {Magalh\~{a}es, Jos\'{e} Wesley de Souza and Woodruff, Jackson and Polgreen, Elizabeth and O'Boyle, Michael F. P.},
title = {C2TACO: Lifting Tensor Code to TACO},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624007.3624053},
doi = {10.1145/3624007.3624053},
abstract = {Domain-specific languages (DSLs) promise a significant performance and portability advantage over traditional languages. DSLs are designed to be high-level and platform-independent, allowing an optimizing compiler significant leeway when targeting a particular device. Such languages are particularly popular with emerging tensor algebra workloads. However, DSLs present their own challenge: they require programmers to learn new programming languages and put in significant effort to migrate legacy code.    We present C2TACO, a synthesis tool for synthesizing TACO, a well-known tensor DSL, from C code. We develop a smart, enumerative synthesizer that uses automatically generated IO examples and source-code analysis to efficiently generate code. C2TACO is able to synthesize 95\% bench marks from a tensor benchmark suite, out-performing an alternative neural machine translation technique, and demonstrates substantially higher levels of accuracy when evaluated against two state-of-the-art existing schemes, TF-Coder and ChatGPT. Our synthesized TACO programs are, by design, portable achieving significant performance improvement when evaluated on a multi-core and GPU platform.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {42–56},
numpages = {15},
keywords = {Program Lifting, Synthesis, TACO, Tensor Algebra},
location = {Cascais, Portugal},
series = {GPCE 2023}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00008,
author = {Chaaben, Meriem Ben and Burgue\~{n}o, Lola and Sahraoui, Houari},
title = {Towards Using Few-Shot Prompt Learning for Automating Model Completion},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00008},
doi = {10.1109/ICSE-NIER58687.2023.00008},
abstract = {We propose a simple yet a novel approach to improve completion in domain modeling activities. Our approach exploits the power of large language models by using few-shot prompt learning without the need to train or fine-tune those models with large datasets that are scarce in this field. We implemented our approach and tested it on the completion of static and dynamic domain diagrams. Our initial evaluation shows that such an approach is effective and can be integrated in different ways during the modeling activities.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {7–12},
numpages = {6},
keywords = {language models, few-shot learning, prompt learning, domain modeling, model completion},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@article{10.1109/TCBB.2021.3135844,
author = {Sun, Yi and Wang, Jian and Lin, Hongfei and Zhang, Yijia and Yang, Zhihao},
title = {Knowledge Guided Attention and Graph Convolutional Networks for Chemical-Disease Relation Extraction},
year = {2022},
issue_date = {Jan.-Feb. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3135844},
doi = {10.1109/TCBB.2021.3135844},
abstract = {The automatic extraction of the chemical-disease relation (CDR) from the text becomes critical because it takes a lot of time and effort to extract valuable CDR manually. Studies have shown that prior knowledge from the biomedical knowledge base is important for relation extraction. The method of combining deep learning models with prior knowledge is worthy of our study. In this paper, we propose a new model called Knowledge Guided Attention and Graph Convolutional Networks (KGAGN) for CDR extraction. First, to make full advantage of domain knowledge, we train entity embedding as a feature representation of input sequence, and relation embedding to capture weighted contextual information further through the attention mechanism. Then, to make full advantage of syntactic dependency information in cross-sentence CDR extraction, we construct document-level syntactic dependency graphs and encode them using a graph convolution network (GCN). Finally, the chemical-induced disease (CID) relation is extracted by using weighted context features and long-range dependency features both of which contain additional knowledge information We evaluated our model on the CDR dataset published by the BioCreative-V community and achieves an F1-score of 73.3%, surpassing other state-of-the-art methods. the code implemented by PyTorch 1.7.0 deep learning library can be downloaded from Github: &lt;uri&gt;https://github.com/sunyi123/cdr&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {489–499},
numpages = {11}
}

@inproceedings{10.1145/3442442.3451375,
author = {Arslan, Yusuf and Allix, Kevin and Veiber, Lisa and Lothritz, Cedric and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Goujon, Anne},
title = {A Comparison of Pre-Trained Language Models for Multi-Class Text Classification in the Financial Domain},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3451375},
doi = {10.1145/3442442.3451375},
abstract = {Neural networks for language modeling have been proven effective on several sub-tasks of natural language processing. Training deep language models, however, is time-consuming and computationally intensive. Pre-trained language models such as BERT are thus appealing since (1) they yielded state-of-the-art performance, and (2) they offload practitioners from the burden of preparing the adequate resources (time, hardware, and data) to train models. Nevertheless, because pre-trained models are generic, they may underperform on specific domains. In this study, we investigate the case of multi-class text classification, a task that is relatively less studied in the literature evaluating pre-trained language models. Our work is further placed under the industrial settings of the financial domain. We thus leverage generic benchmark datasets from the literature and two proprietary datasets from our partners in the financial technological industry. After highlighting a challenge for generic pre-trained models (BERT, DistilBERT, RoBERTa, XLNet, XLM) to classify a portion of the financial document dataset, we investigate the intuition that a specialized pre-trained model for financial documents, such as FinBERT, should be leveraged. Nevertheless, our experiments show that the FinBERT model, even with an adapted vocabulary, does not lead to improvements compared to the generic BERT models.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {260–268},
numpages = {9},
keywords = {BERT, FinBERT, financial text classification},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3757914,
author = {Wang, Wansen and Tu, Caichang and Meng, Zhaoyi and Huang, Wenchao and Xiong, Yan},
title = {WACANA: A Concolic Analyzer for Detecting On-chain Data Vulnerabilities in WASM Smart Contracts},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3757914},
doi = {10.1145/3757914},
abstract = {WebAssembly (WASM) has emerged as a crucial technology in smart contract development for several blockchain platforms. Unfortunately, since their introduction, WASM smart contracts have been subject to several security incidents caused by contract vulnerabilities, resulting in substantial economic losses. However, existing tools for detecting WASM contract vulnerabilities have accuracy limitations, one of the main reasons being the coarse-grained emulation of the on-chain data APIs. In this paper, we introduce WACANA, an analyzer for WASM contracts that accurately detects vulnerabilities through fine-grained emulation of on-chain data APIs. WACANA precisely simulates both the structure of on-chain data tables and their corresponding API functions, and integrates concrete and symbolic execution within a coverage-guided loop to balance accuracy and efficiency. Evaluations on a vulnerability dataset of 2,012 contracts show WACANA outperforming state-of-the-art tools in accuracy. Further validation on 5,602 real-world contracts confirms WACANA’s practical effectiveness.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
keywords = {Smart Contracts, Vulnerability Detection, EOSIO Blockchain}
}

@inproceedings{10.1145/3672608.3707960,
author = {Malandri, Lorenzo and Mercorio, Fabio and Serino, Antonio},
title = {SkiLLMo: Normalized ESCO Skill Extraction through Transformer Models},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707960},
doi = {10.1145/3672608.3707960},
abstract = {In recent years, natural language processing (NLP) technologies have made a significant contribution in addressing a number of labour market tasks. One of the most interesting challenges is the automatic extraction of competences from unstructured texts.This paper presents a pipeline for efficiently extracting and standardizing skills from job advertisements using NLP techniques. The proposed methodology leverages open-source Transformer and Large Language Models to extract skills and map them to the European labour market taxonomy, ESCO.To address the computational challenges of processing lengthy job advertisements, a BERT model was fine-tuned to identify text segments likely containing skills. This filtering step reduces noise and ensures that only relevant content is processed further. The filtered text is then passed to an LLM, which extracts implicit and explicit hard and soft skills through prompt engineering. The extracted skills are subsequently matched with entries in a vector store containing the ESCO taxonomy to achieve standardization.Evaluation by domain experts shows that the pipeline achieves a precision of 91\% for skill extraction, 80\% for skill standardization and a combined overall precision of 79\%. These results demonstrate the effectiveness of the proposed approach in facilitating structured and standardized skill extraction from job postings.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1969–1978},
numpages = {10},
keywords = {skill extraction, large language models, transformer models, information extraction, labor market},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@inproceedings{10.1145/3736733.3736738,
author = {Ganti, Manasi and Zhang, Enhao and Balazinska, Magdalena},
title = {Bootstrapping Compositional Video Query Synthesis with Natural Language and Previous Queries from Users},
year = {2025},
isbn = {9798400719592},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3736733.3736738},
doi = {10.1145/3736733.3736738},
abstract = {With the emerging ubiquity of video data across diverse applications, the accessibility of video analytics is essential. To address this goal, some state-of-the-art systems synthesize declarative queries over video databases using example video fragments provided by the user. However, finding examples of what a user is looking for can still be tedious. This work presents POLY-VOCAL, a new system that eases this burden. POLY-VOCAL uses multiple forms of user input to bootstrap the synthesis of a new query, including textual descriptions of the user's search and previously synthesized queries. Our empirical evaluation demonstrates that POLY-VOCAL significantly improves accuracy and accelerates query convergence compared with query synthesis from only user-labeled examples, while lowering the effort required from users.},
booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
articleno = {8},
numpages = {7},
location = {Intercontinental Berlin, Berlin, Germany},
series = {HILDA '25}
}

@article{10.1145/3656420,
author = {Pelton, Blake and Sapek, Adam and Eguro, Ken and Lo, Daniel and Forin, Alessandro and Humphrey, Matt and Xi, Jinwen and Cox, David and Karandikar, Rajas and de Fine Licht, Johannes and Babin, Evgeny and Caulfield, Adrian and Burger, Doug},
title = {Wavefront Threading Enables Effective High-Level Synthesis},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {PLDI},
url = {https://doi.org/10.1145/3656420},
doi = {10.1145/3656420},
abstract = {Digital systems are growing in importance and computing hardware is growing more heterogeneous. Hardware design, however, remains laborious and expensive, in part due to the limitations of conventional hardware description languages (HDLs) like VHDL and Verilog. A longstanding research goal has been programming hardware like software, with high-level languages that can generate efficient hardware designs. This paper describes Kanagawa, a language that takes a new approach to combine the programmer productivity benefits of traditional High-Level Synthesis (HLS) approaches with the expressibility and hardware efficiency of Register-Transfer Level (RTL) design. The language’s concise syntax, matched with a hardware design-friendly execution model, permits a relatively simple toolchain to map high-level code into efficient hardware implementations.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {190},
numpages = {25},
keywords = {Hardware Threading, Wavefront Threading, Wavefront Consistency}
}

@article{10.1109/TASLP.2022.3198802,
author = {Zhang, Zhihao and Zuo, Yuan and Wu, Junjie},
title = {Aspect Sentiment Triplet Extraction: A Seq2Seq Approach With Span Copy Enhanced Dual Decoder},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3198802},
doi = {10.1109/TASLP.2022.3198802},
abstract = {Aspect Sentiment Triplet Extraction (ASTE) is a relatively new and very challenging task that attempts to provide an integral solution for aspect-based sentiment analysis. Aspect sentiment triplets in a sentence usually have overlaps when, e.g., one aspect is associated with multiple opinions and vice versa. Recently, end-to-end ASTE methods are becoming more and more popular for they can avoid the error propagation problem of pipeline-based methods. However, existing tagging-based end-to-end methods face difficulty to obtain a satisfactory recall, and generative methods fail to take a full account of the underlying interactions between aspects, opinions and their corresponding sentiments. In this paper, we formalize the ASTE task as a Seq2Seq learning problem with span copy mechanism for extracting multiple and possibly overlapped triplets. A novel dual decoder is devised purposefully for the ASTE task, where a multi-head attention based span copy mechanism is proposed to copy multi-token aspects and opinions. The dual decoder benefits from the rich output of encoder that can fuse multi-type information including word semantic, POS tag and BIO tag. Experiments on various benchmark datasets demonstrate that our approach achieves new state-of-the-art results. We also conduct analytical experiments to verify the effectiveness of various model components particularly for overlapped triplets extraction. We find that our model can be further improved through data augmentation and post-training.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2729–2742},
numpages = {14}
}

@article{10.1145/3673906,
author = {Qin, Ruiyang and Yang, Kai and Abbasi, Ahmed and Dobolyi, David and Seyedi, Salman and Griner, Emily and Kwon, Hyeokhyen and Cotes, Robert and Jiang, Zifan and Clifford, Gari and Cook, Ryan A.},
title = {Language Models for Online Depression Detection: A Review and Benchmark Analysis on Remote Interviews},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3673906},
doi = {10.1145/3673906},
abstract = {The use of machine learning (ML) to detect depression in online settings has emerged as an important health and wellness use case. In particular, the use of deep learning methods for depression detection from textual content posted on social media has garnered considerable attention. Conversely, there has been relatively limited evaluation of depression detection in clinical environments involving text generated from remote interviews. In this research, we review state-of-the-art feature-based ML, deep learning, and large language models for depression detection. We use a multidimensional analysis framework to benchmark various language models on a novel testbed comprising speech-to-text transcriptions of remote interviews. Our framework considers the impact of different transcription types and interview segments on depression detection performance. Finally, we summarize the key trends and takeaways from the review and benchmark evaluation and provide suggestions to guide the design of future detection methods.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {12},
numpages = {35},
keywords = {Deep learning, language models, depression detection, mental health, remote interviews}
}

@article{10.1145/3674971,
author = {Ellouze, Mourad and Hadrich Belguith, Lamia},
title = {Artificial Intelligence application for the analysis of personality traits and disorders in social media: A Survey},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3674971},
doi = {10.1145/3674971},
abstract = {Personality analysis has a positive influence on humanity as it aids in identifying personality traits and disorders. In addition, it facilitates the monitoring of cases and enriches doctors’ knowledge bases, particularly in decision-making processes. This study includes a comprehensive literature review on personality analysis approaches from social media, aiming to gain a thorough understanding of the current studies on personality therapy. Moreover, the objective of this study is to identify various limitations present in these studies and explore potential avenues for enhancement. More specifically, this research begins with an introduction that discusses the main concepts of traits and personality disorders, as well as the importance of psychological analysis. Following that, four cluster studies related to personality analysis on social media are presented: personality traits, personality disorders, detection of links between diseases, and monitoring patient status. Then, the majority of the currently available works for each cluster are exposed. Afterward, a comparative study of the different presented works is proposed. Finally, an outline of plans for further research in this area is provided, detailing potential paths for exploration.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
keywords = {Social Media, Personality Traits, Personality Disorders, Artificial Intelligence, Text Mining, Natural Language Processing}
}

@inproceedings{10.1145/3701716.3717851,
author = {Sobti, Divyam and Agumbe Suresh, Mahima and Tortora, Cristina and Viswanathan, Vimal},
title = {Domain-Specific Aspect Extraction for Product Design},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717851},
doi = {10.1145/3701716.3717851},
abstract = {As technology advances, computers become increasingly proficient at interpreting and translating human language into machine-understandable text. With the help of algorithms in natural language processing (NLP), machines can now translate textual data. These algorithms help identify and extract specific text components known as aspects. The aspects represent specific attributes or topics within textual data. For instance, if a product review states,'' This phone has good battery life but poor camera quality,'' and attributes like 'battery life' and 'camera quality' represent aspects in the text. Aspect extraction is a pivotal process involving identifying and isolating key features or topics within text. This research aims to compare and discuss the existing aspect extraction techniques. By effectively extracting the aspects, we will help designers gain the capability to understand and analyze sentiments, thereby enhancing their ability to derive meaningful insights from diverse textual data. Aspect-based sentiment analysis (ABSA) enables the extraction of sentiments towards specific aspects of a product the user provides. For example, when a person writes a review about a restaurant, sentiment analysis can determine whether the review is positive or negative. ABSA can separately determine the review's sentiment towards different aspects of a restaurant, such as, food quality, ambiance, etc. An important input to ABSA are the aspect keywords to find in the reviews. We propose an approach to extract features/aspects from customer-based product reviews. We experiment with different word embedding and clustering techniques to identify the best set of parameters for product design. Our experiments indicate that Global Vectors (GloVe) used on Gaussian Mixture Models (GMMs) yield the most insightful results for the specific problem domain.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2754–2763},
numpages = {10},
keywords = {clustering, mixture models, product design, word embeddings},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3665939.3665962,
author = {Beasley, Cole and Abouzied, Azza},
title = {Pipe(line) Dreams: Fully Automated End-to-End Analysis and Visualization},
year = {2024},
isbn = {9798400706936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665939.3665962},
doi = {10.1145/3665939.3665962},
abstract = {We exploit large language models (LLMs) to automate the end-to-end process of descriptive analytics and visualization. A user simply declares who they are and provides their data set. Our tool LLM4Vis sets analysis goals or metrics, generates code to process and analyze the data, visualizes the results and interprets the visualization to summarize key takeaways for our user. We examine the power of LLMs in democratizing data science for the non-technical user and in handling rich, multimodal data sets. We also explore LLM4Vis's limitations, opportunities for human-in-the-loop interventions, and challenges to measuring and improving the robustness and the utility of LLM-generated end-to-end data analysis pipelines.},
booktitle = {Proceedings of the 2024 Workshop on Human-In-the-Loop Data Analytics},
pages = {1–7},
numpages = {7},
location = {Santiago, AA, Chile},
series = {HILDA  24}
}

@inproceedings{10.1145/3689096.3689458,
author = {Gautam, Sushant and Stor\r{a}s, Andrea M. and Midoglu, Cise and Hicks, Steven A. and Thambawita, Vajira and Halvorsen, P\r{a}l and Riegler, Michael A.},
title = {Kvasir-VQA: A Text-Image Pair GI Tract Dataset},
year = {2024},
isbn = {9798400712074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689096.3689458},
doi = {10.1145/3689096.3689458},
abstract = {We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and Kvasir-Instrument datasets, augmented with question- and-answer annotations to facilitate advanced machine learning tasks in Gastrointestinal (GI) diagnostics. This dataset comprises 6,500 annotated images spanning various GI tract conditions and surgical instruments, and it supports multiple question types including yes/no, choice, location, and numerical count. The dataset is intended for applications such as image captioning, Visual Question Answering (VQA), text-based generation of synthetic medical images, object detection, and classification. Our experiments demonstrate the dataset's effectiveness in training models for three selected tasks, showcasing significant applications in medical image analysis and diagnostics. We also present evaluation metrics for each task, highlighting the usability and versatility of our dataset. The dataset and supporting artifacts are available at https://datasets.simula.no/kvasir-vqa.},
booktitle = {Proceedings of the First International Workshop on Vision-Language Models for Biomedical Applications},
pages = {3–12},
numpages = {10},
keywords = {gastrointestinal diagnostics, machine learning in healthcare, medical image analysis, medical image captioning, visual question answering (vqa)},
location = {Melbourne VIC, Australia},
series = {VLM4Bio'24}
}

@inproceedings{10.1145/3677779.3677791,
author = {Xiao, Weizhen},
title = {A Comparative Review of Advanced Techniques for Financial Sentiment Analysis},
year = {2024},
isbn = {9798400709760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677779.3677791},
doi = {10.1145/3677779.3677791},
abstract = {Financial sentiment analysis, the task of discerning market sentiment from financial texts, plays a crucial role in investment decisions, risk assessment, and understanding economic trends. Traditional sentiment analysis techniques have often faced limitations in handling the complexities and nuances of financial language. The advent of large language models (LLMs) has brought a paradigm shift in this field. With their remarkable ability to process and understand natural language, LLMs are enabling new approaches that increase the accuracy and sophistication of financial sentiment analysis. This paper provides a comparative overview of cutting-edge LLM-based techniques for financial sentiment analysis. We introduce a six-pronged classification framework covering data types, sentiment granularity, model architectures, training approaches, methodological focus, and evaluation metrics. This framework aims to provide a structured perspective for understanding recent research trends. Our analysis reveals several key developments in the field. We discuss the challenges and opportunities associated with advanced techniques, like Instruction-tuning approaches and Retrieval-augmented methods. While LLMs offer clear advantages, ensuring data quality, mitigating bias, enhancing model explainability, and scaling these models to real-world applications remain active research areas. This review offers investors and financial researchers a comprehensive guide to the evolving landscape of financial sentiment analysis, facilitating well-informed choices for different use cases and laying the groundwork for future research.},
booktitle = {Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning},
pages = {76–80},
numpages = {5},
location = {Xi'an, China},
series = {CMNM '24}
}

@inproceedings{10.1145/3650212.3652112,
author = {Ma, Yunlong and Tian, Wentong and Gao, Xiang and Sun, Hailong and Li, Li},
title = {API Misuse Detection via Probabilistic Graphical Model},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652112},
doi = {10.1145/3650212.3652112},
abstract = {API misuses can cause a range of issues in software development, including program crashes, bugs, and vulnerabilities. Different approaches have been developed to automatically detect API misuses by checking the program against usage rules extracted from extensive codebase or API documents. However, these mined rules may not be precise or complete, leading to high false positive/negative rates. In this paper, we propose a novel solution to this problem by representing the mined API usage rules as a probabilistic graphical model, where each rule's probability value represents its trustworthiness of being correct.   Our approach automatically constructs probabilistic usage rules by mining codebase and documents, and aggregating knowledge from different sources.   Here, the usage rules obtained from the codebase initialize the probabilistic model, while the knowledge from the documents serves as a supplement for adjusting and complementing the probabilities accordingly.   We evaluate our approach on the MuBench benchmark.   Experimental results show that our approach achieves 42.0\% precision and 54.5\% recall, significantly outperforming state-of-the-art approaches.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {88–99},
numpages = {12},
keywords = {API misuse detection, Document Mining, Mining Software Repository, Probabilistic Graphical Model},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1109/ICSE55347.2025.00186,
author = {Fuch\ss{}, Dominik and Hey, Tobias and Keim, Jan and Liu, Haoyu and Ewald, Niklas and Thirolf, Tobias and Koziolek, Anne},
title = {LiSSA: Toward Generic Traceability Link Recovery through Retrieval-Augmented Generation},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00186},
doi = {10.1109/ICSE55347.2025.00186},
abstract = {There are a multitude of software artifacts which need to be handled during the development and maintenance of a software system. These artifacts interrelate in multiple, complex ways. Therefore, many software engineering tasks are enabled — and even empowered — by a clear understanding of artifact interrelationships and also by the continued advancement of techniques for automated artifact linking.However, current approaches in automatic Traceability Link Recovery (TLR) target mostly the links between specific sets of artifacts, such as those between requirements and code. Fortunately, recent advancements in Large Language Models (LLMs) can enable TLR approaches to achieve broad applicability. Still, it is a nontrivial problem how to provide the LLMs with the specific information needed to perform TLR.In this paper, we present LiSSA, a framework that harnesses LLM performance and enhances them through Retrieval-Augmented Generation (RAG). We empirically evaluate LiSSA on three different TLR tasks, requirements to code, documentation to code, and architecture documentation to architecture models, and we compare our approach to state-of-the-art approaches.Our results show that the RAG-based approach can significantly outperform the state-of-the-art on the code-related tasks. However, further research is required to improve the performance of RAG-based approaches to be applicable in practice.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1396–1408},
numpages = {13},
keywords = {traceability link recovery, large language models, retrieval-augmented generation},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@article{10.1145/3674735,
author = {Thangamani, Arun and Loechner, Vincent and Genaud, St\'{e}phane},
title = {A Survey of General-purpose Polyhedral Compilers},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3674735},
doi = {10.1145/3674735},
abstract = {Since the 1990s, many implementations of polyhedral compilers have been written and distributed, either as source-to-source translating compilers or integrated into wider-purpose compilers. This article provides a survey on those various available implementations as of today, 2024.First, we list and describe most commonly available polyhedral schedulers and compiler implementations. Then, we compare the general-purpose polyhedral compilers using two main criteria—robustness and performance—on the PolyBench/C set of benchmarks.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {72},
numpages = {26},
keywords = {Polyhedral compilation, benchmark, performance analysis}
}

@article{10.1145/3712302,
author = {Hosseini, Elahe and Srinivas, Abhinav and Nazari, Najmeh and Hale, Charity and Rafatirad, Setareh and Homayoun, Houman},
title = {Large Language Models for Opioid-Induced Respiratory Depression Prediction in Hospitalized Patients: A Retrospective Study},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3712302},
doi = {10.1145/3712302},
abstract = {Opioid-induced adverse events pose significant risks to hospitalized patients. However, there is a limited understanding of which patients on general care floors are at risk for Opioid-Induced Respiratory Depression (OIRD). This study aims to bridge that knowledge gap by utilizing the advancements of AI to interpret Electronic Medical Records (EMRs). Recently, Large Language Models (LLMs) have gained attention for their exceptional capabilities in understanding human language, which makes them crucial for AI systems in healthcare that focus on clinical narratives. In this study, we extracted 2,663 hospitalized adult patient records from UC Davis Medical Center archives between January 2010 and April 2020 to identify patients at high risk of OIRD. For this purpose, we employed clinical language models (BioBERT, ClinicalBERT, and GatorTron) and fine-tuned them on the OIRD dataset. Additionally, we leveraged the capabilities of GPT-4, a state-of-the-art LLM, to select the most informative risk factors and enhance the accuracy of the predictive models.},
journal = {ACM Trans. Comput. Healthcare},
month = apr,
articleno = {27},
numpages = {14},
keywords = {Opioid-Induced Respiratory Depression (OIRD), Large Language Models (LLMs)}
}

@inproceedings{10.1145/3613904.3641910,
author = {Wu, Zihan and Ericson, Barbara J.},
title = {SQL Puzzles: Evaluating Micro Parsons Problems With Different Feedbacks as Practice for Novices},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641910},
doi = {10.1145/3613904.3641910},
abstract = {This paper investigates using micro Parsons problems as a novel practice approach for learning Structured Query Language (SQL). In micro Parsons problems learners arrange predefined code fragments to form a SQL statement instead of typing the code. SQL is a standard language for working with relational databases. Targeting beginner-level SQL statements, we evaluated the efficacy of micro Parsons problems with block-based feedback and execution-based feedback compared to traditional text-entry problems. To delve into learners’ experiences and preferences for the three problem types, we conducted a within-subjects think-aloud study with 12 participants. We found that learners reported very different preferences. Factors they considered included perceived learning, task authenticity, and prior knowledge. Next, we conducted two between-subjects classroom studies to evaluate the effectiveness of micro Parsons problems with different feedback types versus text-entry problems for SQL practice. We found that learners who practiced by solving Parsons problems with block-based feedback had a significantly higher learning gain than those who practiced with traditional text-entry problems.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {930},
numpages = {15},
keywords = {SQL education, empirical study, learning, programming puzzle},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3638530.3664157,
author = {Coombe, Cameron and Howard, David and Browne, Will},
title = {Learning Classifier Systems as a Solver for the Abstraction and Reasoning Corpus},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664157},
doi = {10.1145/3638530.3664157},
abstract = {The abstraction and reasoning corpus (ARC) is a challenging AI benchmark as it requires models to learn unseen relationships from a few data points. Each puzzle only contains 2--5 training examples, which makes it hard for models that require training on large datasets. Models which do not require training on large datasets like learning classifier systems (LCSs) have the potential to solve this kind of complex, low-data problem due to their flexible representation, niche-based learning, and ability to generalise. Whilst some learners that can operate on low data have been applied to ARC, LCS-based architectures remain entirely unexplored. We propose a simple LCS architecture employing a windowing approach. This architecture solves 19 of 400 test grids (4.75\%) in the ARC training set, which shows promise by outperforming other na\"{\i}ve approaches. Additionally, the system uses minimal prior knowledge to achieve this result, bringing it closer to the original vision of an ARC solver, which is to only rely on a core set of concepts. We provide directions on how this basic model could be expanded upon to include more complex structures making use of LCSs' ability to integrate many diverse kinds of representations.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1770–1778},
numpages = {9},
keywords = {learning classifier systems, abstraction, abstraction and reasoning corpus},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3611643.3616317,
author = {Du, Xueying and Lou, Yiling and Liu, Mingwei and Peng, Xin and Yang, Tianyong},
title = {KG4CraSolver: Recommending Crash Solutions via Knowledge Graph},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616317},
doi = {10.1145/3611643.3616317},
abstract = {Fixing crashes is challenging, and developers often discuss their encountered crashes and refer to similar crashes and solutions on online Q&amp;A forums (e.g., Stack Overflow). However, a crash often involves very complex contexts, which includes different contextual elements, e.g., purposes, environments, code, and crash traces. Existing crash solution recommendation or general solution  recommendation techniques only use an incomplete context or treat the entire context as pure texts to search relevant solutions for a given crash, resulting in inaccurate recommendation results. In this work, we propose a novel crash solution knowledge graph (KG) to summarize the complete crash context and its solution with a graph-structured representation. To construct the crash solution KG automatically, we propose to leverage prompt learning to construct the KG from SO threads with a small set of labeled data. Based on the constructed KG, we further propose a novel KG-based crash solution recommendation technique KG4CraSolver, which precisely finds the relevant SO thread for an encountered crash by finely analyzing and matching the complete crash context based on the crash  solution KG. The evaluation results show that the constructed KG is of high quality and KG4CraSolver outperforms baselines in terms of all metrics (e.g., 13.4\%-113.4\% MRR improvements). Moreover, we perform a user study and find that KG4CraSolver helps participants find crash solutions 34.4\% faster and 63.3\% more accurately.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1242–1254},
numpages = {13},
keywords = {Crash Solution Recommendation, Knowledge Graph, Stack Overflow},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3652620.3687778,
author = {Charles, Joel and Michael, Judith and Netz, Lukas and Rumpe, Bernhard},
title = {Teaching Model-Driven Low-Code Development Platforms},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687778},
doi = {10.1145/3652620.3687778},
abstract = {We present and evaluate a method called grammar masking, which is used to guide large language models (LLMs) toward producing syntactically correct models for a given context-free grammar. Prompt engineering methods such as few-shot learning or priming can be used to improve the chances of an LLM producing correct syntax, but the more complex the grammar, the more time-consuming and less promising these methods become. Previous work is focused primarily on the usage of either language model training or prompt engineering. In this work, a method is presented that restricts the output to a given grammar using constrained decoding to ensure the output adheres to a valid syntax. We use several domain-specific languages (DSLs) built with MontiCore and task multiple LLMs to produce models with and without constrained decoding. A corresponding parser is used to confirm the syntactic correctness of each model. We show that grammar masking can dramatically improve the modeling capabilities of several LLMs, reducing the need for well-refined prompting while increasing the chance of producing correct models.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {570–577},
numpages = {8},
keywords = {LLM, MDSE, guidance, CFG, constrained decoding},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3649329.3663500,
author = {Masouros, Dimosthenis and Ferikoglou, Aggelos and Zervakis, Georgios and Xydis, Sotirios and Soudris, Dimitrios},
title = {Late Breaking Results: Language-level QoR modeling for High-Level Synthesis},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3663500},
doi = {10.1145/3649329.3663500},
abstract = {This paper proposes a language-level modeling approach for HighLevel Synthesis based on the state-of-the-art Transformer architecture. Our approach estimates the performance and required resources of HLS applications directly from the source code when different synthesis directives, in terms of HLS #pragmas, are applied. Results show that the proposed architecture achieves 96.02\% accuracy for predicting the feasibility class of applications and an average of 0.95 and 0.91 R2 scores for predicting the actual performance and required resources, respectively.},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {351},
numpages = {2},
location = {San Francisco, CA, USA},
series = {DAC '24}
}

@article{10.1145/3728908,
author = {Lin, Li and Zhu, Qinglin and Chen, Hongqiao and Wang, Zhuangda and Wu, Rongxin and Xie, Xiaoheng},
title = {QTRAN: Extending Metamorphic-Oracle Based Logical Bug Detection Techniques for Multiple-DBMS Dialect Support},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728908},
doi = {10.1145/3728908},
abstract = {Metamorphic testing is a widely used method to detect logical bugs in Database Management Systems (DBMSs), referred to herein as MOLT (Metamorphic-Oracle based Logical Bug Detection Technique). This technique involves constructing SQL statement pairs, including original and mutated queries, and assessing whether the execution results conform to predefined metamorphic relations to detect logical bugs. However, current MOLTs rely heavily on specific DBMS grammar to generate valid SQL statement pairs, which makes it challenging to adapt these techniques to various DBMSs with different grammatical structures. As a result, only a few popular DBMSs, such as PostgreSQL, MySQL, and MariaDB, are supported by existing MOLTs, with extensive manual effort required to expand to other DBMSs. Given that many DBMSs remain inadequately tested, there is a pressing need for a method that enables effortless extension of MOLTs across diverse DBMSs.    In this paper, we propose QTRAN, a novel LLM-powered approach that automatically extends existing MOLTs to various DBMSs. Our key insight is to translate SQL statement pairs to target DBMSs for metamorphic testing from existing MOLTs using LLMs. To address the challenges of LLMs’ limited understanding of dialect differences and metamorphic mechanisms, we propose a two-phase approach comprising the transfer and mutation phases. QTRAN tackles these challenges by drawing inspiration from the developer’s process of creating a MOLT, which includes understanding the grammar of the target DBMS to generate original queries and employing a mutator for customized mutations. The transfer phase is designed to identify potential dialects and leverage information from SQL documents to enhance query retrieval, enabling LLMs to translate original queries across different DBMSs accurately. During the mutation phase, we gather SQL statement pairs from existing MOLTs to fine-tune the pretrained model, tailoring it specifically for mutation tasks. Then we employ the customized LLM to mutate the translated original queries, preserving the defined relationships necessary for metamorphic testing.    We implement our approach as a tool and apply it to extend four state-of-the-art MOLTs for eight DBMSs: MySQL, MariaDB, TiDB, PostgreSQL, SQLite, MonetDB, DuckDB, and ClickHouse. The evaluation results show that over 99\% of the SQL statement pairs transferred by QTRAN satisfy the metamorphic relations required for testing. Furthermore, we have detected 24 logical bugs among these DBMSs, with 16 confirmed as unique and previously unknown bugs. We believe that the generality of QTRAN can significantly enhance the reliability of DBMSs.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA033},
numpages = {22},
keywords = {Database Testing, Metamorphic Testing, SQL Dialects}
}

@article{10.1145/3563327,
author = {Bavishi, Rohan and Joshi, Harshit and Cambronero, Jos\'{e} and Fariha, Anna and Gulwani, Sumit and Le, Vu and Radi\v{c}ek, Ivan and Tiwari, Ashish},
title = {Neurosymbolic repair for low-code formula languages},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563327},
doi = {10.1145/3563327},
abstract = {Most users of low-code platforms, such as Excel and PowerApps, write programs in domain-specific formula languages to carry out nontrivial tasks. Often users can write most of the program they want, but introduce small mistakes that yield broken formulas. These mistakes, which can be both syntactic and semantic, are hard for low-code users to identify and fix, even though they can be resolved with just a few edits. We formalize the problem of producing such edits as the last-mile repair problem. To address this problem, we developed LaMirage, a LAst-MIle RepAir-engine GEnerator that combines symbolic and neural techniques to perform last-mile repair in low-code formula languages. LaMirage takes a grammar and a set of domain-specific constraints/rules, which jointly approximate the target language, and uses these to generate a repair engine that can fix formulas in that language. To tackle the challenges of localizing errors and ranking candidate repairs, LaMirage leverages neural techniques, whereas it relies on symbolic methods to generate candidate edits. This combination allows LaMirage to find repairs that satisfy the provided grammar and constraints, and then pick the most natural repair. We compare LaMirage to state-of-the-art neural and symbolic approaches on 400 real Excel and Power Fx formulas, where LaMirage outperforms all baselines. We release these benchmarks to encourage subsequent work in low-code domains.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {164},
numpages = {30},
keywords = {Low-Code, Neurosymbolic, Program Repair}
}

@inproceedings{10.1145/3706598.3714239,
author = {Zhou, Yunfan and Cai, Xiwen and Shi, Qiming and Huang, Yanwei and Li, Haotian and Qu, Huamin and Weng, Di and Wu, Yingcai},
title = {Xavier: Toward Better Coding Assistance in Authoring Tabular Data Wrangling Scripts},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714239},
doi = {10.1145/3706598.3714239},
abstract = {Data analysts frequently employ code completion tools in writing custom scripts to tackle complex tabular data wrangling tasks. However, existing tools do not sufficiently link the data contexts such as schemas and values with the code being edited. This not only leads to poor code suggestions, but also frequent interruptions in coding processes as users need additional code to locate and understand relevant data. We introduce Xavier, a tool designed to enhance data wrangling script authoring in computational notebooks. Xavier maintains users’ awareness of data contexts while providing data-aware code suggestions. It automatically highlights the most relevant data based on the user’s code, integrates both code and data contexts for more accurate suggestions, and instantly previews data transformation results for easy verification. To evaluate the effectiveness and usability of Xavier, we conducted a user study with 16 data analysts, showing its potential to streamline data wrangling scripts authoring.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {850},
numpages = {16},
keywords = {Interactive data wrangling, coding assistance},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3615318.3615329,
author = {Adam, Julien and Besnard, Jean-Baptiste and Canat, Paul and Taboada, Hugo and Roussel, Adrien and P\'{e}rache, Marc and Jaeger, Julien and Shende, Sameer},
title = {Generating and Scaling a Multi-Language Test-Suite for MPI},
year = {2023},
isbn = {9798400709135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615318.3615329},
doi = {10.1145/3615318.3615329},
abstract = {High-Performance Computing (HPC) is currently facing significant challenges. The hardware pressure has become increasingly difficult to manage due to the lack of parallel abstractions in applications. As a result, parallel programs must undergo drastic evolution to effectively exploit underlying hardware parallelism. Failure to do so results in inefficient code. In this constrained environment, parallel runtimes play a critical role, and their testing becomes crucial. This paper focuses on the MPI interface and leverages the MPI binding tools to develop a multi-language test suite for MPI. By doing so and building on previous work from the Forum document editors, we implement a systematic testing of MPI symbols in the context of the Parallel Computing Validation System (PCVS), which is an HPC validation platform dedicated to running and managing test suites at scale. We first describe PCVS, then outline the process of generating the MPI API test suite, and finally, run these tests at scale. All data sets, code generators, and implementations are made available in open-source to the community. We also set up a dedicated website showcasing the results, which self-updates thanks to the Spack package manager.},
booktitle = {Proceedings of the 30th European MPI Users' Group Meeting},
articleno = {11},
numpages = {10},
keywords = {HPC, MPI, api, test, validation},
location = {Bristol, United Kingdom},
series = {EuroMPI '23}
}

@article{10.14778/3746405.3746426,
author = {Shankar, Shreya and Chambers, Tristan and Shah, Tarak and Parameswaran, Aditya G. and Wu, Eugene},
title = {DocETL: Agentic Query Rewriting and Evaluation for Complex Document Processing},
year = {2025},
issue_date = {May 2025},
publisher = {VLDB Endowment},
volume = {18},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3746405.3746426},
doi = {10.14778/3746405.3746426},
abstract = {Analyzing unstructured data has been a persistent challenge in data processing. Recent proposals offer declarative frameworks for LLM-powered processing of unstructured data, but they typically execute user-specified operations as-is in a single LLM call—focusing on cost rather than accuracy. This is problematic for complex tasks, where even well-prompted LLMs can miss relevant information. For instance, reliably extracting all instances of a specific clause from legal documents often requires decomposing the task, the data, or both.We present DocETL, a system that optimizes complex document processing pipelines, while accounting for LLM shortcomings. DocETL offers a declarative interface for users to deine such pipelines and uses an agent-based approach to automatically optimize them, leveraging novel agent-based rewrites (that we call rewrite directives), as well as an optimization and evaluation framework. We introduce (i) logical rewriting of pipelines, tailored for LLM-based tasks, (ii) an agent-guided plan evaluation mechanism, and (iii) an optimization algorithm that efficiently finds promising plans, considering the latencies of LLM execution. Across four real-world document processing tasks, DocETL improves accuracy by 21–80\% over strong baselines. DocETL is open-source at docetl.org and, as of March 2025, has over 1.7k GitHub stars across diverse domains.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {3035–3048},
numpages = {14}
}

@inproceedings{10.1145/3689492.3690054,
author = {Marron, Mark},
title = {A Programming Language for Data and Configuration!},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689492.3690054},
doi = {10.1145/3689492.3690054},
abstract = {A day in the life of a developer often involves more time working with schemas, configurations, and data description systems than writing code and logic in a classical programming language. As more systems move into distributed worlds, e.g. cloud and microservices, and developers make increasing use of libraries and frameworks, the need to interact with a range of data formats and configuration mechanisms is only increasing. This is a treacherous world, where a misspelled property name or missing field can render an entire service inoperable, a mistake that a number in an API represents     seconds instead of milli-seconds can lead to a message being set for delivery in several months instead of in an hour, misconfigured schema can lead to public exposure of sensitive data, and corrupt or erroneous results from a misunderstood data format could result in massive financial and/or reputational damage.        To address these challenges this paper casts the problems of data and configuration descriptions, not as a problem of data representation, but as a type system problem, that can be addressed with well understood and highly effective programming language techniques! The novel challenge is that data representation and configuration are universal concerns in a system and, particularly in modern cloud or micro-service systems, these systems may involve many programming languages. In the past this has led to specification systems that use a least-common-denominator set of data types, often little more than strings and numbers, and then rely on conventions or (out-of-date) documentation to ensure that the data is interpreted correctly. This paper shows that, with careful design, it is possible to create a rich universal system that can be used to express data and configuration specifications in a way that is human readable/writable and that can be produced/consumed, much like JSON, by a wide range of programming languages and systems.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {147–161},
numpages = {15},
keywords = {Configuration, Data Specification, Programming Language},
location = {Pasadena, CA, USA},
series = {Onward! '24}
}

@inproceedings{10.1145/3706598.3713708,
author = {Gebreegziabher, Simret Araya and Yang, Yukun and Glassman, Elena L. and Li, Toby Jia-Jun},
title = {Supporting Co-Adaptive Machine Teaching through Human Concept Learning and Cognitive Theories},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713708},
doi = {10.1145/3706598.3713708},
abstract = {An important challenge in interactive machine learning, particularly in subjective or ambiguous domains, is fostering bi-directional alignment between humans and models. Users teach models their concept definition through data labeling, while refining their own understandings throughout the process. To facilitate this, we introduce Mocha, an interactive machine learning tool informed by two theories of human concept learning and cognition. First, it utilizes a neuro-symbolic pipeline to support Variation Theory-based counterfactual data generation. By asking users to annotate counterexamples that are syntactically and semantically similar to already-annotated data but predicted to have different labels, the system can learn more effectively while helping users understand the model and reflect on their own label definitions. Second, Mocha uses Structural Alignment Theory to present groups of counterexamples, helping users comprehend alignable differences between data items and annotate them in batch. We validated Mocha’s effectiveness and usability through a lab study with 18 participants.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {533},
numpages = {18},
keywords = {human-AI collaboration, machine teaching, variation theory, structural alignment theory},
location = {
},
series = {CHI '25}
}

@article{10.1145/3729316,
author = {Ferreira, Margarida and Nicolet, Victor and Dodds, Joey and Kroening, Daniel},
title = {Program Synthesis from Partial Traces},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {PLDI},
url = {https://doi.org/10.1145/3729316},
doi = {10.1145/3729316},
abstract = {We present the first technique to synthesize programs that compose side-effecting functions, pure functions, and control flow, from partial traces containing records of only the side-effecting functions. This technique can be applied to synthesize API composing scripts from logs of calls made to those APIs, or a script from traces of system calls made by a workload, for example. All of the provided traces are positive examples, meaning that they describe desired behavior. Our approach does not require negative examples. Instead, it generalizes over the examples and uses cost metrics to prevent over-generalization. Because the problem is too complex for traditional monolithic program synthesis techniques, we propose a new combination of optimizing rewrites and syntax-guided program synthesis. The resulting program is correct by construction, so its output will always be able to reproduce the input traces.   We evaluate the quality of the programs synthesized when considering various optimization metrics and the synthesizer's efficiency on real-world benchmarks. The results show that our approach can generate useful real-world programs.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {213},
numpages = {24},
keywords = {Logic and verification, Theory of computation → Automated reasoning}
}

@article{10.1145/3764593,
author = {Kumar, Smitha S and Lones, Michael and Maarek, Manuel and Zantout, Hind},
title = {Navigating the landscape of automated feedback generation techniques for programming exercises},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3764593},
doi = {10.1145/3764593},
abstract = {Programming demands a variety of cognitive skills, and mastering these competencies is essential for success in computer science education. The importance of formative feedback is well acknowledged in programming education, and thus a diverse range of techniques has been proposed to generate and enhance formative feedback for programming exercises. This paper reviews state-of-the-art automated feedback generation techniques and categorizes the various approaches based on the underlying computational techniques, programming languages, the kind of programming errors they deal with, and the type of feedback they provide. It covers data-driven techniques, those which use program repair methods, machine learning-based techniques, and techniques based around the use of large language models, particularly noting the rapid uptake of the latter. The paper provides a summary of key findings and challenges, alongside recommendations for future work. The findings reveal that although there exist numerous tools for automated programming feedback, many studies depend on non-public benchmarks, which limits reproducibility and independent evaluation of the tools and their datasets. Additionally, tools are not always language-agnostic, and in some cases involve complex configuration steps. Large language models have demonstrated transformative potential in generating feedback. However, most research has focused on introductory courses (CS1 and CS2) indicating the need to apply them in advanced fields like machine learning and image processing. Although large language models have outperformed traditional approaches, challenges related to hallucinations and incorrect responses still need to be addressed as precision is critical in a pedagogical setting. Most of the studies use proprietary models that lack transparency and customization options, emphasizing the need for further research into open LLM alternatives.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Educ.},
month = sep,
keywords = {literature review, automated feedback, hints, learning programming, programming languages}
}

@article{10.1145/3673226,
author = {Uhrmacher, Adelinde M and Frazier, Peter and H\"{a}hnle, Reiner and Kl\"{u}gl, Franziska and Lorig, Fabian and Lud\"{a}scher, Bertram and Nenzi, Laura and Ruiz-Martin, Cristina and Rumpe, Bernhard and Szabo, Claudia and Wainer, Gabriel and Wilsdorf, Pia},
title = {Context, Composition, Automation, and Communication: The C2AC Roadmap for Modeling and Simulation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3673226},
doi = {10.1145/3673226},
abstract = {Simulation has become, in many application areas, a sine qua non. Most recently, COVID-19 has underlined the importance of simulation studies and limitations in current practices and methods. We identify four goals of methodological work for addressing these limitations. The first is to provide better support for capturing, representing, and evaluating the context of simulation studies, including research questions, assumptions, requirements, and activities contributing to a simulation study. In addition, the composition of simulation models and other simulation studies’ products must be supported beyond syntactical coherence, including aspects of semantics and purpose, enabling their effective reuse. A higher degree of automating simulation studies will contribute to more systematic, standardized simulation studies and their efficiency. Finally, it is essential to invest increased effort into effectively communicating results and the processes involved in simulation studies to enable their use in research and decision making. These goals are not pursued independently of each other, but they will benefit from and sometimes even rely on advances in other sub-fields. In this article, we explore the basis and interdependencies evident in current research and practice and delineate future research directions based on these considerations.},
journal = {ACM Trans. Model. Comput. Simul.},
month = aug,
articleno = {23},
numpages = {51},
keywords = {Modeling, simulation, state of the art, open challenges, reuse, composition, communication, reproducibility, automation, intelligent modeling and simulation lifecycle}
}

@article{10.1145/3649593,
author = {Chen, Zhifei and Chen, Lin and Yang, Yibiao and Feng, Qiong and Li, Xuansong and Song, Wei},
title = {Risky Dynamic Typing-related Practices in Python: An Empirical Study},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3649593},
doi = {10.1145/3649593},
abstract = {Python’s dynamic typing nature provides developers with powerful programming abstractions. However, many type-related bugs are accumulated in code bases of Python due to the misuse of dynamic typing. The goal of this article is to aid in the understanding of developers’ high-risk practices toward dynamic typing and the early detection of type-related bugs. We first formulate the rules of six types of risky dynamic typing-related practices (type smells for short) in Python. We then develop a rule-based tool named RUPOR, which builds an accurate type base to detect type smells. Our evaluation shows that RUPOR outperforms the existing type smell detection techniques (including the Large Language Models–based approaches, Mypy, and PYDYPE) on a benchmark of 900 Python methods. Based on RUPOR, we conduct an empirical study on 25 real-world projects. We find that type smells are significantly related to the occurrence of post-release faults. The fault-proneness prediction model built with type smell features slightly outperforms the model built without them. We also summarize the common patterns, including inserting type check to fix type smell bugs. These findings provide valuable insights for preventing and fixing type-related bugs in the programs written in dynamic-typed languages.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {140},
numpages = {35},
keywords = {Dynamic typing, Python, empirical study, bug fixing}
}

@article{10.1145/3706119,
author = {Fatemi, Sorouralsadat and Hu, Yuheng and Mousavi, Maryam},
title = {A Comparative Analysis of Instruction Fine-Tuning Large Language Models for Financial Text Classification},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3706119},
doi = {10.1145/3706119},
abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities across diverse Natural Language Processing (NLP) tasks, including language understanding, reasoning, and generation. However, general-domain LLMs often struggle with financial tasks due to the technical and specialized nature of financial texts. This study investigates the efficacy of instruction fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini, to enhance their performance in financial text classification tasks. We fine-tuned both instruction-tuned and base models across four financial classification tasks, achieving significant improvements in task-specific performance. Furthermore, we evaluated the zero-shot capabilities of these fine-tuned models on three unseen complex financial tasks, including argument classification, deal completeness classification, and causal classification. Our results indicate while base model fine-tuning led to greater degradation, instruction-tuned models maintained more robust performance. To address this degradation, we employed model merging techniques, integrating single-task domain-specific fine-tuned models with the base model. Using this merging method resulted in significant enhancements in zero-shot performance, even exceeding the original model’s accuracy on certain datasets. Our findings underscore the effectiveness of instruction fine-tuning and model merging for adapting LLMs to specialized financial text classification tasks.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {6},
numpages = {30},
keywords = {Large language models, parameter-efficient fine-tuning, instruction fine-tuning, text classification, finance}
}

@article{10.1145/3657299,
author = {Xia, Bolun (Namir) and Rawte, Vipula and Gupta, Aparna and Zaki, Mohammed},
title = {FETILDA: Evaluation Framework for Effective Representations of Long Financial Documents},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {7},
issn = {1556-4681},
url = {https://doi.org/10.1145/3657299},
doi = {10.1145/3657299},
abstract = {In the financial sphere, there is a wealth of accumulated unstructured financial data, such as the textual disclosure documents that companies submit on a regular basis to regulatory agencies, such as the Securities and Exchange Commission. These documents are typically very long and tend to contain valuable soft information about a company’s performance that is not present in quantitative predictors. It is therefore of great interest to learn predictive models from these long textual documents, especially for forecasting numerical key performance indicators. In recent years, there has been great progress in natural language processing via pre-trained language models (LMs) learned from large corpora of textual data. This prompts the important question of whether they can be used effectively to produce representations for long documents, as well as how we can evaluate the effectiveness of representations produced by various LMs. Our work focuses on answering this critical question, namely, the evaluation of the efficacy of various LMs in extracting useful soft information from long textual documents for prediction tasks. In this article, we propose and implement a deep learning evaluation framework that utilizes a sequential chunking approach combined with an attention mechanism. We perform an extensive set of experiments on a collection of 10-K reports submitted annually by U.S. banks, and another dataset of reports submitted by U.S. companies, to investigate thoroughly the performance of different types of language models. Overall, our framework using LMs outperforms strong baseline methods for textual modeling as well as for numerical regression. Our work provides better insights into how utilizing pre-trained domain-specific and fine-tuned long-input LMs for representing long documents can improve the quality of representation of textual data and, therefore, help in improving predictive analyses.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {182},
numpages = {27},
keywords = {Text regression, language models, long text documents, financial documents, 10-K reports}
}

@inproceedings{10.1145/3524610.3527894,
author = {Yang, Shouliang and Gu, Xiaodong and Shen, Beijun},
title = {Self-supervised learning of smart contract representations},
year = {2022},
isbn = {9781450392983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524610.3527894},
doi = {10.1145/3524610.3527894},
abstract = {Learning smart contract representations can greatly facilitate the development of smart contracts in many tasks such as bug detection and clone detection. Existing approaches for learning program representations are difficult to apply to smart contracts which have insufficient data and significant homogenization. To overcome these challenges, in this paper, we propose SRCL, a novel, self-supervised approach for learning smart contract representations. Unlike existing supervised methods, which are tied on task-specific data labels, SRCL leverages large-scale unlabeled data by self-supervised learning of both local and global information of smart contracts. It automatically extracts structural sequences from abstract syntax trees (ASTs). Then, two discriminators are designed to guide the Transformer encoder to learn local and global semantic features of smart contracts. We evaluate SRCL on a dataset of 75,006 smart contracts collected from Etherscan. Experimental results show that SRCL considerably outperforms the state-of-the-art code representation models on three downstream tasks.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension},
pages = {82–93},
numpages = {12},
keywords = {code representation learning, data augmentation, self-supervised learning, smart contract},
location = {Virtual Event},
series = {ICPC '22}
}

@article{10.1145/3735499,
author = {Ousmer, Mehdi and Vanderdonckt, Jean and Bilius, Laura-Bianca and Vatavu, Radu-Daniel and Terenti, Mihail},
title = {Paired Sketching of Distributed User Interfaces: Workflow, Protocol, Software Support, and Experiment},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3735499},
doi = {10.1145/3735499},
abstract = {The evolving landscape of distributed user interfaces requires the prototyping stage also be distributed between users, tasks, platforms, and environments. To create a cohesive distribution of the user interface elements in such ecosystems, paired sketching has emerged as a collaborative design method that leverages multiple stakeholders’ strengths, including designers, developers, and end users, working in pairs. In the context of developer experience applied to paired sketching for distributed user interfaces, we decomposed a workflow into four disciplines according to the Software and Systems Process Engineering Meta-Model (SPEM) notation. First, we defined a protocol to deploy paired sketching of distributed user interfaces, supported by UbiSketch, a collaborative software environment tailored featuring sketch recognition and whiteboarding. Second, to evaluate paired sketching for engineering interactive systems, we conducted an experiment involving five pairs of stakeholders who sketched a distributed user interface for inside-the-vehicule interaction distributed on four platforms: smartphone, tablet, pen display, and tabletop. Empirical results from questionnaires, reactivity, intention, perceived satisfaction, and free comments, suggest a preference order in which the tabletop is ranked first, followed by the tablet, smartphone, and pen display. Based on these results, we discuss the potential of paired sketching for distributed user interfaces.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {EICS018},
numpages = {31},
keywords = {Distributed user interfaces, Multi-platform user interfaces, Paired sketching}
}

@inproceedings{10.1145/3477495.3532077,
author = {Ren, Zhaochun and Tian, Zhi and Li, Dongdong and Ren, Pengjie and Yang, Liu and Xin, Xin and Liang, Huasheng and de Rijke, Maarten and Chen, Zhumin},
title = {Variational Reasoning about User Preferences for Conversational Recommendation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532077},
doi = {10.1145/3477495.3532077},
abstract = {Conversational recommender systems (CRSs) provide recommendations through interactive conversations. CRSs typically provide recommendations through relatively straightforward interactions, where the system continuously inquires about a user's explicit attribute-aware preferences and then decides which items to recommend. In addition, topic tracking is often used to provide naturally sounding responses. However, merely tracking topics is not enough to recognize a user's real preferences in a dialogue.In this paper, we address the problem of accurately recognizing and maintaining user preferences in CRSs. Three challenges come with this problem: (1) An ongoing dialogue only provides the user's short-term feedback; (2) Annotations of user preferences are not available; and (3) There may be complex semantic correlations among items that feature in a dialogue. We tackle these challenges by proposing an end-to-end variational reasoning approach to the task of conversational recommendation. We model both long-term preferences and short-term preferences as latent variables with topical priors for explicit long-term and short-term preference exploration, respectively. We use an efficient stochastic gradient variational Bayesian (SGVB) estimator for optimizing the derived evidence lower bound. A policy network is then used to predict topics for a clarification utterance or items for a recommendation response. The use of explicit sequences of preferences with multi-hop reasoning in a heterogeneous knowledge graph helps to provide more accurate conversational recommendation results.Extensive experiments conducted on two benchmark datasets show that our proposed method outperforms state-of-the-art baselines in terms of both objective and subjective evaluation metric},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {165–175},
numpages = {11},
keywords = {conversational recommendation, task-oriented dialogue systems, user preference tracking, variational inference},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1109/SC41406.2024.00038,
author = {Herten, Andreas and Achilles, Sebastian and Alvarez, Damian and Badwaik, Jayesh and Behle, Eric and Bode, Mathis and Breuer, Thomas and Caviedes-Voulli\`{e}me, Daniel and Cherti, Mehdi and Dabah, Adel and El Sayed, Salem and Frings, Wolfgang and Gonzalez-Nicolas, Ana and Gregory, Eric B. and Mood, Kaveh Haghighi and Hater, Thorsten and Jitsev, Jenia and John, Chelsea Maria and Meinke, Jan H. and Meyer, Catrin I. and Mezentsev, Pavel and Mirus, Jan-Oliver and Nassyr, Stepan and Penke, Carolin and R\"{o}mmer, Manoel and Sinha, Ujjwal and von St. Vieth, Benedikt and Stein, Olaf and Suarez, Estela and Willsch, Dennis and Zhukov, Ilya},
title = {Application-Driven Exascale: The JUPITER Benchmark Suite},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00038},
doi = {10.1109/SC41406.2024.00038},
abstract = {Benchmarks are essential in the design of modern HPC installations, as they define key aspects of system components. Beyond synthetic workloads, it is crucial to include real applications that represent user requirements into benchmark suites, to guarantee high usability and widespread adoption of a new system. Given the significant investments in leadership-class supercomputers of the exascale era, this is even more important and necessitates alignment with a vision of Open Science and reproducibility. In this work, we present the JUPITER Benchmark Suite, which incorporates 16 applications from various domains. It was designed for and used in the procurement of JUPITER, the first European exascale supercomputer. We identify requirements and challenges and outline the project and software infrastructure setup. We provide descriptions and scalability studies of selected applications and a set of key takeaways. The JUPITER Benchmark Suite is released as open source software with this work at github.com/FZJ-JSC/jubench.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {32},
numpages = {45},
keywords = {Accelerator, Benchmark, Exascale, GPU, Procurement, System Architecture, System Design},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.1109/ICSE55347.2025.00257,
author = {Rinard, Martin C.},
title = {Research in Program Repair and Approximate Computing: A Retrospective},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00257},
doi = {10.1109/ICSE55347.2025.00257},
abstract = {This paper and accompanying talk trace the trajectory of my research in program repair and approximate computing. The prevailing value system in the field at the time focused on program correctness as a fundamental goal. This research, in contrast, was driven by a new perspective that emphasized acceptable (but not necessarily fully correct) survival through errors and the automatic identification and exploitation of performance versus accuracy tradeoff spaces implicitly present in computations coded to operate at only a single point in this space.Because the research challenged the prevailing value system at the time, it met with some skepticism despite empirical results highlighting its effectiveness. The following quote from an anonymous reviewer may give some idea of the reaction:"The basic idea—to assist incorrect programs in their efforts to emit incorrect output—is an abomination and if adopted would likely usher in a new dark age."As the research progressed, we gained a deeper understanding of the reasons behind the surprising — at least to us — phenomena we observed. We were able to formalize this understanding to generate source code patches and obtain performance, accuracy, and acceptability guarantees for computations that leveraged our techniques, bringing the research full circle to once again focus on reasoning statically about program behavior but with different reasoning techniques and guarantees.Finally, I discuss lessons learned and future relevance of the principles, perspectives, and concepts that this research pioneered.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1–15},
numpages = {15},
keywords = {program repair, approximate computing},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@article{10.1145/3725352,
author = {Zhang, Enhao and Sullivan, Nicole and Haynes, Brandon and Krishna, Ranjay and Balazinska, Magdalena},
title = {Self-Enhancing Video Data Management System for Compositional Events with Large Language Models},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3725352},
doi = {10.1145/3725352},
abstract = {Complex video queries can be answered by decomposing them into modular subtasks. However, existing video data management systems assume the existence of predefined modules for each subtask. We introduce VOCAL-UDF, a novel self-enhancing system that supports compositional queries over videos without the need for predefined modules. VOCAL-UDF automatically identifies and constructs missing modules and encapsulates them as user-defined functions (UDFs), thus expanding its querying capabilities. To achieve this, we formulate a unified UDF model that leverages large language models (LLMs) to aid in new UDF generation. VOCAL UDF handles a wide range of concepts by supporting both program-based UDFs (i.e., Python functions generated by LLMs) and distilled-model UDFs (lightweight vision models distilled from strong pretrained models). To resolve the inherent ambiguity in user intent, VOCAL-UDF generates multiple candidate UDFs and uses active learning to efficiently select the best one. With the self-enhancing capability, VOCAL-UDF significantly improves query performance across three video datasets.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {215},
numpages = {29},
keywords = {compositional queries, knowledge distillation, large language models, program generation, scene graphs, video analytics, vision-language models}
}

@inproceedings{10.1145/3691620.3695520,
author = {Tang, Shuncheng and Zhang, Zhenya and Zhou, Jixiang and Lei, Lei and Zhou, Yuan and Xue, Yinxing},
title = {LeGEND: A Top-Down Approach to Scenario Generation of Autonomous Driving Systems Assisted by Large Language Models},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695520},
doi = {10.1145/3691620.3695520},
abstract = {Autonomous driving systems (ADS) are safety-critical and require comprehensive testing before their deployment on public roads. While existing testing approaches primarily aim at the criticality of scenarios, they often overlook the diversity of the generated scenarios that is also important to reflect system defects in different aspects. To bridge the gap, we propose LeGEND, that features a top-down fashion of scenario generation: it starts with abstract functional scenarios, and then steps downwards to logical and concrete scenarios, such that scenario diversity can be controlled at the functional level. However, unlike logical scenarios that can be formally described, functional scenarios are often documented in natural languages (e.g., accident reports) and thus cannot be precisely parsed and processed by computers. To tackle that issue, LeGEND leverages the recent advances of large language models (LLMs) to transform textual functional scenarios to formal logical scenarios. To mitigate the distraction of useless information in functional scenario description, we devise a two-phase transformation that features the use of an intermediate language; consequently, we adopt two LLMs in LeGEND, one for extracting information from functional scenarios, the other for converting the extracted information to formal logical scenarios. We experimentally evaluate LeGEND on Apollo, an industry-grade ADS from Baidu. Evaluation results show that LeGEND can effectively identify critical scenarios, and compared to baseline approaches, LeGEND exhibits evident superiority in diversity of generated scenarios. Moreover, we also demonstrate the advantages of our two-phase transformation framework, and the accuracy of the adopted LLMs.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1497–1508},
numpages = {12},
keywords = {autonomous driving systems, critical scenario generation, large language models},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3758326,
author = {Aguzzi, Gianluca and Farabegoli, Nicolas and Viroli, Mirko},
title = {A Language-based Approach to Macroprogramming for IoT Systems through Large Language Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3758326},
doi = {10.1145/3758326},
abstract = {Large language models (LLMs) have transformed software engineering, particularly in code generation, where they assist developers in writing functions or entire programs. However, code generation remains challenging when the target domain is complex, as is the case with Internet of Things (IoT) systems. The challenge lies in capturing the entire system behavior within a single specification. developers often model only a subset of the system’s functionality, focusing primarily on individual device behavior or data processing aspects, which may not address the core challenges of IoT, such as large-scale distributed coordination and emergent behavior. To address this, macroprogramming paradigms have been proposed as a means to specify the collective behavior of IoT systems more holistically. Among these approaches, aggregate computing stands out for its ability to express system-wide properties through a top-down, global-to-local perspective. Despite its potential, the adoption of aggregate computing remains limited due to the complexity of writing and maintaining such programs. To overcome these barriers, we propose a language-based approach based on macroprogramming that leverages LLMs for IoT code generation. Specifically, we employ the in-context learning capabilities of LLMs, guiding them to generate code based on an aggregate computing abstraction. This creates code that reflects system-wide properties and frees programmers from writing low-level code by letting them specify desired global properties in natural language. The LLM then translates these specifications into executable code, thus facilitating the development of collective intelligence applications in IoT systems.},
note = {Just Accepted},
journal = {ACM Trans. Internet Things},
month = aug,
keywords = {Large Language Models, Macroprogramming, Internet of Things, Aggregate Computing, Code Generation}
}

@inproceedings{10.1145/3485447.3511941,
author = {Islam, Sk Mainul and Bhattacharya, Sourangshu},
title = {AR-BERT: Aspect-relation enhanced Aspect-level Sentiment Classification with Multi-modal Explanations},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511941},
doi = {10.1145/3485447.3511941},
abstract = {Aspect level sentiment classification (ALSC) is a difficult problem with state-of-the-art models showing less than 80\% macro-F1 score on benchmark datasets. Existing models do not incorporate information on aspect-aspect relations in knowledge graphs (KGs), e.g. DBpedia. Two main challenges stem from inaccurate disambiguation of aspects to KG entities, and the inability to learn aspect representations from the large KGs in joint training with ALSC models. We propose AR-BERT, a novel two-level global-local entity embedding scheme that allows efficient joint training of KG-based aspect embeddings and ALSC models. A novel incorrect disambiguation detection technique addresses the problem of inaccuracy in aspect disambiguation. We also introduce the problem of determining mode significance in multi-modal explanation generation, and propose a two step solution. The proposed methods show a consistent improvement of 2.5 − 4.1 percentage points, over the recent BERT-based baselines on benchmark datasets.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {987–998},
numpages = {12},
keywords = {Explainable Deep Learning, Knowledge Graph Embedding, Sentiment Analysis},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3719160.3736638,
author = {Mishra, Anchit and Schneider, Oliver},
title = {TacTalk: Personalizing Haptics Through Conversation},
year = {2025},
isbn = {9798400715273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719160.3736638},
doi = {10.1145/3719160.3736638},
abstract = {Haptic experiences are highly personal, but despite prior work exploring interfaces enabling personalization, we don’t know what process drives the personalization of haptics. To enable a study of this process, including users’ mental models and vocabularies, we introduce TacTalk, a conversational system enabling real time tuning of virtual haptic experiences. We present an application using TacTalk in a popular racing video game, Forza Horizon 5. Through an empirical study, we find that tracking user preference profiles may improve TacTalk’s ability to cater to individual differences, and that TacTalk is more usable than an existing slider-based personalization tool. A thematic analysis of participant interviews reveals an archetypal process of conversational personalization - starting with real-world experiences and domain-specific metaphors, then subsequently inspecting specific aspects of the experience including in-game events and the game controller.},
booktitle = {Proceedings of the 7th ACM Conference on Conversational User Interfaces},
articleno = {50},
numpages = {19},
keywords = {Haptics, Mental Models, Vocabulary, Personalization, Conversational Interfaces},
location = {
},
series = {CUI '25}
}

@inproceedings{10.1145/3696410.3714891,
author = {Gui, Yi and Wan, Yao and Li, Zhen and Zhang, Zhongyi and Chen, Dongping and Zhang, Hongyu and Su, Yi and Chen, Bohua and Zhou, Xing and Jiang, Wenbin and Zhang, Xiangliang},
title = {UICopilot: Automating UI Synthesis via Hierarchical Code Generation from Webpage Designs},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714891},
doi = {10.1145/3696410.3714891},
abstract = {Automating the synthesis of User Interfaces (UIs) plays a crucial role in enhancing productivity and accelerating the development lifecycle, reducing both development time and manual effort. Recently, the rapid development of Multimodal Large Language Models (MLLMs) has made it possible to generate front-end Hypertext Markup Language (HTML) code directly from webpage designs. However, real-world webpages encompass not only a diverse array of HTML tags but also complex stylesheets, resulting in significantly lengthy code. The lengthy code poses challenges for the performance and efficiency of MLLMs, especially in capturing the structural information of UI designs. To address these challenges, this paper proposes UICopilot, a novel approach to automating UI synthesis via hierarchical code generation from webpage designs. To validate the effectiveness of UICopilot, we conduct experiments on a real-world dataset, i.e., WebCode2M. Experimental results demonstrate that UICopilot significantly outperforms existing baselines in both automatic evaluation metrics and human evaluations. Specifically, statistical analysis reveals that the majority of human annotators prefer the webpages generated by UICopilot over those produced by GPT-4V.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {1846–1855},
numpages = {10},
keywords = {UI automation, UI synthesis, code generation, design to code},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3744660,
author = {Rodrigues, Tiago and Teixeira Lopes, Carla},
title = {Harnessing Large Language Models for Clinical Information Extraction: A Systematic Literature Review},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744660},
doi = {10.1145/3744660},
abstract = {Electronic Health Records store extensive patient health data, playing a crucial role in healthcare management. Extracting information from these text-heavy records is difficult due to their domain-specific vocabulary, which challenges applying general-domain techniques. Recent advancements in Large Language Models (LLMs) and an increasing interest in the field have sparked considerable progress in solving Clinical Information Extraction (IE) tasks. We review these applications in Clinical IE, highlighting the most common tasks, most successful methods, and most used datasets and evaluation criteria. Examining 85 studies, we synthesize and organize the current research trends, highlighting common points between papers. The presence of LLMs can be felt in the most common tasks, with novel approaches being attempted and showing promising results. However, breakthroughs are still necessary in designing reliable end-to-end systems that can perform all the Clinical IE tasks within a single system.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = jun,
keywords = {Information Extraction, Large Language Models, BERT, Clinical Notes, Electronic Health Records}
}

@inproceedings{10.1145/3677052.3698688,
author = {Fatemi, Sorouralsadat and Hu, Yuheng},
title = {FinVision: A Multi-Agent Framework for Stock Market Prediction},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698688},
doi = {10.1145/3677052.3698688},
abstract = {Financial trading has been a challenging task, as it requires the integration of vast amounts of data from various modalities. Traditional deep learning and reinforcement learning methods require large training data and often involve encoding various data types into numerical formats for model input, which limits the explainability of model behavior. Recently, LLM-based agents have demonstrated remarkable advancements in handling multi-modal data, enabling them to execute complex, multi-step decision-making tasks while providing insights into their thought processes. This research introduces a multi-modal multi-agent system designed specifically for financial trading tasks. Our framework employs a team of specialized LLM-based agents, each adept at processing and interpreting various forms of financial data, such as textual news reports, candlestick charts, and trading signal charts. A key feature of our approach is the integration of a reflection module, which conducts analyses of historical trading signals and their outcomes. This reflective process is instrumental in enhancing the decision-making capabilities of the system for future trading scenarios. Furthermore, the ablation studies indicate that the visual reflection module plays a crucial role in enhancing the decision-making capabilities of our framework.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {582–590},
numpages = {9},
keywords = {Large Language Models, Multi-Agent Framework},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@inproceedings{10.1145/3639477.3639732,
author = {Song, Yewei and Ezzini, Saad and Tang, Xunzhu and Lothritz, Cedric and Klein, Jacques and Bissyande, Tegawende and Boytsov, Andrey and Ble, Ulrick and Goujon, Anne},
title = {Enhancing Text-to-SQL Translation for Financial System Design},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639732},
doi = {10.1145/3639477.3639732},
abstract = {Text-to-SQL, the task of translating natural language questions into SQL queries, is part of various business processes. Its automation, which is an emerging challenge, will empower software practitioners to seamlessly interact with relational databases using natural language, thereby bridging the gap between business needs and software capabilities.In this paper, we consider Large Language Models (LLMs), which have achieved state of the art for various NLP tasks. Specifically, we benchmark Text-to-SQL performance, the evaluation methodologies, as well as input optimization (e.g., prompting). In light of the empirical observations that we have made, we propose two novel metrics that were designed to adequately measure the similarity between SQL queries.Overall, we share with the community various findings, notably on how to select the right LLM on Text-to-SQL tasks. We further demonstrate that a tree-based edit distance constitutes a reliable metric for assessing the similarity between generated SQL queries and the oracle for benchmarking Text2SQL approaches. This metric is important as it relieves researchers from the need to perform computationally expensive experiments such as executing generated queries as done in prior works. Our work implements financial domain use cases and, therefore contributes to the advancement of Text2SQL systems and their practical adoption in this domain.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {252–262},
numpages = {11},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3652620.3688197,
author = {Silva Mercado, Jonathan},
title = {AI Assisted Domain Modeling Explainability and Traceability},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688197},
doi = {10.1145/3652620.3688197},
abstract = {Domain Models are abstract representations of selected elements in a domain that is created in a collaborative process between domain and modeler experts. The participants share domain knowledge to conceptualize and reason about the elements that will create the domain models. Through this exchange, a comprehensive and accurate representation of the domain is achieved, ensuring that the model captures the relevant aspects and relationships in the domain. Research in Artificial Intelligence (AI) has explored various methods to assist in the creation of domain models from text using Natural Language Processing (NLP) and Machine Learning (ML). Recent advancements with Large Language Models (LLMs) have shown that it is possible to create domain models using prompting techniques; however, the generated domain models contain errors and remain constrained by the performance of the LLM used.Despite the impressive capabilities of LLMs to create domain models, it is evident that it does not address the needs of domain and modelers experts that participate in the creation of domain models. Every AI technique has its advantages and limitations that must be integrated with human feedback in a collaboration process. Therefore, we propose an approach that incorporates human-AI collaboration supported by AI assistants that follows a dialogue approach to understand the users needs and purpose to suggest relevant models. Our proposal combines symbolic and subsymbolic AI techniques with explainability and traceability of the decisions that assist to create domain models that are relevant for the users.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {130–135},
numpages = {6},
keywords = {domain modeling, large language models, uncertainty, explainability, traceability},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1109/ASE56229.2023.00117,
author = {Ramos, Daniel and Mitchell, Hailie and Lynce, In\^{e}s and Manquinho, Vasco and Martins, Ruben and Goues, Claire Le},
title = {MELT: Mining Effective Lightweight Transformations from Pull Requests},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00117},
doi = {10.1109/ASE56229.2023.00117},
abstract = {Software developers often struggle to update APIs, leading to manual, time-consuming, and error-prone processes. We introduce Melt, a new approach that generates lightweight API migration rules directly from pull requests in popular library repositories. Our key insight is that pull requests merged into open-source libraries are a rich source of information sufficient to mine API migration rules. By leveraging code examples mined from the library source and automatically generated code examples based on the pull requests, we infer transformation rules in Comby, a language for structural code search and replace. Since inferred rules from single code examples may be too specific, we propose a generalization procedure to make the rules more applicable to client projects. Melt rules are syntax-driven, interpretable, and easily adaptable. Moreover, unlike previous work, our approach enables rule inference to seamlessly integrate into the library workflow, removing the need to wait for client code migrations. We evaluated Melt on pull requests from four popular libraries, successfully mining 461 migration rules from code examples in pull requests and 114 rules from autogenerated code examples. Our generalization procedure increases the number of matches for mined rules by 9\texttimes{}. We applied these rules to client projects and ran their tests, which led to an overall decrease in the number of warnings and fixing some test cases demonstrating MELT's effectiveness in real-world scenarios.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1516–1528},
numpages = {13},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3726010.3726012,
author = {Liu, Fei and Ren, Huanhuan and Guan, Yu and Li, Na},
title = {Enhancing Automotive PDF Chatbots: A Graph RAG Approach with Custom Function Calling for Locally Deployed Ollama Models},
year = {2025},
isbn = {9798400712845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726010.3726012},
doi = {10.1145/3726010.3726012},
abstract = {This research explores state-of-the-art retrieval augmented generation (RAG) methodologies utilizing a locally hosted Ollama model to address the rising demand for streamlined offline PDF chatbots in the automotive industry. In this paper we present a Langchain-based optimization of the a forementioned framework for modeling all input text from automotive documents. For example, using JoinChpy which use specialized finetuning to a base model where it is fine-tuned adding specialized modifications, the first on PDF and retrieval algorithms as well as context compression strictly for automotive literature We define embedding pipeline classes and graph RAG agents.To instantiate our methodology, we developed our own automotive industry document dataset and compared the enhanced graph RAG and self-RAG agents with the original RAG baseline on our automotive dataset as well as QuAC and CANARD. The results demonstrate a consistent increase along the dimensions, in correctness, swiftness, relevance, and fidelity, specifically for automotive domain specific content. Hence offering a guideline on how to implement a local RAG in automotive setting, thus serving as a significant contribution to the research in an industrial context to process information and make a step towards smart manufacturing.},
booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence, Digital Media Technology and Interaction Design},
pages = {6–13},
numpages = {8},
keywords = {Automotive Industry, Graph RAG, Langchain, Ollama, PDF Processing},
location = {
},
series = {ICADI '24}
}

@inproceedings{10.1145/3620666.3651351,
author = {Guan, Yue and Yu, Changming and Zhou, Yangjie and Leng, Jingwen and Li, Chao and Guo, Minyi},
title = {Fractal: Joint Multi-Level Sparse Pattern Tuning of Accuracy and Performance for DNN Pruning},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620666.3651351},
doi = {10.1145/3620666.3651351},
abstract = {Model pruning, which eliminates redundant parameters and reduces computational complexity, emerges as a viable strategy for efficient deep neural network (DNN) deployment. Owing to the irregular memory access and computation patterns in the sparse DNN models after pruning, existing arts have suggested various structured sparse patterns to enhance sparse DNN performance. In this work, we propose a unique perspective of understanding existing sparse pattern design as computation-skipping after tiling the tensor computation into multi-level hierarchies. This unified perspective opens up a new design space of multi-level sparse tiling to maximize the sparsity benefits of DNNs, as opposed to the single-level choice in current practices. We present Fractal, an auto-tuning system for sparse patterns that identifies the optimal multi-level sparse tiling pattern. We introduce PatternIR, a novel high-level intermediate representation (IR), to express a diverse range of multi-level sparse patterns. By leveraging insights from prior dense operator optimizations, we translate PatternIR into low-level compiler IRs, facilitating further operator optimization and code generation. Our evaluations demonstrate that Fractal yields substantial speedups of up to on average 3.16\texttimes{} on CUDA Core, 2.52\texttimes{} on TensorCore of GPUs compared to the state-of-art dense baseline under 75\% sparsity while upholding minimal accuracy degradation compared to prior sparse operator libraries.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {416–430},
numpages = {15},
keywords = {structural pruning, sparse tensor compiler, sparse computation acceleration, deep learning},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3711896.3737233,
author = {Ghassel, Abdellah and Robinson, Ian and Tanase, Gabriel and Cooper, Hal and Thompson, Bryan and Han, Zhen and Ioannidis, Vassilis and Adeshina, Soji and Rangwala, Huzefa},
title = {Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737233},
doi = {10.1145/3711896.3737233},
abstract = {Retrieval-Augmented Generation (RAG) grounds large language models in external evidence, yet it still falters when answers must be pieced together across semantically distant documents. We close this gap with the Hierarchical Lexical Graph (HLG), a three-tier index that (i) traces every atomic proposition to its source(ii) clusters propositions into latent topics, and (iii) links entities and relations to expose cross-document paths. On top of HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG, which performs fine-grained entity-aware beam search over propositions for high-precision factoid questions, and TopicGraphRAG, which selects coarse topics before expanding along entity links to supply broad yet relevant context for exploratory queries. Additionally, existing benchmarks lack the complexity required to rigorously evaluate multi-hop summarization systems, often focusing on single-document queries or limited datasets. To address this, we introduce a synthetic dataset generation pipeline that curates realistic, multi-document question-answer pairs, enabling robust evaluation of multi-hop retrieval systems. Extensive experiments across five datasets demonstrate that our methods outperform naive chunk-based RAG, achieving an average relative improvement of 23.1\% in retrieval recall and correctness. Open-source Python library is available at https://github.com/awslabs/graphrag-toolkit.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {4457–4466},
numpages = {10},
keywords = {data generation, graph structures, question answering},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3701716.3715247,
author = {Tan, Xiaoyu and Li, Bin and Qiu, Xihe and Qu, Chao and Chu, Wei and Xu, Yinghui and Qi, Yuan},
title = {Meta-Agent-Workflow: Streamlining Tool Usage in LLMs through Workflow Construction, Retrieval, and Refinement},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715247},
doi = {10.1145/3701716.3715247},
abstract = {Large language models (LLMs) have recently shown significant advancements and are increasingly used as key components in automated agents for various web-based tasks. Typically, this agentization is achieved by carefully prompting LLMs to guide their behavior in using tools for specific tasks. However, this approach can be limited by the complexity of tasks and the inherent capabilities of LLMs. To enhance task-specific performance, a pre-defined workflow approach can be employed, reducing repetitive and error-prone planning for particular tasks. This workflow-driven process is especially well-suited for industrial applications, where task-specific agents can be easily configured using visual interfaces supported by various open-source platforms. In this paper, we introduce a novel framework called Meta-Agent-Workflowto create, retrieve, and refine agent workflows. Experiments on ToolBench demonstrate that our framework effectively transforms LLM tool-reasoning processes into task-specific workflows, retrieves workflows for different tasks based on various queries, and updates them based on execution feedback. We also open-source our code and follow the workflow architecture of an open-source agent platform (e.g., Dify) to facilitate further industrial and community use. The Meta-Agent-Workflow will be open-sourced in https://github.com/testlbin/meta_agent_workflows.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {458–467},
numpages = {10},
keywords = {agent, large language models, meta agent workflow, workflow},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3626246.3653389,
author = {de la R\'{u}a Mart\'{\i}nez, Javier and Buso, Fabio and Kouzoupis, Antonios and Ormenisan, Alexandru A. and Niazi, Salman and Bzhalava, Davit and Mak, Kenneth and Jouffrey, Victor and Ronstr\"{o}m, Mikael and Cunningham, Raymond and Zangis, Ralfs and Mukhedkar, Dhananjay and Khazanchi, Ayushman and Vlassov, Vladimir and Dowling, Jim},
title = {The Hopsworks Feature Store for Machine Learning},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3653389},
doi = {10.1145/3626246.3653389},
abstract = {Data management is the most challenging aspect of building Machine Learning (ML) systems. ML systems can read large volumes of historical data when training models, but inference workloads are more varied, depending on whether it is a batch or online ML system. The feature store for ML has recently emerged as a single data platform for managing ML data throughout the ML lifecycle, from feature engineering to model training to inference.  In this paper, we present the Hopsworks feature store for machine learning as a highly available platform for managing feature data with API support for columnar, row-oriented, and similarity search query workloads. We introduce and address challenges solved by the feature stores related to feature reuse, how to organize data transformations, and how to ensure correct and consistent data between feature engineering, model training, and model inference. We present the engineering challenges in building high-performance query services for a feature store and show how Hopsworks outperforms existing cloud feature stores for training and online inference query workloads.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {135–147},
numpages = {13},
keywords = {arrow flight, duckdb, feature store, mlops, rondb},
location = {Santiago AA, Chile},
series = {SIGMOD '24}
}

@article{10.1145/3689754,
author = {Bembenek, Aaron and Greenberg, Michael and Chong, Stephen},
title = {Making Formulog Fast: An Argument for Unconventional Datalog Evaluation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689754},
doi = {10.1145/3689754},
abstract = {With its combination of Datalog, SMT solving, and functional programming, the language Formulog provides an appealing mix of features for implementing SMT-based static analyses (e.g., refinement type checking, symbolic execution) in a natural, declarative way. At the same time, the performance of its custom Datalog solver can be an impediment to using Formulog beyond prototyping—a common problem for Datalog variants that aspire to solve large problem instances. In this work we speed up Formulog evaluation, with some surprising results: while 2.2\texttimes{} speedups can be obtained by using the conventional techniques for high-performance Datalog (e.g., compilation, specialized data structures), the big wins come by abandoning the central assumption in modern performant Datalog engines, semi-naive Datalog evaluation. In the place of semi-naive evaluation, we develop eager evaluation, a concurrent Datalog evaluation algorithm that explores the logical inference space via a depth-first traversal order. In practice, eager evaluation leads to an advantageous distribution of Formulog’s SMT workload to external SMT solvers and improved SMT solving times: our eager evaluation extensions to the Formulog interpreter and Souffl\'{e}’s code generator achieve mean 5.2\texttimes{} and 7.6\texttimes{} speedups, respectively, over the optimized code generated by off-the-shelf Souffl\'{e} on SMT-heavy Formulog benchmarks.                                All in all, using compilation and eager evaluation (as appropriate), Formulog implementations of refinement type checking, bottom-up pointer analysis, and symbolic execution achieve speedups on 20 out of 23 benchmarks over previously published, hand-tuned analyses written in F♯, Java, and C++, providing strong evidence that Formulog can be the basis of a realistic platform for SMT-based static analysis. Moreover, our experience adds nuance to the conventional wisdom that traditional semi-naive evaluation is the one-size-fits-all best Datalog evaluation algorithm for static analysis workloads.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {314},
numpages = {30},
keywords = {Datalog, Formulog, SMT solving, compilation, parallel evaluation}
}

@inproceedings{10.1145/3661167.3661233,
author = {Phan, Hung and Jannesari, Ali},
title = {Leveraging Statistical Machine Translation for Code Search},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661233},
doi = {10.1145/3661167.3661233},
abstract = {Machine Translation (MT) has numerous applications in Software Engineering (SE). Recently, it has been employed not only for programming language translation but also as an oracle for deriving information for various research problems in SE. In this application branch, MT’s impact has been assessed through metrics measuring the accuracy of these problems rather than traditional translation evaluation metrics. For code search, a recent work, ASTTrans, introduced an MT-based model for extracting relevant non-terminal nodes from the Abstract Syntax Tree (AST) of an implementation based on natural language descriptions. While ASTTrans demonstrated the effectiveness of MT in enhancing code search on small datasets with low embedding dimensions, it struggled to improve the accuracy of code search on the standard benchmark CodeSearchNet. In this work, we present Oracle4CS, a novel approach that integrates the classical MT model called Statistical Machine Translation to support modernized models for code search. To accomplish this, we introduce a new code representation technique called ASTSum, which summarizes each code snippet using a limited number of AST nodes. Additionally, we devise a fresh approach to code search, replacing natural language queries with a new representation that incorporates the results of our query-to-ASTSum translation process. Through experiments, we demonstrate that Oracle4CS can enhance code search performance on both the original BERT-based model UniXcoder and the optimized BERT-based model CoCoSoDa by up to 1.18\% and 2\% in Mean Reciprocal Rank (MRR) across eight selected well-known datasets. We also explore ASTSum as a promising code representation for supporting code search, potentially improving MRR by over 17\% on average when paired with an optimal SMT model for query-to-ASTSum translation.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {191–200},
numpages = {10},
keywords = {Abstract Syntax Tree, Information Retrieval, Statistical Machine Translation},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3613904.3642055,
author = {Yu, Zhengyan and Namkung, Hun and Guo, Jiang and Milner, Henry and Goldfoot, Joel and Wang, Yang and Sekar, Vyas},
title = {SEAM-EZ: Simplifying Stateful Analytics through Visual Programming},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642055},
doi = {10.1145/3613904.3642055},
abstract = {Across many domains (e.g., media/entertainment, mobile apps, finance, IoT, cybersecurity), there is a growing need for stateful analytics over streams of events to meet key business outcomes. Stateful analytics over event streams entails carefully modeling the sequence, timing, and contextual correlations of events to dynamic attributes. Unfortunately, existing frameworks and languages (e.g., SQL, Flink, Spark) entail significant code complexity and expert effort to express such stateful analytics because of their dynamic and stateful nature. Our overarching goal is to simplify and democratize stateful analytics. Through an iterative design and evaluation process including a foundational user study and two rounds of formative evaluations with 15 industry practitioners, we created SEAM-EZ, a no-code visual programming platform for quickly creating and validating stateful metrics. SEAM-EZ features a node-graph editor, interactive tooltips, embedded data views, and auto-suggestion features to facilitate the creation and validation of stateful analytics. We then conducted three real-world case studies of SEAM-EZ with 20 additional practitioners. Our results suggest that practitioners who previously could not or had to spend significant effort to create stateful metrics using traditional tools such as SQL or Spark can now easily and quickly create and validate such metrics using SEAM-EZ.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {1041},
numpages = {23},
keywords = {data analytics, metrics, stateful computation, visual programming},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1109/TASLP.2023.3250825,
author = {Zhang, Chen and D'Haro, Luis Fernando and Zhang, Qiquan and Friedrichs, Thomas and Li, Haizhou},
title = {PoE: A Panel of Experts for Generalized Automatic Dialogue Assessment},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3250825},
doi = {10.1109/TASLP.2023.3250825},
abstract = {Chatbots are expected to be knowledgeable across multiple domains, e.g. for daily chit-chat, exchange of information, and grounding in emotional situations. To effectively measure the quality of such conversational agents, a model-based automatic dialogue evaluation metric (ADEM) is expected to perform well across multiple domains. Despite significant progress, existing ADEMs tend to perform well only on data that are similar to its training data (overfit to its training domain). This calls for a domain-generalized metric that can assess dialogues of different characteristics. To this end, we propose a &lt;italic&gt;Panel of Experts&lt;/italic&gt; (PoE), a multitask network that consists of a shared transformer encoder and a collection of lightweight adapters. The shared encoder captures the general knowledge of dialogues across domains, while each adapter specializes in one specific domain and serves as a domain expert. To validate the idea, we construct a high-quality multi-domain dialogue dataset leveraging data augmentation and pseudo-labeling. The PoE network is comprehensively assessed on 16 dialogue evaluation datasets spanning a wide range of dialogue domains. It achieves state-of-the-art performance in terms of mean Spearman correlation over all the evaluation datasets. It exhibits better zero-shot generalization than existing state-of-the-art ADEMs and the ability to easily adapt to new domains with few-shot transfer learning.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1234–1250},
numpages = {17}
}

@article{10.1145/3712701,
author = {Lee, Seungpil and Sim, Woochang and Shin, Donghyeon and Seo, Wongyu and Park, Jiwon and Lee, Seokki and Hwang, Sanha and Kim, Sejin and Kim, Sundong},
title = {Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3712701},
doi = {10.1145/3712701},
abstract = {The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been predominantly results-centric, making it challenging to assess the inference process comprehensively. We introduce a novel approach using the Abstraction and Reasoning Corpus (ARC) benchmark to evaluate the inference and contextual understanding abilities of LLMs in a process-centric manner, focusing on three key components from the Language of Thought Hypothesis (LoTH): Logical Coherence, Compositionality, and Productivity. Our carefully designed experiments reveal that while LLMs demonstrate some inference capabilities, they still significantly lag behind human-level reasoning in these three aspects. The main contribution of this paper lies in introducing the LoTH perspective, which provides a method for evaluating the reasoning process that conventional results-oriented approaches fail to capture, thereby offering new insights into the development of human-level reasoning in artificial intelligence systems.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
keywords = {Large Language Models, Abstraction and Reasoning Corpus, Language of Thought Hypothesis, Logical Coherence, Compositionality, Productivity}
}

@inproceedings{10.1109/ICSE-Companion66252.2025.00067,
author = {Chen, Boqi},
title = {Consistent Graph Model Generation with Large Language Models},
year = {2025},
isbn = {9798331536831},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion66252.2025.00067},
doi = {10.1109/ICSE-Companion66252.2025.00067},
abstract = {Graph model generation from natural language requirements is an essential task in software engineering, for which large language models (LLMs) have become increasingly popular. A key challenge is ensuring that the generated graph models are consistent with domain-specific well-formed constraints. LLM-generated graphs are often partially correct due to inconsistency with the constraints, limiting their practical usage. To address this, we propose a novel abstraction-concretization framework motivated by self-consistency for generating consistent models. Our approach first abstracts candidate models into a probabilistic partial model and then concretizes this abstraction into a consistent graph model. Preliminary evaluations on taxonomy generation demonstrate that our method significantly enhances both the consistency and quality of generated graph models.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings},
pages = {218–219},
numpages = {2},
keywords = {large language models, graph model generation, constraint optimization},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3579027.3608973,
author = {Galindo, Jos\'{e} A. and Dominguez, Antonio J. and White, Jules and Benavides, David},
title = {Large Language Models to generate meaningful feature model instances},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608973},
doi = {10.1145/3579027.3608973},
abstract = {Feature models are the "de facto" standard for representing variability in software-intensive systems. Automated analysis of feature models is the computer-aided extraction of information of feature models and is used in testing, maintenance, configuration, and derivation, among other tasks. Testing the analyses of feature models often requires relying on a large number of models that are as realistic as possible. There exist different proposals to generate synthetic feature models using random techniques or metamorphic relations; however, the existing methods do not take into account the semantics of the concepts of the domain that are being represented and the interrelations between them, leading to less realistic feature models. In this paper, we propose a novel approach that uses Large Language Models (LLMs), such as Codex or GPT-3, to generate realistic feature models that preserve semantic coherence while maintaining syntactic validity. The approach automatically generates instances of feature models from a given domain. Concretely, two language models were used, first OpenAI's Codex to generate new instances of feature models using the Universal Variability Language (UVL) syntax and then Cohere's semantic analysis to verify if the newly introduced concepts are from the same domain. This approach enabled the generation of 90\% of valid instances according to the UVL syntax. In addition, the valid models score well on model complexity metrics, and the generated features mirror the domain of the original UVL instance used as prompts. With this work, we envision a new thread of research where variability is generated and analyzed using LLMs. This opens the door for a new generation of techniques and tools for variability management.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {15–26},
numpages = {12},
keywords = {universal variability language, synthetic models, large language models, deep learning},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@article{10.1145/3758967,
author = {Ravi, Kamalakkannan and Yuan, Jiann-Shiun},
title = {ALERT: Active Learning and Explainable AI for Robust Threat Detection in Telegram},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3758967},
doi = {10.1145/3758967},
abstract = {The increasing regulatory scrutiny of social media, particularly regarding extremist content and misinformation, underscores the need for advanced threat detection systems. This paper presents ALERT (Active Learning and Explainable AI for Robust Threat Detection in Telegram), a novel framework that enhances threat classification by introducing refined categories and creating tailored datasets1. ALERT processes 2,301,110 replies from 17 Telegram channels, focusing on extreme content, with a dataset that predominantly reflects far-right discourse, consistent with activity trends on the platform. By leveraging an iterative active learning approach, it reduces labeling efforts by 86.5\%, yielding a labeled dataset of 15,076 replies. ALERT's RoBERTa+ model, pre-trained on domain-specific data, achieved over 90\% in precision, recall, accuracy, and F1-score, demonstrating strong generalization for threat detection. The integrated explainable AI (XAI) modules highlight key text features driving model predictions, ensuring transparency while maintaining performance. ALERT offers significant improvements in classification precision and user confidence, providing a critical tool2 for addressing digital threats while navigating regulatory and privacy challenges.},
note = {Just Accepted},
journal = {Digital Threats},
month = aug,
keywords = {active learning, cyberbullying, explainable AI, extremism, Learning (artificial intelligence), natural language processing, online radicalization, political violence, social media, telegram, user-generated content}
}

@article{10.1145/3689799,
author = {Drosos, Georgios-Petros and Sotiropoulos, Thodoris and Alexopoulos, Georgios and Mitropoulos, Dimitris and Su, Zhendong},
title = {When Your Infrastructure Is a Buggy Program: Understanding Faults in Infrastructure as Code Ecosystems},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689799},
doi = {10.1145/3689799},
abstract = {Modern applications have become increasingly complex and their manual installation and configuration is  no longer practical. Instead, IT organizations heavily rely on Infrastructure as Code (IaC) technologies, to automate the provisioning, configuration, and maintenance of computing infrastructures and systems. IaC systems typically offer declarative, domain-specific languages (DSLs) that allow system administrators and developers to write high-level programs that specify the desired state of their infrastructure in a reliable, predictable, and documented fashion. Just like traditional programs, IaC software is not immune to faults, with issues ranging from deployment failures to critical misconfigurations that often impact production systems used by millions of end users. Surprisingly, despite its crucial role in global infrastructure management, the tooling and techniques for ensuring IaC reliability still have room for improvement.     In this work, we conduct a comprehensive analysis of 360 bugs identified in IaC software within prominent IaC ecosystems including Ansible, Puppet, and Chef. Our work is the first in-depth exploration of bug characteristics in these widely-used IaC environments. Through our analysis we aim to understand: (1) how these bugs manifest, (2) their underlying root causes, (3) their reproduction requirements in terms of system state (e.g., operating system versions) or input characteristics, and (4) how these bugs are fixed. Based on our findings, we evaluate the state-of-the-art techniques for IaC reliability, identify their limitations, and provide a set of recommendations for future research. We believe that our study helps researchers to (1) better understand the complexity and peculiarities of IaC software, and (2) develop advanced tooling for more reliable and robust system configurations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {359},
numpages = {31},
keywords = {Ansible, Chef, IaC, Puppet, bug, deployment, infrastructure as code, testing}
}

@article{10.1145/3588958,
author = {Miao, Xiaoye and Wu, Yangyang and Peng, Jiazhen and Gao, Yunjun and Yin, Jianwei},
title = {Efficient and Effective Cardinality Estimation for Skyline Family},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588958},
doi = {10.1145/3588958},
abstract = {Cardinality estimation, predicting the query result size, is a fundamental problem in databases. Existing skyline cardinality estimation methods are computationally infeasible for massive skyline queries over the large-scale database. In this paper, we introduce a unified skyline family w.r.t. various skyline variants. We propose an efficient and effective skyline family cardinality estimation model, named EECE, in an end-to-end manner. EECE consists of two modules, unsupervised data distribution learning (DDL) and supervised monotonic cardinality estimation (MCE). DDL leverages the mixture data guided transformer to learn the distribution of database and query parameters for model pre-training. MCE further incorporates supervised learning and parameter clamping to enhance the estimation under monotonicity guarantees. We develop an efficient incremental learning algorithm for EECE to adapt the database and query logs update. Extensive experiments on several real-world and synthetic datasets demonstrate that, EECE speeds up the cardinality estimation by six orders of magnitude, with more than 39\% accuracy gain, compared to the state-of-the-art approaches.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {104},
numpages = {21},
keywords = {cardinality estimation, data distribution learning, monotonic cardinality estimation, skyline family}
}

@inproceedings{10.1145/3712716.3712726,
author = {Korol, Allan and Sikos, Leslie F.},
title = {FEAR: A Novel Framework for Representing Digital Forensic Artifacts in Knowledge Graphs},
year = {2025},
isbn = {9798400710766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712716.3712726},
doi = {10.1145/3712716.3712726},
abstract = {In digital forensics, knowledge graphs have demonstrated significant potential via software agent automation and knowledge discovery using encoded expert knowledge in, for example, the form of Semantic Web rules. These advancements have been limited in terms of efficiently encoding extracted digital artifacts into graph representation, the associated overhead of implementing frameworks to handle digital forensic evidence, and the lack of sharing of such code that is often a preamble to other research. This paper introduces a digital forensic framework, Forensic Extraction and Representation (FEAR), that enables a simplified process of accessing and encoding extracted digital forensic artifacts in semantic knowledge graphs. The adoption of such a framework can facilitate the sharing of expert knowledge and reduce the burden of development for researchers exploring the application of knowledge graphs, software agents, and automated reasoning in the field of digital forensics, while accelerating the adoption of emerging research by practitioners.},
booktitle = {Proceedings of the Digital Forensics Doctoral Symposium},
articleno = {9},
numpages = {8},
keywords = {digital forensic artifacts, semantic knowledge graph, digital forensic software agent, digital forensic framework, declarative language},
location = {
},
series = {DFDS '25}
}

@inproceedings{10.1145/3488932.3517393,
author = {Dib, Mirabelle and Torabi, Sadegh and Bou-Harb, Elias and Bouguila, Nizar and Assi, Chadi},
title = {EVOLIoT: A Self-Supervised Contrastive Learning Framework for Detecting and Characterizing Evolving IoT Malware Variants},
year = {2022},
isbn = {9781450391405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488932.3517393},
doi = {10.1145/3488932.3517393},
abstract = {Recent years have witnessed the emergence of new and more sophisticated malware targeting the Internet of Things. Moreover, the public release of the source code of popular malware families such as Mirai has spawned diverse variants, making it harder to disambiguate their ownership, lineage, and correct label. Such a rapidly evolving landscape makes it also harder to deploy and generalize effective learning models against retired, updated, and/or new threat campaigns. In this paper, we present EVOLIoT, a novel approach aiming at combating "concept drift" and the limitations of inter-family IoT malware classification by detecting drifting IoT malware families and understanding their diverse evolutionary trajectories. We introduce a robust and effective contrastive method that learns and compares semantically meaningful representations of IoT malware binaries and codes without the need for expensive target labels. We find that the evolution of IoT binaries can be used as an augmentation strategy to learn effective representations to contrast (dis)similar variant pairs. We discuss the impact and findings of our analysis and present several evaluation studies to highlight the tangled relationships of IoT malware, as well as the efficiency of our contrastively learned feature vectors in preserving semantics and reducing out-of-vocabulary size in cross-architecture IoT malware binaries.},
booktitle = {Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security},
pages = {452–466},
numpages = {15},
keywords = {iot malware classification, contrastive learning, concept drift},
location = {Nagasaki, Japan},
series = {ASIA CCS '22}
}

@inproceedings{10.1145/3658644.3690356,
author = {Baldimtsi, Foteini and Chalkias, Konstantinos Kryptos and Ji, Yan and Lindstr\o{}m, Jonas and Maram, Deepak and Riva, Ben and Roy, Arnab and Sedaghat, Mahdi and Wang, Joy},
title = {zkLogin: Privacy-Preserving Blockchain Authentication with Existing Credentials},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3690356},
doi = {10.1145/3658644.3690356},
abstract = {For many users, a private key based wallet serves as the primary entry point to blockchains. Commonly recommended wallet authentication methods, such as mnemonics or hardware wallets, can be cumbersome. This difficulty in user onboarding has significantly hindered the adoption of blockchain-based applications.We develop zkLogin, a novel technique that leverages identity tokens issued by popular platforms (any OpenID Connect enabled platform e.g., Google, Facebook, etc.) to authenticate transactions. At the heart of zkLogin lies a signature scheme allowing the signer to sign using their existing OpenID accounts and nothing else. This improves the user experience significantly as users do not need to remember a new secret and can reuse their existing accounts.zkLogin provides strong security and privacy guarantees. Unlike prior works, zkLogin's security relies solely on the underlying platform's authentication mechanism without the need for any additional trusted parties (e.g., trusted hardware or oracles). As the name suggests, zkLogin leverages zero-knowledge proofs (ZKP) to ensure that the sensitive link between a user's off-chain and on-chain identities is hidden, even from the platform itself.zkLogin enables a number of important applications outside blockchains. It allows billions of users to produce verifiable digital content leveraging their existing digital identities, e.g., email address. For example, a journalist can use zkLogin to sign a news article with their email address, allowing verification of the article's authorship by any party.We have implemented and deployed zkLogin on the Sui blockchain as an additional alternative to traditional digital signature-based addresses. Due to the ease of web3 on-boarding just with social login, many hundreds of thousands of zkLogin accounts have already been generated in various industries such as gaming, DeFi, direct payments, NFT collections, sports racing, cultural heritage, and many more.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {3182–3196},
numpages = {15},
keywords = {authentication, blockchain, privacy, zero-knowledge},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3639476.3639776,
author = {Rukmono, Satrio Adi and Ochoa, Lina and Chaudron, Michel},
title = {Deductive Software Architecture Recovery via Chain-of-thought Prompting},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639776},
doi = {10.1145/3639476.3639776},
abstract = {As software evolves, software architecture recovery techniques can help for effective maintenance. We envision a deductive software architecture recovery approach supported by Large Language Models (LLMs). Unlike existing inductive (bottom-up) recovery techniques, which reconstruct architecture by considering the properties observed at implementation level, our top-down approach starts with architectural properties and seeks their manifestations in the implementation. It employs a known Reference Architecture (RA) and involves two phases: RA definition and code units classification. A proof-of-concept with GPT-4 emulates deductive reasoning via chain-of-thought prompting. It demonstrates the deductive SAR approach, applying it to the Android application K-9 Mail and achieving a 70\% accuracy in classifying 54 classes and 184 methods. The future plans focus on evaluating and refining the approach through ground-truth assessments, deeper exploration of reference architectures, and advancing toward automated human-like software architecture explanations. We highlight the potential for LLMs in achieving more comprehensive and explainable software architecture recovery.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {92–96},
numpages = {5},
keywords = {software architecture, software architecture recovery, deductive SAR, chain-of-thought prompting},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@article{10.1145/3712296,
author = {Sun, Wei and Li, Mingxiao and Sileo, Damien and Davis, Jesse and Moens, Marie-Francine},
title = {Generating Explanations in Medical Question-Answering by Expectation Maximization Inference over Evidence},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3712296},
doi = {10.1145/3712296},
abstract = {Medical Question Answering (medical QA) systems play an essential role in assisting healthcare workers in finding answers to their questions. However, it is not sufficient to merely provide answers by medical QA systems because users might want explanations, that is, more analytic statements in natural language that describe the elements and context that support the answer. To do so, we propose a novel approach for generating natural language explanations for answers predicted by medical QA systems. As high-quality medical explanations require additional medical knowledge, so that our system extracts knowledge from medical textbooks to enhance the quality of explanations during the explanation generation process. Concretely, we designed an Expectation-Maximization approach that makes inferences about the evidence found in these texts, offering an efficient way to focus attention on lengthy evidence passages. Experimental results, conducted on two datasets MQAE-diag and MQAE, demonstrate the effectiveness of our framework for reasoning with textual evidence. Our approach outperforms state-of-the-art models, achieving a significant improvement of 6.13 and 5.47 percentage points on the Rouge-L score; 6.49 and 5.28 percentage points on the Bleu-4 score on the MQAE-diag and MQAE datasets.},
journal = {ACM Trans. Comput. Healthcare},
month = apr,
articleno = {23},
numpages = {23},
keywords = {Expectation Maximization, Medical Question Answering, Explanation Generation}
}

@inproceedings{10.1145/3665939.3665966,
author = {H\"{a}ttasch, Benjamin and Binnig, Carsten},
title = {More of that, please: Domain Adaptation of Information Extraction through Examples \&amp; Feedback},
year = {2024},
isbn = {9798400706936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665939.3665966},
doi = {10.1145/3665939.3665966},
abstract = {Automatic information extraction, e.g., into a tabular format, is crucial for leveraging knowledge in large text collections. Yet, creating such extraction pipelines for custom target attributes can cause high overheads, while off-the-shelf tools might miss domain-specific information. Therefore, in this paper, we propose an interactive system that augments generic extractions and aligns them with a target definition. The necessary domain adaptation is reached through examples provided by the users during the interaction with the system. As part of this, we propose different low-overhead extractors and evaluate them individually and end-to-end to demonstrate how our approach minimizes the necessary interactions. We publish our code as open source.},
booktitle = {Proceedings of the 2024 Workshop on Human-In-the-Loop Data Analytics},
pages = {1–7},
numpages = {7},
location = {Santiago, AA, Chile},
series = {HILDA  24}
}

@inproceedings{10.1145/3613424.3623794,
author = {Xu, Ceyu and Sharma, Pragya and Wang, Tianshu and Wills, Lisa Wu},
title = {Fast, Robust and Transferable Prediction for Hardware Logic Synthesis},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623794},
doi = {10.1145/3613424.3623794},
abstract = {The increasing complexity of computer chips and the slow logic synthesis process have become major bottlenecks in the hardware design process, also hindering the ability of hardware generators to make informed design decisions while considering hardware costs. While various models have been proposed to predict physical characteristics of hardware designs, they often suffer from limited domain adaptability and open-source hardware design data scarcity. In this paper, we present SNS v2, a fast, robust, and transferable hardware synthesis predictor based on deep learning models. Inspired by modern natural language processing models, SNS v2 adopts a three-phase training approach encompassing pre-training, fine-tuning, and domain adaptation, enabling it to leverage more abundant unlabeled and off-domain training data. Additionally, we propose a novel contrastive learning approach based on circuit equivalence to enhance model robustness. Our experiments demonstrate that SNS v2 achieves two to three orders of magnitude faster speed compared to conventional EDA tools, while maintaining state-of-the-art prediction accuracy. We also show that SNS v2 can be seamlessly integrated into hardware generator frameworks for real-time cost estimation, resulting in higher quality design recommendations in a significantly reduced time frame.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {167–179},
numpages = {13},
keywords = {Integrated Circuits, Logic Synthesis Prediction, Neural Networks, RTL-level Synthesis},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@article{10.1145/3622863,
author = {Chen, Qiaochu and Banerjee, Arko and Demiralp, \c{C}a\u{g}atay and Durrett, Greg and Dillig, I\c{s}\i{}l},
title = {Data Extraction via Semantic Regular Expression Synthesis},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622863},
doi = {10.1145/3622863},
abstract = {Many data extraction tasks of practical relevance require not only syntactic pattern matching but also semantic reasoning about the content of the underlying text. While regular expressions are very well suited for tasks that require only syntactic pattern matching, they fall short for data extraction tasks that involve both a syntactic and semantic component. To address this issue, we introduce semantic regexes, a generalization of regular expressions that facilitates combined syntactic and semantic reasoning about textual data. We also propose a novel learning algorithm that can synthesize semantic regexes from a small number of positive and negative examples. Our proposed learning algorithm uses a combination of neural sketch generation and compositional type-directed synthesis for fast and effective generalization from a small number of examples.  We have implemented these ideas in a new tool called Smore and evaluated it on representative data extraction tasks involving several textual datasets. Our evaluation shows that semantic regexes can better support complex data extraction tasks than standard regular expressions and that our learning algorithm significantly outperforms existing tools, including state-of-the-art neural networks and program synthesis tools.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {287},
numpages = {30},
keywords = {Regular Expression, Program Synthesis}
}

@inproceedings{10.1145/3677052.3698671,
author = {Sarmah, Bhaskarjit and Mehta, Dhagash and Hall, Benika and Rao, Rohan and Patel, Sunil and Pasquali, Stefano},
title = {HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698671},
doi = {10.1145/3677052.3698671},
abstract = {Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&amp;A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&amp;A format, and hence provide a natural set of pairs of ground-truth Q&amp;As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {608–616},
numpages = {9},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@inbook{10.1145/3676641.3716022,
author = {Yang, Chenyuan and Zhao, Zijie and Zhang, Lingming},
title = {KernelGPT: Enhanced Kernel Fuzzing via Large Language Models},
year = {2025},
isbn = {9798400710797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676641.3716022},
abstract = {Bugs in operating system kernels can affect billions of devices and users all over the world. As a result, a large body of research has been focused on kernel fuzzing, i.e., automatically generating syscall (system call) sequences to detect potential kernel bugs or vulnerabilities. Kernel fuzzing aims to generate valid syscall sequences guided by syscall specifications that define both the syntax and semantics of syscalls. While there has been existing work trying to automate syscall specification generation, this remains largely manual work, and a large number of important syscalls are still uncovered.In this paper, we propose KernelGPT, the first approach to automatically synthesizing syscall specifications via Large Language Models (LLMs) for enhanced kernel fuzzing. Our key insight is that LLMs have seen massive kernel code, documentation, and use cases during pre-training, and thus can automatically distill the necessary information for making valid syscalls. More specifically, KernelGPT leverages an iterative approach to automatically infer the specifications, and further debug and repair them based on the validation feedback. Our results demonstrate that KernelGPT can generate more new and valid specifications and achieve higher coverage than state-of-the-art techniques. So far, by using newly generated specifications, KernelGPT has already detected 24 new unique bugs in Linux kernel, with 12 fixed and 11 assigned with CVE numbers. Moreover, a number of specifications generated by KernelGPT have already been merged into the kernel fuzzer Syzkaller, following the request from its development team.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {560–573},
numpages = {14}
}

@inproceedings{10.1145/3711896.3737140,
author = {Liu, Yu and Tao, Weiyao and Xia, Tong and Knight, Simon and Zhu, Tingting},
title = {SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737140},
doi = {10.1145/3711896.3737140},
abstract = {Survival analysis, which estimates the probability of event occurrence over time from censored data, is fundamental in numerous real-world applications, particularly in high-stakes domains such as healthcare and risk assessment. Despite advances in numerous survival models, quantifying the uncertainty of predictions from these models remains underexplored and challenging. The lack of reliable uncertainty quantification limits the interpretability and trustworthiness of survival models, hindering their adoption in clinical decision-making and other sensitive applications. To bridge this gap, in this work, we introduce SurvUnc, a novel meta-model based framework for post-hoc uncertainty quantification for survival models. SurvUnc introduces an anchor-based learning strategy that integrates concordance knowledge into meta-model optimization, leveraging pairwise ranking performance to estimate uncertainty effectively. Notably, our framework is model-agnostic, ensuring compatibility with any survival model without requiring modifications to its architecture or access to its internal parameters. Especially, we design a comprehensive evaluation pipeline tailored to this critical yet overlooked problem. Through extensive experiments on four publicly available benchmarking datasets and five representative survival models, we demonstrate the superiority of SurvUnc across multiple evaluation scenarios, including selective prediction, misprediction detection, and out-of-domain detection. Our results highlight the effectiveness of SurvUnc in enhancing model interpretability and reliability, paving the way for more trustworthy survival predictions in real-world},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {1903–1914},
numpages = {12},
keywords = {meta model, out-of-domain detection, survival analysis, uncertainty quantification},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3487351.3489443,
author = {Paschalides, Demetris and Pallis, George and Dikaiakos, Marios D.},
title = {POLAR: a holistic framework for the modelling of polarization and identification of polarizing topics in news media},
year = {2022},
isbn = {9781450391283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487351.3489443},
doi = {10.1145/3487351.3489443},
abstract = {Polarization is an alarming trend in modern societies with serious implications on social cohesion and the democratic process. Typically, polarization manifests itself in the public discourse in politics, governance and ideology. In recent years, however, polarization arises increasingly in a wider range of issues, from identity and culture to healthcare and the environment. As the public and private discourse moves online, polarization feeds in and is fed by phenomena like fake news and hate speech. The identification and analysis of online polarization is challenging because of the massive scale, diversity, and unstructured nature of online content, and the rapid and unpredictable evolution of polarizing issues. Therefore, we need effective ways to identify, quantify, and represent polarization and polarizing topics algorithmically and at scale. In this work, we introduce POLAR - an unsupervised, large-scale framework for modeling and identifying polarizing topics in any domain, without prior domain-specific knowledge. POLAR comprises a processing pipeline that analyzes a corpus of an arbitrary number of news articles to construct a hierarchical knowledge graph that models polarization and identify polarizing topics discussed in the corpus. Our evaluation shows that POLAR is able to identify and rank polarizing topics accurately and efficiently.},
booktitle = {Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {348–355},
numpages = {8},
keywords = {signed networks, polarizing topic extraction, polarization, natural language processing, inter-group conflict},
location = {Virtual Event, Netherlands},
series = {ASONAM '21}
}

@inproceedings{10.1145/3583131.3590502,
author = {Pantridge, Edward and Helmuth, Thomas},
title = {Solving Novel Program Synthesis Problems with Genetic Programming using Parametric Polymorphism},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590502},
doi = {10.1145/3583131.3590502},
abstract = {Contemporary genetic programming (GP) systems for general program synthesis have been primarily concerned with evolving programs that can manipulate values from a standard set of primitive data types and simple indexed data structures. In contrast, human programmers do not limit themselves to a small finite set of data types and use polymorphism to express an unbounded number of types including nested data structures, product types, and generic functions. Code-building Genetic Programming (CBGP) is a recently introduced method that compiles type-safe programs from linear genomes using stack-based compilation and a formal type system. Although prior work with CBGP has shown initial demonstrations of polymorphism inside evolved programs, we have provided a deeper exploration of these capabilities through the evolution of programs which make use of generic data types such as key-value maps, tuples, and sets, as well as higher order functions and functions with polymorphic type signatures. In our experiments, CBGP is able to solve problems with all of these properties, where every other GP system that we know of has restrictions that make it unable to even consider problems with these properties. This demonstration provides a significant step towards fully aligning the expressiveness of GP to real world programming.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1175–1183},
numpages = {9},
keywords = {polymorphism, inductive program synthesis, genetic programming, automatic programming},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@article{10.1145/3729319,
author = {He, Yang and Fang, Ruijie and Dillig, I\c{s}\i{}l and Wang, Yuepeng},
title = {Graphiti: Bridging Graph and Relational Database Queries},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {PLDI},
url = {https://doi.org/10.1145/3729319},
doi = {10.1145/3729319},
abstract = {This paper presents an automated reasoning technique for checking equivalence between graph database queries written in Cypher and relational queries in SQL. To formalize a suitable notion of equivalence in this setting, we introduce the concept of database transformers, which transform database instances between graph and relational models. We then propose a novel verification methodology that checks equivalence modulo a given transformer by reducing the original problem to verifying equivalence between a pair of SQL queries. This reduction is achieved by embedding a subset of Cypher into SQL through syntax-directed translation, allowing us to leverage existing research on automated reasoning for SQL while obviating the need for reasoning simultaneously over two different data models. We have implemented our approach in a tool called Graphiti and used it to check equivalence between graph and relational queries. Our experiments demonstrate that Graphiti is useful both for verification and refutation and that it can uncover subtle bugs, including those found in Cypher tutorials and academic papers.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {216},
numpages = {25},
keywords = {Equivalence Checking, Graph Databases, Program Verification, Relational Databases}
}

@inproceedings{10.1145/3711896.3737403,
author = {Qin, Chuan and Chen, Xin and Wang, Chengrui and Wu, Pengmin and Chen, Xi and Cheng, Yihang and Zhao, Jingyi and Xiao, Meng and Dong, Xiangchao and Long, Qingqing and Pan, Boya and Wu, Han and Li, Chengzan and Zhou, Yuanchun and Xiong, Hui and Zhu, Hengshu},
title = {SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737403},
doi = {10.1145/3711896.3737403},
abstract = {In recent years, the rapid advancement of Artificial Intelligence (AI) technologies, particularly Large Language Models (LLMs), has revolutionized the paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a dynamic and evolving field. However, there is still a lack of an effective framework for the overall assessment of AI4Science, particularly from a holistic perspective on data quality and model capability. Therefore, in this study, we propose SciHorizon, a comprehensive assessment framework designed to benchmark the readiness of AI4Science from both scientific data and LLM perspectives. First, we introduce a generalizable framework for assessing AI-ready scientific data, encompassing four key dimensions-Quality, FAIRness, Explainability, and Compliance-which are subdivided into 15 sub-dimensions. Drawing on data resource papers published between 2018 and 2023 in peer-reviewed journals, we present recommendation lists of AI-ready datasets for Earth, Life, and Materials Sciences, making a novel and original contribution to the field. Concurrently, to assess the capabilities of LLMs across multiple scientific disciplines, we establish 16 assessment dimensions based on five core indicators-Knowledge, Understanding, Reasoning, Multimodality, and Values-spanning Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space Sciences. Using the developed benchmark datasets, we have conducted a comprehensive evaluation of over 50 representative open-source and closed-source LLMs. All the results are publicly available and can be accessed online at www.scihorizon.cn/en.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {5754–5765},
numpages = {12},
keywords = {AI-for-science, AI-ready, benchmarking, large language models, scientific data},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3650212.3652119,
author = {Zhou, Mingyi and Gao, Xiang and Liu, Pei and Grundy, John and Chen, Chunyang and Chen, Xiao and Li, Li},
title = {Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652119},
doi = {10.1145/3650212.3652119},
abstract = {Recent studies show that on-device deployed deep learning (DL) models, such as those of Tensor Flow Lite (TFLite), can be easily extracted from real-world applications and devices by attackers to generate many kinds of adversarial and other attacks. Although securing deployed on-device DL models has gained increasing attention, no existing methods can fully prevent these attacks. Traditional software protection techniques have been widely explored. If on-device models can be implemented using pure code, such as C++, it will open the possibility of reusing existing robust software protection techniques. However, due to the complexity of DL models, there is no automatic method that can translate DL models to pure code. To fill this gap, we propose a novel method, CustomDLCoder, to automatically extract on-device DL model information and synthesize a customized executable program for a wide range of DL models. CustomDLCoder first parses the DL model, extracts its backend computing codes, configures the extracted codes, and then generates a customized program to implement and deploy the DL model without explicit model representation. The synthesized program hides model information for DL deployment environments since it does not need to retain explicit model representation, preventing many attacks on the DL model. In addition, it improves ML performance because the customized code removes model parsing and preprocessing steps and only retains the data computing process. Our experimental results show that CustomDLCoder improves model security by disabling on-device model sniffing. Compared with the original on-device platform (i.e., TFLite), our method can accelerate model inference by 21.0\% and 24.3\% on x86-64 and ARM64 platforms, respectively. Most importantly, it can significantly reduce memory consumption by 68.8\% and 36.0\% on x86-64 and ARM64 platforms, respectively.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {174–185},
numpages = {12},
keywords = {AI safety, SE for AI, software optimization for AI deployment},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1109/SCW63240.2024.00256,
author = {Vu, Anh Duc and Kehrer, Timo},
title = {Towards Generating Contracts for Scientific Data Analysis Workflows},
year = {2025},
isbn = {9798350355543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SCW63240.2024.00256},
doi = {10.1109/SCW63240.2024.00256},
abstract = {To increase the dependability and portability of scientific data analysis workflows (DAWs), recent work has proposed contract-driven design of DAWs, providing verifiable expectations and obligations to ensure that tasks run in a proper environment and produce correct results. However, the specification of suitable contracts is still left to the discretion of DAW developers, imposing labor-intensive manual work which likely hampers the widespread adoption of contracts in scientific practice. We report about work-in-progress of developing a pipeline empowered by Large Language Models for automatically generating code contracts from logical workflow descriptions. We instantiate this pipeline within the workflow system Nextflow, and evaluate its contract generation capabilities in an experiment using real-world Nextflow modules. Our findings indicate that we generate a substantial amount of contracts serving as starting point for DAW developers. Our approach demonstrates potential in assisting domain scientists with contract-driven design of DAWs, laying the groundwork for its future adoption.},
booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {2048–2055},
numpages = {8},
location = {Atlanta, GA, USA},
series = {SC-W '24}
}

@article{10.1145/3702234,
author = {Fang, Chen and Wang, Yidong and Song, Yunze and Long, Qingqing and Lu, Wang and Chen, Linghui and Feng, Guihai and Zhou, Yuanchun and Li, Xin},
title = {How do Large Language Models understand Genes and Cells},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3702234},
doi = {10.1145/3702234},
abstract = {Researching genes and their interactions is crucial for deciphering the fundamental laws of cellular activity, advancing disease treatment, drug discovery, and more. Large language Models (LLMs), with their profound text comprehension and generation capabilities, have made significant strides across various natural science fields. However, their application in cell biology remains limited and a systematic evaluation of their performance is lacking. To address this gap, in this paper, we select seven mainstream LLMs and evaluate their performance across nine gene-related problem scenarios. Our findings indicate that LLMs possess a certain level of understanding of genes and cells, but still lag behind domain-specific models in comprehending transcriptional expression profiles. Moreover, we have improved the current method of textual representation of cells, enhancing the LLMs’ ability to tackle cell annotation tasks. We encourage cell biology researchers to leverage LLMs for problem-solving while being mindful of the associated challenges. We release our code and data at .},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
keywords = {large language models, cell biology, gene gene interaction, cell annotation}
}

@article{10.14778/3750601.3750622,
author = {Jiang, Zhe and Wang, Zhaoguo and Lan, Haoning and Tang, Chuzhe and Ding, Haoran and Wang, Lefeng and Zou, Songyun and Wei, Zhuoran and Liu, Yongcun and Yu, Xiang and Ren, Yang and Li, Guoliang and Chen, Haibo},
title = {GRewriter: Practical Query Rewriting with Automatic Rule Set Expansion in GaussDB},
year = {2025},
issue_date = {August 2025},
publisher = {VLDB Endowment},
volume = {18},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3750601.3750622},
doi = {10.14778/3750601.3750622},
abstract = {Effectively rewriting a wide range of complex and diverse queries is critical for database systems. Huawei GaussDB has been experiencing limited extensibility of its existing query rewriter. The problem is rooted in the need for one-size-fits-all rewrites by its pipelined rewrite workflow and the source code-level coupling of rewrite logic. This makes it not only difficult to identify generic, broadly applicable rewrites but also engineering-intensive to program them into the system.This paper presents GRewriter, GaussDB's new bolt-on extensible query rewriter powered by automated rewrite rule discovery. GRewriter sits atop the existing optimizer stack to explore useful rewrites, allowing a variety of rules to coexist and be selected on a per-query basis. A new rule language, G-DSL, is used to express rewrite rules so that the rewrite engine is not coupled with specific rules. To improve rewrite efficiency, a new rule index structure and a rewrite history cache are introduced. Rules in GRewriter are produced by an offline rule generator. With novel enumeration techniques and a new equivalence theorem, our rule generator can efficiently discover formally verified rules that are much more expressive than prior research prototypes. For operational convenience, GRewriter also supports manual rule authoring and interactive management of rules through familiar SQL interfaces.GRewriter has been integrated into GaussDB and is gradually rolling out to customers. GRewriter equips GaussDB with over a hundred rules while maintaining negligible overhead (&lt;1\%). These new rewrite rules have enhanced query performance for two key customer applications, an ERP system and a Banking transaction system, reducing production query latency by up to 99.9\%—from 26 seconds to just 17 milliseconds.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {4991–5003},
numpages = {13}
}

@inproceedings{10.1145/3702634.3702950,
author = {Wang, Li and Jiang, Yankai and Mi, Ningfang},
title = {Advancing Serverless Computing for Scalable AI Model Inference: Challenges and Opportunities},
year = {2024},
isbn = {9798400713361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702634.3702950},
doi = {10.1145/3702634.3702950},
abstract = {Artificial Intelligence (AI) model inference has emerged as a crucial component across numerous applications. Serverless computing, known for its scalability, flexibility, and cost-efficiency, is an ideal paradigm for executing AI model inference tasks. This survey provides a comprehensive review of recent research on AI model inference systems in serverless environments, focusing on studies published since 2019. We investigate system-level advancements aimed at optimizing performance and cost-efficiency through a range of innovative techniques. By analyzing high-impact papers from leading venues in AI model inference and serverless computing, we highlight key breakthroughs and solutions. This survey serves as a valuable resource for both practitioners and academic researchers, offering critical insights into the current state and future trends in integrating AI model inference with serverless architectures. To the best of our knowledge, this is the first survey that includes Large Language Models (LLMs) inference in the context of serverless computing.},
booktitle = {Proceedings of the 10th International Workshop on Serverless Computing},
pages = {1–6},
numpages = {6},
keywords = {serverless computing, LLMs inference, DL inference, ML inference},
location = {Hong Kong, Hong Kong},
series = {WoSC10 '24}
}

@article{10.1145/3660825,
author = {Wu, Yaoxuan and Humayun, Ahmad and Gulzar, Muhammad Ali and Kim, Miryung},
title = {Natural Symbolic Execution-Based Testing for Big Data Analytics},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660825},
doi = {10.1145/3660825},
abstract = {Symbolic execution is an automated test input generation technique that models individual program paths as logical constraints. However, the realism of concrete test inputs generated by SMT solvers often comes into question. Existing symbolic execution tools only seek arbitrary solutions for given path constraints. These constraints do not incorporate the naturalness of inputs that observe statistical distributions, range constraints, or preferred string constants. This results in unnatural-looking inputs that fail to emulate real-world data.                In this paper, we extend symbolic execution with consideration for incorporating naturalness. Our key insight is that users typically understand the semantics of program inputs, such as the distribution of height or possible values of zipcode, which can be leveraged to advance the ability of symbolic execution to produce natural test inputs. We instantiate this idea in NaturalSym, a symbolic execution-based test generation tool for data-intensive scalable computing (DISC) applications. NaturalSym generates natural-looking data that mimics real-world distributions by utilizing user-provided input semantics to drastically enhance the naturalness of inputs, while preserving strong bug-finding potential. On DISC applications and commercial big data test benchmarks, NaturalSym achieves a higher degree of realism —as evidenced by a perplexity score 35.1 points lower on median, and detects 1.29\texttimes{} injected faults compared to the state-of-the-art symbolic executor for DISC, BigTest. This is because BigTest draws inputs purely based on the satisfiability of path constraints constructed from branch predicates, while NaturalSym is able to draw natural concrete values based on user-specified semantics and prioritize using these values in input generation. Our empirical results demonstrate that NaturalSym finds injected faults 47.8\texttimes{} more than NaturalFuzz (a coverage-guided fuzzer) and 19.1\texttimes{} more than ChatGPT. Meanwhile, TestMiner (a mining-based approach) fails to detect any injected faults. NaturalSym is the first symbolic executor that combines the notion of input naturalness in symbolic path constraints during SMT-based input generation. We make our code available at https://github.com/UCLA-SEAL/NaturalSym.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {118},
numpages = {24},
keywords = {DISC Applications, Naturalness, Symbolic Execution}
}

@inproceedings{10.1145/3544548.3580817,
author = {Liu, Michael Xieyang and Sarkar, Advait and Negreanu, Carina and Zorn, Benjamin and Williams, Jack and Toronto, Neil and Gordon, Andrew D.},
title = {“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580817},
doi = {10.1145/3544548.3580817},
abstract = {Code-generating large language models map natural language to code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the user’s natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users’ understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {598},
numpages = {31},
keywords = {Human-AI Interaction, Large Language Models, Natural Language Programming, Spreadsheets},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3726302.3729901,
author = {Sakhovskiy, Andrey and Tutubalina, Elena},
title = {BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3729901},
doi = {10.1145/3726302.3729901},
abstract = {In recent years, there has been substantial progress in using pretrained Language Models (LMs) on a range of tasks aimed at improving the understanding of biomedical texts. Nonetheless, existing biomedical LLMs show limited comprehension of complex, domain-specific concept structures and the factual information encoded in biomedical Knowledge Graphs (KGs). In this work, we propose BALI (Biomedical Knowledge Graph and Language Model Ali gnment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph. For a given textual sequence, we link biomedical concept mentions to the Unified Medical Language System (UMLS) KG and utilize local KG subgraphs as cross-modal positive samples for these mentions. Our empirical findings indicate that implementing our method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations, even with minimal pre-training on a small alignment dataset sourced from PubMed scientific abstracts.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1152–1164},
numpages = {13},
keywords = {biomedical knowledge graph, biomedical language model, biomedical natural language processing, contrastive learning, natural language processing, representation learning},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3665939.3665960,
author = {Strasser, Sebastian and Klettke, Meike},
title = {Transparent Data Preprocessing for Machine Learning},
year = {2024},
isbn = {9798400706936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665939.3665960},
doi = {10.1145/3665939.3665960},
abstract = {Data preprocessing is an important task in machine learning which can significantly improve model outcomes. However, evaluating the impact of data preprocessing is often difficult. There is a need for tools which make it transparent to the user on how certain transformations conducted in preprocessing affect the data. Thus, we propose a vision of a transparency system for data preprocessing that provides insights into data preparation pipelines. Our envisioned system consists of a Python library which enables users to log transformations and processed data. Subsequently, the system generates summaries of the data which was processed in the pipeline and so-called change profiles which capture the changes conducted in each processing step. These abstractions offer insight into the transformations and their effects on data. Additionally, the system includes an user interface where users can interactively discover the implemented pipeline and the changes made during preprocessing. This paper presents an initial concept of such a system. It also examines further challenges related to making preprocessing transparent and discusses potential solutions to address these challenges.},
booktitle = {Proceedings of the 2024 Workshop on Human-In-the-Loop Data Analytics},
pages = {1–6},
numpages = {6},
keywords = {data preprocessing, data profiles, change profiles, transparency},
location = {Santiago, AA, Chile},
series = {HILDA  24}
}

@inproceedings{10.1145/3624062.3626283,
author = {Bader, Jonathan and Belak, Jim and Bement, Matthew and Berry, Matthew and Carson, Robert and Cassol, Daniela and Chan, Stephen and Coleman, John and Day, Kastan and Duque, Alejandro and Fagnan, Kjiersten and Froula, Jeff and Jha, Shantenu and Katz, Daniel S. and Kica, Piotr and Kindratenko, Volodymyr and Kirton, Edward and Kothadia, Ramani and Laney, Daniel and Lehmann, Fabian and Leser, Ulf and Licho\l{}ai, Sabina and Malawski, Maciej and Melara, Mario and Player, Elais and Rolchigo, Matt and Sarrafan, Setareh and Sul, Seung-Jin and Syed, Abdullah and Thamsen, Lauritz and Titov, Mikhail and Turilli, Matteo and Caino-Lores, Silvina and Mandal, Anirban},
title = {Novel Approaches Toward Scalable Composable Workflows in Hyper-Heterogeneous Computing Environments},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3626283},
doi = {10.1145/3624062.3626283},
abstract = {The annual Workshop on Workflows in Support of Large-Scale Science (WORKS) is a premier venue for the scientific workflow community to present the latest advances in research and development on the many facets of scientific workflows throughout their life-cycle. The Lightning Talks at WORKS focus on describing a novel tool, scientific workflow, or concept, which are work-in-progress and address emerging technologies and frameworks to foster discussion in the community. This paper summarizes the lightning talks at the 2023 edition of WORKS, covering five topics: leveraging large language models to build and execute workflows; developing a common workflow scheduler interface; scaling uncertainty workflow applications on exascale computing systems; evaluating a transcriptomics workflow for cloud vs. HPC systems; and best practices in migrating legacy workflows to workflow management systems.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {2097–2108},
numpages = {12},
keywords = {Scientific workflows, cloud computing, high-performance computing, large language models, legacy workflows., workflow scheduling},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3626246.3653378,
author = {Pavlenko, Anna and Cahoon, Joyce and Zhu, Yiwen and Kroth, Brian and Nelson, Michael and Carter, Andrew and Liao, David and Wright, Travis and Camacho-Rodr\'{\i}guez, Jes\'{u}s and Saur, Karla},
title = {Vertically Autoscaling Monolithic Applications with CaaSPER: Scalable Container-as-a-Service Performance Enhanced Resizing Algorithm for the Cloud},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3653378},
doi = {10.1145/3626246.3653378},
abstract = {Kubernetes has emerged as a prominent open-source platform for managing cloud applications, including stateful databases. These monolithic applications rely on vertical scaling, adjusting CPU cores based on load fluctuations. However, our analysis of Kubernetes-based Database-as-a-Service (DBaaS) offerings at Microsoft revealed that many customers consistently over-provision resources for peak workloads, neglecting cost-saving opportunities through resource scale-down. We found that there is a gap in the ability of existing vertical autoscaling tools to minimize resource slack and respond promptly to throttling, leading to increased costs and impacting crucial metrics such as throughput and availability.To address this challenge, we propose CaaSPER, a vertical autoscaling algorithm that blends reactive and proactive strategies. By dynamically adjusting CPU resources, CaaSPER minimizes resource slack, maintains optimal CPU utilization, and reduces throttling. Importantly, customers have the flexibility to prioritize either cost savings or high performance based on their preferences. Extensive testing demonstrates that CaaSPER effectively reduces throttling and keeps CPU utilization within target levels. CaaSPER is designed to be application-agnostic and platform-agnostic, with potential for extension to other applications requiring vertical autoscaling.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {241–254},
numpages = {14},
keywords = {containers, kubernetes, resource optimization, vertical auto-scaling},
location = {Santiago AA, Chile},
series = {SIGMOD '24}
}

@article{10.1145/3727638,
author = {Hakimi, Yacine and Baghdadi, Riyadh and Challal, Yacine},
title = {Supporting Dynamic Program Sizes in Deep Learning-Based Cost Models for Code Optimization},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3727638},
doi = {10.1145/3727638},
abstract = {Automatic code optimization enables developers to write high-level code relying on compilers to optimize it and generate efficient code for target hardware. State-of-the-art methods for automatic code optimization leverage deep learning to build cost models that predict the impact of code optimizations on execution time. However, these models are typically limited in terms of the size and complexity of the programs they support. This research presents a novel approach to developing deep learning-based cost models that address these limitations. Our approach introduces a new program representation that efficiently represents programs with complex structures and large sizes such as varying loop depths, buffer numbers, and dimensions. Furthermore, we propose a novel deep learning architecture, that can handle this dynamic program representation. This allows the model to work on larger and more complex programs than those it was trained on. We implemented this model in Tiramisu, a state-of-the-art compiler. Our evaluation shows that our proposed model can generalize to programs larger than those seen during training, while the original Tiramisu cost model cannot. We also show that such generality does not lead to a significant increase in our proposed model’s Mean Absolute Percentage Error or a decrease in the quality of code optimizations found when the model is used for automatic code optimization. In contrast, our proposed model on average achieves a 41.89\% improvement in speed compared to the original cost model when both models are trained on the same dataset, showing better generalization over unseen programs. This is a significant advantage over previous approaches, which typically do not support program sizes beyond those seen during the training.},
journal = {ACM Trans. Archit. Code Optim.},
month = jul,
articleno = {67},
numpages = {25},
keywords = {Automatic code optimization, deep learning, cost model, tiramisu}
}

@article{10.1145/3729287,
author = {Wang, Bo and Li, Tianyu and Li, Ruishi and Mathur, Umang and Saxena, Prateek},
title = {Program Skeletons for Automated Program Translation},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {PLDI},
url = {https://doi.org/10.1145/3729287},
doi = {10.1145/3729287},
abstract = {Translating software between programming languages is a challenging task, for which automated techniques have been elusive and hard to scale up to larger programs. A key difficulty in cross-language translation is that one has to re-express the intended behavior of the source program into idiomatic constructs of a different target language. This task needs abstracting away from the source language-specific details, while keeping the overall functionality the same. In this work, we propose a novel and systematic approach for making such translation amenable to automation based on a framework we call program skeletons. A program skeleton retains the high-level structure of the source program by abstracting away and effectively summarizing lower-level concrete code fragments, which can be mechanically translated to the target programming language. A skeleton, by design, permits many different ways of filling in the concrete implementation for fragments, which can work in conjunction with existing data-driven code synthesizers. Most importantly, skeletons can conceptually enable sound decomposition, i.e., if each individual fragment is correctly translated, taken together with the mechanically translated skeleton, the final translated program is deemed to be correct as a whole. We present a prototype system called SKEL embodying the idea of skeleton-based translation from Python to JavaScript. Our results show promising scalability compared to prior works. For 9 real-world Python programs, some with more than about 1k lines of code, 95\% of their code fragments can be automatically translated, while about 5\% require manual effort. All the final translations are correct with respect to whole-program test suites.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {184},
numpages = {25},
keywords = {Large Language Models, Program Synthesis, Program Translation}
}

@article{10.1145/3591280,
author = {Li, Ziyang and Huang, Jiani and Naik, Mayur},
title = {Scallop: A Language for Neurosymbolic Programming},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591280},
doi = {10.1145/3591280},
abstract = {We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-the-art models in terms of accuracy. Furthermore, Scallop's solutions outperform these models in aspects such as runtime and data efficiency, interpretability, and generalizability.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {166},
numpages = {25},
keywords = {Neurosymbolic methods, Differentiable reasoning}
}

@inproceedings{10.1145/3580305.3599907,
author = {Bashlovkina, Vasilisa and Matthews, Riley and Kuang, Zhaobin and Baumgartner, Simon and Bendersky, Michael},
title = {SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599907},
doi = {10.1145/3580305.3599907},
abstract = {We study the ability of transformer-based language models (LMs) to understand social media language. Social media (SM) language is distinct from standard written language, yet existing benchmarks fall short of capturing LM performance in this socially, economically, and politically important domain. We quantify the degree to which social media language differs from conventional language and conclude that the difference is significant both in terms of token distribution and rate of linguistic shift. Next, we introduce a new benchmark for Social MedIa Language Evaluation (SMILE) that covers four SM platforms and eleven tasks. Finally, we show that learning a tokenizer and pretraining on a mix of social media and conventional language yields an LM that outperforms the best similar-sized alternative by 4.2 points on the overall SMILE score.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3737–3749},
numpages = {13},
keywords = {datasets, language modeling, neural networks, social media, t5, transfer learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1109/CGO57630.2024.10444873,
author = {Jangda, Abhinav and Maleki, Saeed and Dehnavi, Maryam Mehri and Musuvathi, Madan and Saarikivi, Olli},
title = {A Framework for Fine-Grained Synchronization of Dependent GPU Kernels},
year = {2024},
isbn = {9798350395099},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO57630.2024.10444873},
doi = {10.1109/CGO57630.2024.10444873},
abstract = {Machine Learning (ML) models execute several parallel computations including Generalized Matrix Multiplication, Convolution, Dropout, etc. These computations are commonly executed on Graphics Processing Units (GPUs), by dividing the computation into independent processing blocks, known as tiles. Since the number of tiles are usually higher than the execution units of a GPU, tiles are executed on all execution units in one or more waves. However, the number of tiles is not always a multiple of the number of execution units. Thus, tiles executed in the final wave can under-utilize the GPU.To address this issue, we present cuSync, a framework for synchronizing dependent kernels using a user-defined finegrained synchronization policy to improve the GPU utilization. cuSync synchronizes tiles instead of kernels, which allows executing independent tiles of dependent kernels concurrently. We also present a compiler to generate diverse fine-grained synchronization policies based on dependencies between kernels. Our experiments found that synchronizing CUDA kernels using cuSync reduces the inference times of four popular ML models: MegatronLM GPT-3 by up to 15\%, LLaMA by up to 14\%, ResNet-38 by up to 22\%, and VGG-19 by up to 16\% over several batch sizes.},
booktitle = {Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {93–105},
numpages = {13},
keywords = {CUDA, GPU, generalized matrix multiplication, convolution, fine-grained synchronization, machine learning},
location = {Edinburgh, United Kingdom},
series = {CGO '24}
}

@article{10.1145/3749488,
author = {Zhang, Zhihan and Thavikulwat, Puvarin and Metzger, Alexander Le and Mei, Yuxuan and H\"{a}hnlein, Felix and Englhardt, Zachary and Abowd, Gregory D. and Patel, Shwetak and Schulz, Adriana and Cheng, Tingyu and Iyer, Vikram},
title = {Living Sustainability: In-Context Interactive Environmental Impact Communication},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
url = {https://doi.org/10.1145/3749488},
doi = {10.1145/3749488},
abstract = {Climate change demands urgent action, yet understanding the environmental impact (EI) of everyday objects and activities remains challenging for the general public. While Life Cycle Assessment (LCA) offers a comprehensive framework for EI analysis, its traditional implementation requires extensive domain expertise, structured input data, and significant time investment, creating barriers for non-experts seeking real-time sustainability insights. Here we present the first autonomous sustainability assessment tool that bridges this gap by transforming unstructured natural language descriptions into in-context, interactive EI visualizations. Our approach combines language modeling and AI agents, and achieves &gt;97\% accuracy in transforming natural language into a data abstraction designed for simplified LCA modeling. The system employs a non-parametric datastore to integrate proprietary LCA databases while maintaining data source attribution and allowing personalized source management. We demonstrate through case studies that our system achieves results within 11\% of traditional full LCA, while accelerating from hours of expert time to real-time. We conducted a formative elicitation study (N=6) to inform the design objectives of such EI communication augmentation tools. We implemented and deployed the tool as a Chromium browser extension and further evaluated it through a user study (N=12). This work represents a significant step toward democratizing access to environmental impact information for the general public with zero LCA expertise.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {153},
numpages = {42},
keywords = {AI Agents, AI for Sustainability, Augmented Communication, Life Cycle Assessment, Sustainable Computing}
}

@article{10.1145/3689789,
author = {Mariano, Benjamin and Wang, Ziteng and Pailoor, Shankara and Collberg, Christian and Dillig, I\c{s}il},
title = {Control-Flow Deobfuscation using Trace-Informed Compositional Program Synthesis},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689789},
doi = {10.1145/3689789},
abstract = {Code deobfuscation, which attempts to simplify code that has been intentionally obfuscated to prevent understanding, is a critical technique for downstream security analysis tasks like malware detection. While there has been significant prior work on code deobfuscation, most techniques either do not handle control flow obfuscations that modify control flow or they target specific classes of control flow obfuscations, making them unsuitable for handling new types of obfuscations or combinations of existing ones. In this paper, we study a new deobfuscation technique that is based on program synthesis and that can handle a broad class of control flow obfuscations. Given an obfuscated program P, our approach aims to synthesize a smallest program that is a control-flow reduction of P and that is semantically equivalent. Since our method does not assume knowledge about the types of obfuscations that have been applied to the original program, the underlying synthesis problem ends up being very challenging. To address this challenge, we propose a novel trace-informed compositional synthesis algorithm that leverages hints present in dynamic traces of the obfuscated program to decompose the synthesis problem into a set of simpler subproblems. In particular, we show how dynamic traces can be useful for inferring a suitable control-flow skeleton of the deobfuscated program and performing independent synthesis of each basic block. We have implemented this approach in a tool called Chisel and evaluate it on 546 benchmarks that have been obfuscated using combinations of six different obfuscation techniques. Our evaluation shows that our approach is effective and that it produces code that is almost identical (modulo variable renaming) to the original (non-obfuscated) program in 86\% of cases. Our evaluation also shows that Chisel significantly outperforms existing techniques.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {349},
numpages = {31},
keywords = {Deobfuscation, Obfuscation, Program Synthesis}
}

@article{10.1145/3715069,
author = {Varshney, Deeksha and Behera, Niranshu and Katari, Prajeet and Ekbal, Asif},
title = {MedProm: Bridging Dialogue Gaps in Healthcare with Knowledge-Enhanced Generative Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715069},
doi = {10.1145/3715069},
abstract = {In medical dialogue systems, recent advancements underscore the critical role of incorporating relevant medical knowledge to enhance performance. However, existing knowledge bases often lack completeness, posing a challenge in sourcing pertinent information. We present MedProm, a novel generative model tailored for medical dialogue generation to address this gap. Motivated by the need for comprehensive and contextually relevant responses, MedProm leverages state-of-the-art language models such as BioGPT. Our model is designed to integrate extensive medical knowledge into conversations, facilitating effective communication between patients and healthcare providers. At the core of MedProm lies the MediConnect Graph, a meticulously constructed knowledge graph capturing intricate relationships among medical entities extracted from dialogue contexts. By employing a KnowFusion encoder with a pretraining objective and masked multi-head self-attention, MedProm effectively processes the MediConnect graph, enabling precise control over information flow to capture its underlying structure. Furthermore, MedProm incorporates a sophisticated Curriculum Knowledge Decoder, leveraging transformer-based decoding to generate response utterances conditioned on input representations from the KnowFusion Encoder. The training process is guided through curriculum learning, gradually increasing optimization difficulty based on a coherence-based criterion. Experimental results on two datasets demonstrate the efficacy of MedProm in generating accurate and contextually relevant responses compared to state-of-the-art models.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = jan,
keywords = {Medical Dialogue Systems (MDS), Generative Neural Model, MediConnect Graph, KnowFusion Encoder, Curriculum Knowledge Decoder}
}

@article{10.1145/3686921,
author = {Narayanan Venkit, Pranav and Graziul, Christopher and Goodman, Miranda Ardith and Kenny, Samantha Nicole and Wilson, Shomir},
title = {Race and Privacy in Broadcast Police Communications},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW2},
url = {https://doi.org/10.1145/3686921},
doi = {10.1145/3686921},
abstract = {Radios are essential for the operations of modern police departments, and they function as both a collaborative communication technology and a sociotechnical system. However, little prior research has examined their usage or their connections to individual privacy and the role of race in policing, two growing topics of concern in the US. As a case study, we examine the Chicago Police Department's (CPD's) use of broadcast police communications (BPC) to coordinate the activity of law enforcement officers (LEOs) in the city. From a recently assembled archive of 80,775 hours of BPC associated with CPD operations, we analyze human-generated text transcripts of radio transmissions broadcast 9:00 AM to 5:00 PM on August 10th, 2018 in one majority Black, one majority White, and one majority Hispanic area of the city (24 hours of audio) to explore four research questions: (1) Do BPC reflect reported racial disparities in policing? (2) How and when is gender, race/ethnicity, and age mentioned in BPC? (3) To what extent do BPC include sensitive information, and who is put at most risk by this practice? (4) To what extent can large language models (LLMs) heighten this risk? We explore the vocabulary and speech acts used by police in BPC, comparing mentions of personal characteristics to local demographics, the personal information shared over BPC, and the privacy concerns that it poses. Analysis indicates (a) policing professionals in the city of Chicago exhibit disproportionate attention to Black members of the public regardless of context, (b) sociodemographic characteristics like gender, race/ethnicity, and age are primarily mentioned in BPC about event information, and (c) disproportionate attention introduces disproportionate privacy risks for Black members of the public. This study shows BPC can provide a novel window into disproportionate attention (i.e., via radio communications) by law enforcement officers to specific racial groups, leading to increased privacy vulnerability for those groups, particularly Black males.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {382},
numpages = {26},
keywords = {broadcast police communication, lexical analysis, policing disparities, privacy vulnerability, qualitative coding, social informatics}
}

@article{10.1145/3733598,
author = {Zheng, Kai and Wang, Zan and Zhao, Yingquan and Chen, Junjie and You, Hanmo and Wang, Haoyu and Du, Yiheng and Gao, Tianchang},
title = {Exploring JVM Garbage Collector Testing with Event-Coverage},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3733598},
doi = {10.1145/3733598},
abstract = {Garbage Collection (GC) in the Java Virtual Machine (JVM) serves as an automatic memory management mechanism, efficiently reclaiming unused memory space in different production scenarios. To optimize JVM performance, developers typically fine-tune the garbage collector by identifying an optimal set of GC configurations for specific scenarios. Despite the sophisticated design of garbage collectors, they still have the potential for bugs in different settings, and these bugs can result in more severe consequences. Hence, comprehensive testing of these garbage collectors is imperative before their release. Code coverage criteria are typically employed to assess the comprehensiveness of a test suite. However, traditional code coverage metrics, such as branch coverage, are hardly applicable for GC testing due to their inherent concurrency. Additionally, existing JVM testing techniques do not adequately consider the characteristics of garbage collectors, making it difficult to test these garbage collectors sufficiently. In this paper, we make the first effort to design coverage criteria against garbage collectors based on the events of GC called Event-Coverage. Its key insight is to measure the diversity of GC executions for testing purposes by assessing the range of GC events these executions cover. Furthermore, we design a new testing method for maximizing Event-Coverage called GCFuzz. GCFuzz conducts an exhaustive investigation of the memory state space of GC and thoroughly explores the memory state under various GC configurations. To enhance GCFuzz’s efficiency in achieving higher Event-Coverage, we have further designed a coverage-driven strategy for preserving candidate seed programs and selecting GC configurations. Extensive evaluations demonstrate a positive correlation between Event-Coverage and the bug-revealing efficiency. Moreover, GCFuzz outperforms state-of-the-art techniques in detecting unique GC-related inconsistencies and achieving higher Event-Coverage. Remarkably, GCFuzz has identified 20 previously undetected GC bugs, with 15 of them already confirmed or fixed by developers.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
keywords = {Java Virtual Machine, Program Generation, JVM Testing, Garbage Collection, GC Testing, Event-Coverage}
}

@article{10.1145/3638243,
author = {Oakes, Bentley James and Famelis, Michalis and Sahraoui, Houari},
title = {Building Domain-Specific Machine Learning Workflows: A Conceptual Framework for the State of the Practice},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3638243},
doi = {10.1145/3638243},
abstract = {Domain experts are increasingly employing machine learning to solve their domain-specific problems. This article presents to software engineering researchers the six key challenges that a domain expert faces in addressing their problem with a computational workflow, and the underlying executable implementation. These challenges arise out of our conceptual framework which presents the “route” of transformations that a domain expert may choose to take while developing their solution.To ground our conceptual framework in the state of the practice, this article discusses a selection of available textual and graphical workflow systems and their support for the transformations described in our framework. Example studies from the literature in various domains are also examined to highlight the tools used by the domain experts as well as a classification of the domain specificity and machine learning usage of their problem, workflow, and implementation.The state of the practice informs our discussion of the six key challenges, where we identify which challenges and transformations are not sufficiently addressed by available tools. We also suggest possible research directions for software engineering researchers to increase the automation of these tools and disseminate best-practice techniques between software engineering and various scientific domains.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {91},
numpages = {50},
keywords = {Computational workflow, workflow composition, domain experts, machine learning, machine learning pipelines, software engineering framework}
}

@article{10.1145/3705306,
author = {Reiss, Steven P. and Wei, Xuan and Yuan, Jiahao and Xin, Qi},
title = {ROSE: An IDE-Based Interactive Repair Framework for Debugging},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705306},
doi = {10.1145/3705306},
abstract = {Debugging is costly. Automated program repair (APR) holds the promise of reducing its cost by automatically fixing errors. However, current techniques are not easily applicable in a realistic debugging scenario because they assume a high-quality test suite and frequent program re-execution, have low repair efficiency, and only handle a limited set of errors. To improve the practicality of APR for debugging, we propose ROSE, an interactive repair framework that is able to suggest quick and effective repairs of semantic errors while debugging in an Integrated Development Environment (IDE). ROSE allows an easy integration of existing APR patch generators and can do program repair without assuming the existence of a test suite and without requiring program re-execution. It works in conjunction with an IDE debugger and assumes a debugger stopping point where a problem symptom is observed. ROSE asks the developer to quickly describe the symptom. Then it uses the stopping point, the identified symptom, and the current environment to identify potentially faulty lines, uses a variety of APR techniques to suggest repairs at those lines, and validates those repairs without re-executing the program. Finally, it presents the results so the developer can examine, select, and make the appropriate repair. ROSE uses novel approaches to achieve effective fault localization and patch validation without a test suite or program re-execution. For fault localization, ROSE builds on a fast abstract interpretation-based flow analysis to compute a static backward slice approximating the real dynamic slice while taking into account the symptom and the current execution. For patch validation without re-running the program, ROSE generates simulated traces based on a live-programming system for both the original and repaired executions and compares the traces with respect to the problem symptoms to infer patch correctness. We implemented a prototype of ROSE that works in an Eclipse-based IDE and evaluated its potency and utility with an effectiveness study and a user study. We found that ROSE’s fault localization and validation are highly effective and a ROSE-based tool using existing APR patch generators generated correct repair suggestions for many errors in only seconds. Moreover, the user study demonstrated that ROSE was helpful for debugging and developers liked to use it.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {112},
numpages = {39},
keywords = {Debugging, Interactive Repair Framework, Automated Program Repair, Integrated Development Environment}
}

@inproceedings{10.1145/3652620.3686248,
author = {Bonetti, Federico and Bucchiarone, Antonio and Michael, Judith and Cicchetti, Antonio and Marconi, Annapaola and Rumpe, Bernhard},
title = {Digital Twins of Socio-Technical Ecosystems to Drive Societal Change},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3686248},
doi = {10.1145/3652620.3686248},
abstract = {While the engineering of digital twins (DTs) of cyber-physical systems already faces a number of challenges, DTs of socio-technical systems are made even more complex by human and social factors, and a comprehensive representation of their internal relations is currently lacking. DTs for socio-technical systems could open up new ways of achieving common societal goals by i) providing an understanding of complex interactions and processes, and by ii) facilitating the design of and participation in collective actions. In this context, dynamic adaptation and motivational strategies would be required to swiftly address sub-optimal system behavior. To enable the model-driven engineering of DTs responding to such requirements, we propose a conceptual model of socio-technical systems and discuss it with use-case scenarios. The presented approach supports our vision of future DT-based model-driven interventions, empowering citizens and stakeholders in driving societal change and increasing community resilience.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {275–286},
numpages = {12},
keywords = {digital twin, modeling, socio-technical system, model-driven engineering, system engineering},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3583780.3614870,
author = {Tiwari, Abhisek and Saha, Anisha and Saha, Sriparna and Bhattacharyya, Pushpak and Dhar, Minakshi},
title = {Experience and Evidence are the eyes of an excellent summarizer! Towards Knowledge Infused Multi-modal Clinical Conversation Summarization},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614870},
doi = {10.1145/3583780.3614870},
abstract = {With the advancement of telemedicine, both researchers and medical practitioners are working hand-in-hand to develop various techniques to automate various medical operations, such as diagnosis report generation. In this paper, we first present a multi-modal clinical conversation summary generation task that takes a clinician-patient interaction (both textual and visual information) and generates a succinct synopsis of the conversation. We propose a knowledge-infused, multi-modal, multi-tasking medical domain identification and clinical conversation summary generation (MM-CliConSummation) framework. It leverages an adapter to infuse knowledge and visual features and unify the fused feature vector using a gated mechanism. Furthermore, we developed a multi-modal, multi-intent clinical conversation summarization corpus annotated with intent, symptom, and summary. The extensive set of experiments, both quantitatively and qualitatively, led to the following findings: (a) critical significance of visuals, (b) more precise and medical entity preserving summary with additional knowledge infusion, and (c) a correlation between medical department identification and clinical synopsis generation. Furthermore, the dataset and source code are available at https://github.com/NLP-RL/MM-CliConSummation},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2452–2461},
numpages = {10},
keywords = {multimodal infusion, multimodal medical dialogue summarization, online counselling, text generation},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3652620.3687800,
author = {Aslan O\u{g}uz, Evin and Kuester, Jochen Malte},
title = {A Comparative Analysis of ChatGPT-Generated and Human-Written Use Case Descriptions},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687800},
doi = {10.1145/3652620.3687800},
abstract = {The development of comprehensive use case descriptions is a critical task in software engineering, providing essential insights for requirement analysis and system design. The advent of advanced natural language processing models, such as ChatGPT, has sparked interest in their potential to automate tasks traditionally performed by humans, including the generation of use case descriptions in software engineering. Understanding the capabilities and limitations of ChatGPT in generating use case descriptions is crucial for software engineers. Without a clear understanding of its performance, practitioners may either overestimate its utility, leading to reliance on suboptimal drafts, or underestimate its capabilities, missing opportunities to streamline the drafting process. This paper addresses how well ChatGPT performs in generating use case descriptions, evaluating their quality compared to human-written descriptions. To do so, we employ a structured approach using established quality guidelines and the concept of "bad smells" for use case descriptions. Our study presents the first attempt to bridge the knowledge gap by offering a comparative analysis of ChatGPT-generated and human-written use case descriptions. By providing an approach to objectively assess ChatGPT's performance, we highlight its potential and limitations, offering software engineers insights to effectively integrate AI tools into their workflows.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {533–540},
numpages = {8},
keywords = {use case description, ChatGPT, requirements engineering, quality},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3643991.3644923,
author = {Chen, Binger and Golebiowski, Jacek and Abedjan, Ziawasch},
title = {Data Augmentation for Supervised Code Translation Learning},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644923},
doi = {10.1145/3643991.3644923},
abstract = {Data-driven program translation has been recently the focus of several lines of research. A common and robust strategy is supervised learning. However, there is typically a lack of parallel training data, i.e., pairs of code snippets in the source and target language. While many data augmentation techniques exist in the domain of natural language processing, they cannot be easily adapted to tackle code translation due to the unique restrictions of programming languages. In this paper, we develop a novel rule-based augmentation approach tailored for code translation data, and a novel retrieval-based approach that combines code samples from unorganized big code repositories to obtain new training data. Both approaches are language-independent. We perform an extensive empirical evaluation on existing Java-C#-benchmarks showing that our method improves the accuracy of state-of-the-art supervised translation techniques by up to 35\%.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {444–456},
numpages = {13},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3729352,
author = {Chen, Wei-Hao and Cheoh, Jia Lin and Keim, Manthan and Brunswicker, Sabine and Zhang, Tianyi},
title = {Towards Understanding Fine-Grained Programming Mistakes and Fixing Patterns in Data Science},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729352},
doi = {10.1145/3729352},
abstract = {Programming is an essential activity in data science (DS). Unlike regular software developers, DS programmers often use Jupyter notebooks instead of conventional IDEs. Moreover, DS programmers focus on statistics, data analytics, and modeling rather than writing production-ready code following best practices in software engineering. Thus, in order to provide effective tool support to improve their productivity, it is important to understand what kinds of errors they make and how they fix them. Previous studies have analyzed DS code from public code-sharing platforms such as GitHub and Kaggle. However, they only accounted for code changes committed to the version history, omitting many programming mistakes that are resolved before code commits. To bridge the gap, we present an in-depth analysis of the fine-grained logs of a DS competition, which includes 390 Jupyter Notebooks written by participants over six weeks. In addition, we conducted semi-structured interviews with 10 DS programmers from different domains to understand the reasons behind their programming mistakes. We identified several unique programming mistakes and fix patterns that had not been reported before, highlighting opportunities for designing new tool support for DS programming.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE082},
numpages = {23},
keywords = {Computational Notebook, Data Science, Programming Practice}
}

@article{10.1145/3664522,
author = {Tsiakas, Konstantinos and Murray-Rust, Dave},
title = {Unpacking Human-AI interactions: From Interaction Primitives to a Design Space},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/3664522},
doi = {10.1145/3664522},
abstract = {This article aims to develop a semi-formal representation for Human-AI (HAI) interactions, by building a set of interaction primitives which can specify the information exchanges between users and AI systems during their interaction. We show how these primitives can be combined into a set of interaction patterns which can capture common interactions between humans and AI/ML models. The motivation behind this is twofold: firstly, to provide a compact generalization of existing practices for the design and implementation of HAI interactions; and secondly, to support the creation of new interactions by extending the design space of HAI interactions. Taking into consideration frameworks, guidelines, and taxonomies related to human-centered design and implementation of AI systems, we define a vocabulary for describing information exchanges based on the model’s characteristics and interactional capabilities. Based on this vocabulary, a message passing model for interactions between humans and models is presented, which we demonstrate can account for existing HAI interaction systems and approaches. Finally, we build this into design patterns which can describe common interactions between users and models, and we discuss how this approach can be used toward a design space for HAI interactions that creates new possibilities for designs as well as keeping track of implementation issues and concerns.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = aug,
articleno = {18},
numpages = {51},
keywords = {Human-AI interaction, interaction patterns, explainable AI, human-in-the-loop, hybrid intelligence}
}

@inproceedings{10.1145/3637528.3671801,
author = {Zhang, Wentao and Zhao, Lingxuan and Xia, Haochong and Sun, Shuo and Sun, Jiaze and Qin, Molei and Li, Xinyi and Zhao, Yuqing and Zhao, Yilei and Cai, Xinyu and Zheng, Longtao and Wang, Xinrun and An, Bo},
title = {A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671801},
doi = {10.1145/3637528.3671801},
abstract = {Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes. The agent's emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 12 state-of-the-art baselines in terms of 6 financial metrics with over 36\% average improvement on profit. Specifically, a 92.27\% return (a 84.39\% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4314–4325},
numpages = {12},
keywords = {financial ai agents, large language models, quantitative trading},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3643832.3661892,
author = {Jia, Fucheng and Jiang, Shiqi and Cao, Ting and Cui, Wei and Xia, Tianrui and Cao, Xu and Li, Yuanchun and Wang, Qipeng and Zhang, Deyu and Ren, Ju and Liu, Yunxin and Qiu, Lili and Yang, Mao},
title = {Empowering In-Browser Deep Learning Inference on Edge Through Just-In-Time Kernel Optimization},
year = {2024},
isbn = {9798400705816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643832.3661892},
doi = {10.1145/3643832.3661892},
abstract = {Web is increasingly becoming the primary platform to deliver AI services onto edge devices, making in-browser deep learning (DL) inference more prominent. Nevertheless, the heterogeneity of edge devices, combined with the underdeveloped state of Web hardware acceleration practices, hinders current in-browser inference from achieving its full performance potential on target devices.To address this issue, this paper presents the pioneering inbrowser inference system, nnJIT, which enables just-in-time (JIT) auto-generation of optimized computing kernels for edge devices. nnJIT is built upon two novel techniques that significantly reduce kernel search and compilation overhead while improving performance firmly: Tensor-Web Compiling Co-Design lowers compiling costs by around 100\texttimes{} through eliminating redundant and ineffective compiling passes; Web-Specific Lite Kernel Optimization Space reduces kernel tuning costs by focusing on Web programming requirements and efficient device resource utilization, pruning the optimization space from millions to only dozens.nnJIT1 is evaluated for modern models, e.g., BART, T5, and Llama 2, on a range of edge devices including laptops and smartphones using different browsers and hardware from ARM, Intel, AMD and Nvidia. The results show that nnJIT can achieve up to 8.2\texttimes{} faster within 30 seconds compared to the existing baselines.},
booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services},
pages = {438–450},
numpages = {13},
keywords = {in-browser deep learning inference, just-in-time kernel optimizations, WebAssembly, WebGPU},
location = {Minato-ku, Tokyo, Japan},
series = {MOBISYS '24}
}

@article{10.1145/3679200,
author = {Liao, Weibin and Zhu, Yifan and Li, Yanyan and Zhang, Qi and Ou, Zhonghong and Li, Xuesong},
title = {RevGNN: Negative Sampling Enhanced Contrastive Graph Learning for Academic Reviewer Recommendation},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3679200},
doi = {10.1145/3679200},
abstract = {Acquiring reviewers for academic submissions is a challenging recommendation scenario. Recent graph learning-driven models have made remarkable progress in the field of recommendation, but their performance in the academic reviewer recommendation task may suffer from a significant false negative issue. This arises from the assumption that unobserved edges represent negative samples. In fact, the mechanism of anonymous review results in inadequate exposure of interactions between reviewers and submissions, leading to a higher number of unobserved interactions compared to those caused by reviewers declining to participate. Therefore, investigating how to better comprehend the negative labeling of unobserved interactions in academic reviewer recommendations is a significant challenge. This study aims to tackle the ambiguous nature of unobserved interactions in academic reviewer recommendations. Specifically, we propose an unsupervised Pseudo Neg-Label strategy to enhance graph contrastive learning (GCL) for recommending reviewers for academic submissions, which we call RevGNN. RevGNN utilizes a two-stage encoder structure that encodes both scientific knowledge and behavior using Pseudo Neg-Label to approximate review preference. Extensive experiments on three real-world datasets demonstrate that RevGNN outperforms all baselines across four metrics. Additionally, detailed further analyses confirm the effectiveness of each component in RevGNN.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {1},
numpages = {26},
keywords = {Academic reviewer recommendation, expert finding, GNN-based recommendation, negative sampling in GCL}
}

@inproceedings{10.1145/3691620.3694986,
author = {Xiong, Yiheng and Su, Ting and Wang, Jue and Sun, Jingling and Pu, Geguang and Su, Zhendong},
title = {General and Practical Property-based Testing for Android Apps},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3694986},
doi = {10.1145/3691620.3694986},
abstract = {Finding non-crashing functional bugs for Android apps is challenging for both manual testing and automated GUI testing techniques. This paper introduces and designs a general and practical testing technique based on the idea of property-based testing for finding such bugs. Specifically, our technique incorporates (1) a property description language (PDL) to allow specifying desired app properties, and (2) two exploration strategies as the input generators for effectively validating the properties. We implemented our technique as a tool named Kea and evaluated it on 124 historical bugs from eight real-world, popular Android apps. Our evaluation shows that our PDL can specify all the app properties violated by these historical bugs, demonstrating its generability for finding functional bugs. Kea successfully found 66 (68.0\%) and 92 (94.8\%) of the 97 historical bugs in scope under the two exploration strategies, demonstrating its practicability. Moreover, Kea found 25 new functional bugs on the latest versions of these eight apps, given the specified properties. To date, all these bugs have been confirmed, and 21 have been fixed. In comparison, prior state-of-the-art techniques found only 13 (13.4\%) historical bugs and 1 new bug. We have made all the artifacts publicly available at https://github.com/ecnusse/Kea.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {53–64},
numpages = {12},
keywords = {property-based testing, Android app testing, non-crashing functional bugs},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3611643.3616322,
author = {Wang, Bo and Li, Ruishi and Li, Mingkai and Saxena, Prateek},
title = {TransMap: Pinpointing Mistakes in Neural Code Translation},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616322},
doi = {10.1145/3611643.3616322},
abstract = {Automated code translation between programming languages can greatly reduce the human effort needed in learning new languages or in migrating code. Recent neural machine translation models, such as Codex, have been shown to be effective on many code generation tasks including translation. However, code produced by neural translators often has semantic mistakes. These mistakes are difficult to eliminate from the neural translator itself because the translator is a black box, which is difficult to interpret or control compared to rule-based transpilers. We propose the first automated approach to pinpoint semantic mistakes in code obtained after neural code translation. Our techniques are implemented in a prototype tool called TransMap which translates Python to JavaScript, both of which are popular scripting languages. On our created micro-benchmarks of Python programs with 648 semantic mistakes in total, TransMap accurately pinpoints the correct location for a fix for 87.96\%, often highlighting 1-2 lines for the user to inspect per mistake. We report on our experience in translating 5 Python libraries with up to 1k lines of code with TransMap. Our preliminary user study suggests that TransMap can reduce the time for fixing semantic mistakes by around 70\% compared to using a standard IDE with debuggers.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {999–1011},
numpages = {13},
keywords = {Code Translation, Large Language Models, Semantic Mistakes},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3597503.3639130,
author = {Keim, Jan and Corallo, Sophie and Fuch\ss{}, Dominik and Hey, Tobias and Telge, Tobias and Koziolek, Anne},
title = {Recovering Trace Links Between Software Documentation And Code},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639130},
doi = {10.1145/3597503.3639130},
abstract = {Introduction Software development involves creating various artifacts at different levels of abstraction and establishing relationships between them is essential. Traceability link recovery (TLR) automates this process, enhancing software quality by aiding tasks like maintenance and evolution. However, automating TLR is challenging due to semantic gaps resulting from different levels of abstraction. While automated TLR approaches exist for requirements and code, architecture documentation lacks tailored solutions, hindering the preservation of architecture knowledge and design decisions. Methods This paper presents our approach TransArC for TLR between architecture documentation and code, using component-based architecture models as intermediate artifacts to bridge the semantic gap. We create transitive trace links by combining the existing approach ArDoCo for linking architecture documentation to models with our novel approach ArCoTL for linking architecture models to code.Results We evaluate our approaches with five open-source projects, comparing our results to baseline approaches. The model-to-code TLR approach achieves an average F1-score of 0.98, while the documentation-to-code TLR approach achieves a promising average F1-score of 0.82, significantly outperforming baselines. Conclusion Combining two specialized approaches with an intermediate artifact shows promise for bridging the semantic gap. In future research, we will explore further possibilities for such transitive approaches.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {215},
numpages = {13},
keywords = {software traceability, software architecture, documentation, transitive links, intermediate artifacts, information retrieval},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/3666005,
author = {Metzger, Andreas and Laufer, Jan and Feit, Felix and Pohl, Klaus},
title = {A User Study on Explainable Online Reinforcement Learning for Adaptive Systems},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3666005},
doi = {10.1145/3666005},
abstract = {Online reinforcement learning (RL) is increasingly used for realizing adaptive systems in the presence of design time uncertainty because Online RL can leverage data only available at run time. With Deep RL gaining interest, the learned knowledge is no longer represented explicitly but hidden in the parameterization of the underlying artificial neural network. For a human, it thus becomes practically impossible to understand the decision-making of Deep RL, which makes it difficult for (1) software engineers to perform debugging, (2) system providers to comply with relevant legal frameworks, and (3) system users to build trust. The explainable RL technique XRL-DINE, introduced in earlier work, provides insights into why certain decisions were made at important time steps. Here, we perform an empirical user study concerning XRL-DINE involving 73 software engineers split into treatment and control groups. The treatment group is given access to XRL-DINE, while the control group is not. We analyze (1) the participants’ performance in answering concrete questions related to the decision-making of Deep RL, (2) the participants’ self-assessed confidence in giving the right answers, (3) the perceived usefulness and ease of use of XRL-DINE, and (4) the concrete usage of the XRL-DINE dashboard.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = sep,
articleno = {15},
numpages = {44},
keywords = {adaptive system, machine learning, reinforcement learning, explainability, interpretability, debugging}
}

@article{10.1145/3709374,
author = {Thiagarajan, Kedar and Carisimo, Esteban and Bustamante, Fabi\'{a}n E.},
title = {The Aleph: Decoding Geographic Information from DNS PTR Records Using Large Language Models},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CoNEXT1},
url = {https://doi.org/10.1145/3709374},
doi = {10.1145/3709374},
abstract = {Geolocating network devices is essential for various research areas. Yet, despite notable advancements, it continues to be one of the most challenging issues for experimentalists. An approach for geolocating that has proved effective is leveraging geolocating hints in PTR records associated with network devices. Extracting and interpreting geo-hints from PTR records is challenging because the labels are primarily intended for human interpretation rather than computational processing. Additionally, a lack of standardization across operators -- and even within a single operator, due to factors like rebranding, mergers, and acquisitions -- complicates the process. We argue that Large Language Models (LLMs), rather than humans, are better equipped to identify patterns in DNS PTR records, and significantly scale the coverage of tools like Hoiho. We introduce The Aleph, an approach and system for network device geolocation that utilizes information embedded in PTR records. The Aleph leverages LLMs to classify PTR records, generate regular expressions for these classes, and establish hint-to-location mapping per operator. We present results showing the applicability of using LLMs as a scalable approach to leverage PTR records for infrastructure geolocation.},
journal = {Proc. ACM Netw.},
month = mar,
articleno = {7},
numpages = {20},
keywords = {dns ptr records, internet geolocation, large language models (llms)}
}

@inproceedings{10.1145/3658644.3690357,
author = {Gumusel, Ece and Xiao, Yue and Qin, Yue and Qin, Jiaxin and Liao, Xiaojing},
title = {Understanding Legal Professionals' Practices and Expectations in Data Breach Incident Reporting},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3690357},
doi = {10.1145/3658644.3690357},
abstract = {Legal professionals are essential in analyzing data breach incident reports and guiding the response to comply with data privacy laws and regulations. Their expertise helps mitigate privacy and security risks and prevents failures in privacy compliance. However, little research has been done to understand how legal professionals perceive, react to, and face challenges within the data breach incident reporting procedure. In this study, we conducted a simulated incident report assessment experiment and semi-structured interviews with 33 legal professionals who varied in age, gender, and legal background. We reported the criteria used by legal professionals to identify privacy-related items and also uncovered that the agreement among legal professionals on the concepts of privacy-related items is low. Furthermore, we presented findings regarding the perceptions and strategies of legal professionals concerning legal and regulatory compliance, as well as the key features of incident responses that facilitate efficient analysis of data privacy and security law compliance. After taking into account the challenges and suggestions provided by legal professionals, we concluded this study with recommendations for enhancing the effectiveness of legal compliance analysis for incident responses.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {2711–2725},
numpages = {15},
keywords = {GDPR compliance, data breach incident reporting, legal professionals' practices, security and privacy in legal contexts},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3661167.3661220,
author = {G\'{o}mez-Abajo, Pablo and P\'{e}rez-Soler, Sara and Ca\~{n}izares, Pablo C. and Guerra, Esther and de Lara, Juan},
title = {Mutation Testing for Task-Oriented Chatbots},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661220},
doi = {10.1145/3661167.3661220},
abstract = {Conversational agents, or chatbots, are increasingly used to access all sorts of services using natural language. While open-domain chatbots – like ChatGPT – can converse on any topic, task-oriented chatbots – the focus of this paper – are designed for specific tasks, like booking a flight, obtaining customer support, or setting an appointment. Like any other software, task-oriented chatbots need to be properly tested, usually by defining and executing test scenarios (i.e., sequences of user-chatbot interactions). However, there is currently a lack of methods to quantify the completeness and strength of such test scenarios, which can lead to low-quality tests, and hence to buggy chatbots. To fill this gap, we propose adapting mutation testing (MuT) for task-oriented chatbots. To this end, we introduce a set of mutation operators that emulate faults in chatbot designs, an architecture that enables MuT on chatbots built using heterogeneous technologies, and a practical realisation as an Eclipse plugin. Moreover, we evaluate the applicability, effectiveness and efficiency of our approach on open-source chatbots, with promising results.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {232–241},
numpages = {10},
keywords = {Botium, Dialogflow, Mutation testing, Rasa, Task-oriented chatbots},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3694715.3695952,
author = {Lattuada, Andrea and Hance, Travis and Bosamiya, Jay and Brun, Matthias and Cho, Chanhee and LeBlanc, Hayley and Srinivasan, Pranav and Achermann, Reto and Chajed, Tej and Hawblitzel, Chris and Howell, Jon and Lorch, Jacob R. and Padon, Oded and Parno, Bryan},
title = {Verus: A Practical Foundation for Systems Verification},
year = {2024},
isbn = {9798400712517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3694715.3695952},
doi = {10.1145/3694715.3695952},
abstract = {Formal verification is a promising approach to eliminate bugs at compile time, before they ship. Indeed, our community has verified a wide variety of system software. However, much of this success has required heroic developer effort, relied on bespoke logics for individual domains, or sacrificed expressiveness for powerful proof automation.Building on prior work on Verus, we aim to enable faster, cheaper verification of rich properties for realistic systems. We do so by integrating and optimizing the best choices from prior systems, tuning our design to overcome barriers encountered in those systems, and introducing novel techniques.We evaluate Verus's effectiveness with a wide variety of case-study systems, including distributed systems, an OS page table, a library for NUMA-aware concurrent data structure replication, a crash-safe storage system, and a concurrent memory allocator, together comprising 6.1K lines of implementation and 31K lines of proof. Verus verifies code 3--61\texttimes{} faster and with less effort than the state of the art.Our results suggest that Verus offers a platform for exploring the next frontiers in system-verification research. Because Verus builds on Rust, Verus is also positioned for wider use in production by developers who have already adopted Rust in the pursuit of more robust systems.},
booktitle = {Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
pages = {438–454},
numpages = {17},
location = {Austin, TX, USA},
series = {SOSP '24}
}

@article{10.14778/3636218.3636234,
author = {Zhang, Hailin and Zhao, Penghao and Miao, Xupeng and Shao, Yingxia and Liu, Zirui and Yang, Tong and Cui, Bin},
title = {Experimental Analysis of Large-Scale Learnable Vector Storage Compression},
year = {2023},
issue_date = {December 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3636218.3636234},
doi = {10.14778/3636218.3636234},
abstract = {Learnable embedding vector is one of the most important applications in machine learning, and is widely used in various database-related domains. However, the high dimensionality of sparse data in recommendation tasks and the huge volume of corpus in retrieval-related tasks lead to a large memory consumption of the embedding table, which poses a great challenge to the training and deployment of models. Recent research has proposed various methods to compress the embeddings at the cost of a slight decrease in model quality or the introduction of other overheads. Nevertheless, the relative performance of these methods remains unclear. Existing experimental comparisons only cover a subset of these methods and focus on limited metrics. In this paper, we perform a comprehensive comparative analysis and experimental evaluation of embedding compression. We introduce a new taxonomy that categorizes these techniques based on their characteristics and methodologies, and further develop a modular benchmarking framework that integrates 14 representative methods. Under a uniform test environment, our benchmark fairly evaluates each approach, presents their strengths and weaknesses under different memory budgets, and recommends the best method based on the use case. In addition to providing useful guidelines, our study also uncovers the limitations of current methods and suggests potential directions for future research.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {808–822},
numpages = {15}
}

@inproceedings{10.1145/3613904.3642172,
author = {Zhou, Tongyu and Huang, Jeff and Chan, Gromit Yeuk-Yin},
title = {Epigraphics: Message-Driven Infographics Authoring},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642172},
doi = {10.1145/3613904.3642172},
abstract = {The message a designer wants to convey plays a pivotal role in directing the design of an infographic, yet most authoring workflows start with creating the visualizations or graphics first without gauging whether they fit the message. To address this gap, we propose Epigraphics, a web-based authoring system that treats an “epigraph” as the first-class object, and uses it to guide infographic asset creation, editing, and syncing. The system uses the text-based message to recommend visualizations, graphics, data filters, color palettes, and animations. It further supports between-asset interactions and fine-tuning such as recoloring, highlighting, and animation syncing that enhance the aesthetic cohesiveness of the assets. A gallery and case studies show that our system can produce infographics inspired by existing popular ones, and a task-based usability study with 10 designers show that a text-sourced workflow can standardize content, empower users to think more about the big picture, and facilitate rapid prototyping.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {200},
numpages = {18},
keywords = {data visualization, infographics authoring, visual storytelling},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3652620.3687806,
author = {Burgue\~{n}o, Lola and Keet, Maria and Kienzle, J\"{o}rg and Michael, Judith and Babur, \"{O}nder},
title = {A Human Behavior Exploration Approach Using LLMs for Cyber-Physical Systems},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687806},
doi = {10.1145/3652620.3687806},
abstract = {In the early phases of Cyber-Physical Systems (CPS) development, scoping human behavior plays a significant role, especially when interactions extend beyond expected behavior. Here, it is especially challenging to develop cases that capture the full spectrum of human behavior. Up to now, identifying such behavior of humans remains a task for domain experts. We explore how one can use Large Languages Models (LLMs) in the design phase of systems to provide additional information about human-CPS interaction. Our approach proposes a preliminary ontology describing a hierarchy of types of behavior and relevant CPS components as input for prompt templates. It uses them to generate parts of human behavior descriptions, as well as a canned prompt with one variable about behavior. For demonstration, we take a smart building with a Home Energy System as the use case.An initial user evaluation shows that the behavior descriptions generated with standard and ontology-driven prompts complement each other and are useful when assisting humans. The discovered uncommon behaviors can be used to complete interaction scenarios that eventually result in a more robust CPS implementation.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {578–586},
numpages = {9},
keywords = {human behavior, large language models, cyber-physical systems, user scenario, digital twin},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.5555/3712729.3712738,
author = {Haas, Peter J.},
title = {Tutorial: Artificial Neural Networks for Discrete-Event Simulation},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {This advanced tutorial explores some recent applications of artificial neural networks (ANNs) to stochastic discrete-event simulation (DES). We first review some basic concepts and then give examples of how ANNs are being used in the context of DES to facilitate simulation input modeling, random variate generation, simulation metamodeling, optimization via simulation, and more. Combining ANNs and DES allows exploitation of the deep domain knowledge embodied in simulation models while simultaneously leveraging the ability of modern ML techniques to capture complex patterns and relationships in data.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {116–130},
numpages = {15},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@article{10.14778/3685800.3685810,
author = {Yi, Peng and Liang, Lei and Zhang, Da and Chen, Yong and Zhu, Jinye and Liu, Xiangyu and Tang, Kun and Chen, Jialin and Lin, Hao and Qiu, Leijie and Zhou, Jun},
title = {KGFabric: A Scalable Knowledge Graph Warehouse for Enterprise Data Interconnection},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685810},
doi = {10.14778/3685800.3685810},
abstract = {Based on the diversified application scenarios at Ant Group, we built the Ant Knowledge Graph Platform (AKGP). It has constructed numerous domain-specific knowledge graphs related to merchants, companies, accounts, products, and more. AKGP manages trillions of structured knowledge graphs, serving search, recommendation, risk control and other businesses. However, as the demand increasing for various workloads such as graph pattern matching, graph representation learning, and cross-domain knowledge reuse, the existing warehouse systems based on relational DBMS or graph databases are unable to meet the requirements. To address these issues, we propose KGFabric, an industrial-scale knowledge graph management system built on the distributed file system (DFS). KGFabric offers a nearline knowledge storage engine that utilizes a Semantic-enhanced Programmable Graph (SPG) model, which is compatible with the Labeled Property Graph (LPG) model. The data is persistently stored in DFS, such as HDFS, which leverages the POSIX file system API, making it suitable for deployment in multi-cloud environment at low cost. KGFabric provides a native graph-based and hybrid storage format that can serve as a shared backend for parallel graph computing systems, significantly accelerating the analysis of multi-workload. Additionally, KGFabric includes a graph fabric framework that minimizes data duplication and guarantees data security.KGFabric is able to manage Peta-scale data and has supported graph fabric and analysis with over 100 billion relations at Ant Group. We conduct experiments on various datasets to evaluate the performance of KGFabric. Compared with popular relational DBMS and graph databases, the storage space for semantic relations is reduced by over 90\%. The performance of graph fabric improves by 21\texttimes{} in real-world workloads. In multi-hop semantic graph analysis, KGFabric enhances performance by 100\texttimes{}.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3841–3854},
numpages = {14}
}

@inproceedings{10.1145/3726302.3730277,
author = {Chen, Qiaosheng and Huang, Kaijia and Zhou, Xiao and Luo, Weiqing and Cui, Yuanning and Cheng, Gong},
title = {Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730277},
doi = {10.1145/3726302.3730277},
abstract = {The rapid growth of open source machine learning (ML) resources, such as models and datasets, has accelerated IR research. However, existing platforms like Hugging Face do not explicitly utilize structured representations, limiting advanced queries and analyses such as tracing model evolution and recommending relevant datasets. To fill the gap, we construct HuggingKG, the first large-scale knowledge graph built from the Hugging Face community for ML resource management. With 2.6 million nodes and 6.2 million edges, HuggingKG captures domain-specific relations and rich textual attributes. It enables us to further present HuggingBench, a multi-task benchmark with three novel test collections for IR tasks including resource recommendation, classification, and tracing. Our experiments reveal unique characteristics of HuggingKG and the derived tasks. Both resources are publicly available, expected to advance research in open source resource sharing and management.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3433–3443},
numpages = {11},
keywords = {hugging face, knowledge graph, model tracing, resource recommendation, task classification, test collection},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3698205.3729558,
author = {Malik, Rizwaan and Hao, Rebecca Li and Kacholia, Ritika and Demszky, Dorottya},
title = {MathemaTikZ: A Dataset and Benchmark for Mathematical Diagram Generation},
year = {2025},
isbn = {9798400712913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698205.3729558},
doi = {10.1145/3698205.3729558},
abstract = {Diagrams play a fundamental role in mathematics education, serving both as essential components of mathematical problems and as powerful scaffolding tools to support student comprehension. While AI tools have shown promise in supporting teachers with lesson preparation, especially with text-based mathematical content, they still struggle with reliably generating visual diagrams. Our work makes two main contributions: (1) We introduce MathemaTikZ, a dataset derived from the Illustrative Mathematics curriculum, comprising 3,793 mathematical diagrams paired with their natural language descriptions, problem contexts, and TikZ implementations. These span the full range of diagrams utilized in the K12 math curriculum. (2) We conduct comprehensive baseline evaluations using state-of-the-art language models (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) to assess current capabilities in mathematical diagram generation. Our findings reveal that even the best-performing models achieve a 73.9\% success rate in accurately generating mathematical diagrams, with performance varying significantly across different types of visualizations. Through detailed error analysis, we identify four key challenge areas that future work should address: spatial reasoning and element placement, adherence to geometric constraints, pedagogical knowledge of mathematical diagrams, and preservation of mathematical relationships. Our results establish baselines for mathematical diagram generation and highlight critical areas for improvement in making AI tools more effective for mathematics education.},
booktitle = {Proceedings of the Twelfth ACM Conference on Learning @ Scale},
pages = {95–104},
numpages = {10},
keywords = {curriculum development, large language models, mathematics education, visual generation},
location = {Palermo, Italy},
series = {L@S '25}
}

@inproceedings{10.1145/3713082.3730378,
author = {Tsai, Lillian and Bagdasarian, Eugene},
title = {Contextual Agent Security: A Policy for Every Purpose},
year = {2025},
isbn = {9798400714757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3713082.3730378},
doi = {10.1145/3713082.3730378},
abstract = {Judging an action's safety requires knowledge of the context in which the action takes place. To human agents who act in various contexts, this may seem obvious: performing an action such as email deletion may or may not be appropriate depending on the email's content, the goal (e.g., erase sensitive emails or clean up trash), and the type of email address (e.g., work or personal). Unlike people, computational systems have often had only limited agency in limited contexts. Thus, manually crafted policies and user confirmation such as smartphone app permissions or access control lists---while imperfect---have sufficed to restrict harmful actions. However, with the upcoming deployment of generalist agents that support a multitude of tasks (e.g., an automated personal assistant), we argue that we must rethink security designs to adapt to the scale of contexts and capabilities of these systems. As a first step, this paper explores contextual security in the domain of agents and proposes contextual agent security (Conseca), a framework to generate just-in-time, contextual, and human-verifiable security policies.},
booktitle = {Proceedings of the 2025 Workshop on Hot Topics in Operating Systems},
pages = {8–17},
numpages = {10},
location = {Banff, AB, Canada},
series = {HotOS '25}
}

@inproceedings{10.1145/3604237.3627721,
author = {Kim, Seonmi and Kim, Seyoung and Kim, Yejin and Park, Junpyo and Kim, Seongjin and Kim, Moolkyeol and Sung, Chang Hwan and Hong, Joohwan and Lee, Yongjae},
title = {LLMs Analyzing the Analysts: Do BERT and GPT Extract More Value from Financial Analyst Reports?},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3627721},
doi = {10.1145/3604237.3627721},
abstract = {This paper examines the use of Large Language Models (LLMs), specifically BERT-based models and GPT-3.5, in the sentiment analysis of Korean financial analyst reports. Due to the specialized language in these reports, traditional natural language processing techniques often prove insufficient, making LLMs a better alternative. These models are capable of understanding the complexity and subtlety of the language, allowing for a more nuanced interpretation of the data. We focus our study on the extraction of sentiment scores from these reports, using them to construct and test investment strategies. Given that Korean analyst reports present unique linguistic challenges and a significant ‘buy’ recommendation bias, we employ LLMs fine-tuned for the Korean language and Korean financial texts. The aim of this study is to investigate and compare the effectiveness of LLMs in enhancing the sentiment analysis of financial reports, and subsequently utilize the sentiment scores to construct and test investment strategies, thereby evaluating these models’ potential in extracting valuable insights from the reports. The code is available at https://github.com/msraask3.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {383–391},
numpages = {9},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@inproceedings{10.1145/3639478.3639814,
author = {Speth, Sandro},
title = {Architecture-Based Cross-Component Issue Management and Propagation Analysis},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639814},
doi = {10.1145/3639478.3639814},
abstract = {This paper addresses the challenge of issue management in complex, component-based software architectures. In these systems, issues in one component often propagate across the architecture along the call chains. Yet, traditional issue management systems (IMSs) are limited to the boundaries of a single component and lack mechanisms for managing issues concerning their architectural dependencies. We present Gropius, a novel method that enhances issue management by integrating issues in an architecture graph. Gropius allows semantically linking issues across different components, synchronizes changes with underlying IMSs like GitHub, and allows modeling the architecture ontologically by defining the components' semantics at runtime. We explore whether combining issue and architecture management improves the development of component-based architectures regarding issue management. We hypothesize that this method will improve the efficiency and effectiveness of identifying and resolving cross-component issues, maintaining a comprehensive view of the application's state.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {145–149},
numpages = {5},
keywords = {issue management, issue propagation analysis, component-based software architecture, model-based analysis},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3713082.3730395,
author = {Lazarek, Lukas and Jung, Seong-Heon and Lamprou, Evangelos and Li, Zekai and Narsipur, Anirudh and Zhao, Eric and Greenberg, Michael and Kallas, Konstantinos and Mamouras, Konstantinos and Vasilakis, Nikos},
title = {From Ahead-of- to Just-in-Time and Back Again: Static Analysis for Unix Shell Programs},
year = {2025},
isbn = {9798400714757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3713082.3730395},
doi = {10.1145/3713082.3730395},
abstract = {Shell programming is as prevalent as ever. It is also quite complex, due to the structure of shell programs, their use of opaque software components, and their complex interactions with the broader environment. As a result, even when exercising an abundance of care, shell developers discover devastating bugs in their programs only at runtime: at best, shell programs going wrong crash the execution of a long-running task; at worst, they silently corrupt the broader environment in which they execute---affecting user data, modifying system files, and rendering entire systems unusable. Could the shell's users enjoy the benefits of semantics-driven static analysis before their programs' execution---as offered by most other production languages?},
booktitle = {Proceedings of the 2025 Workshop on Hot Topics in Operating Systems},
pages = {88–95},
numpages = {8},
keywords = {Linux, Unix, inference, shell, static analysis, type systems},
location = {Banff, AB, Canada},
series = {HotOS '25}
}

@inproceedings{10.1145/3626772.3657833,
author = {Dong, Xingning and Feng, Zipeng and Zhou, Chunluan and Yu, Xuzheng and Yang, Ming and Guo, Qingpei},
title = {M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657833},
doi = {10.1145/3626772.3657833},
abstract = {We present a Recipe for Effective and Efficient zero-shot video-text Retrieval, dubbed M2-RAAP. Upon popular image-text models like CLIP, most current adaptation-based video-text pre-training methods are confronted by three major issues, i.e., noisy data corpus, time-consuming pre-training, and limited performance gain. Towards this end, we conduct a comprehensive study including four critical steps in video-text pre-training. Specifically, we investigate 1) data filtering and refinement, 2) video input type selection, 3) temporal modeling, and 4) video feature enhancement. We then summarize this empirical study into the M2-RAAP recipe, where our technical contributions lie in 1) the data filtering and text re-writing pipeline resulting in 1M high-quality bilingual video-text pairs, 2) the promotion of video inputs with key-frames to accelerate pre-training, and 3) the Auxiliary-Caption-Guided (ACG) strategy to enhance video features. We conduct extensive experiments by adapting three image-text foundation models on two refined video-text datasets from different languages, validating the robustness and reproducibility of M2-RAAP for adaptation-based pre-training. Results demonstrate that M2-RAAP yields superior performance with significantly less data (-90\%) and time consumption (-95\%), establishing a new SOTA on four English zero-shot retrieval datasets and two Chinese ones. Codebase and refined bilingual data annotations are available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2156–2166},
numpages = {11},
keywords = {auxiliary-caption-guided video feature enhancement, video key-frame extraction, video-text data filtering and refinement, video-text pre-training, zero-shot video-text retrieval},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3696443.3708924,
author = {Li, Long and Lai, Jianxin and Yuan, Peng and Sui, Tianxiang and Liu, Yan and Zhu, Qing and Zhang, Xiaojing and Xiao, Linjie and Chen, Wenguang and Xue, Jingling},
title = {ANT-ACE: An FHE Compiler Framework for Automating Neural Network Inference},
year = {2025},
isbn = {9798400712753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696443.3708924},
doi = {10.1145/3696443.3708924},
abstract = {Fully Homomorphic Encryption (FHE) facilitates computations on encrypted data without requiring access to the decryption key, offering substantial privacy benefits for deploying neural network applications in sensitive sectors such as healthcare and finance. Nonetheless, programming these applications within the FHE framework is complex and demands extensive cryptographic expertise to guarantee correctness, performance, and security.                                In this paper, we present ANT-ACE, a production-quality, open-source FHE compiler designed to automate neural network inference on encrypted data. ANT-ACE accepts ONNX models and generates C/C++ programs, leveraging its custom open-source FHE library. We explore the design challenges encountered in the development of ANT-ACE, which is engineered to support a variety of input formats and architectures across diverse FHE schemes through a novel Intermediate Representation (IR) that facilitates multiple levels of abstraction. Comprising 44,000 lines of C/C++ code, ANT-ACE efficiently translates ONNX models into C/C++ programs for encrypted inference on CPUs, specifically utilizing the RNS-CKKS scheme. Preliminary evaluations on a single CPU indicate that ANT-ACE achieves significant speed enhancements in ResNet models, surpassing expert manual implementations and fulfilling our design goals.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {193–208},
numpages = {16},
keywords = {Compilers, FHE, Neural Network Inference},
location = {Las Vegas, NV, USA},
series = {CGO '25}
}

@inproceedings{10.1145/3616855.3635825,
author = {akota, Marija and Peyrard, Maxime and West, Robert},
title = {Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635825},
doi = {10.1145/3616855.3635825},
abstract = {Generative language models (LMs) have become omnipresent across data science. For a wide variety of tasks, inputs can be phrased as natural language prompts for an LM, from whose output the solution can then be extracted. LM performance has consistently been increasing with model size---but so has the monetary cost of querying the ever larger models. Importantly, however, not all inputs are equally hard: some require larger LMs for obtaining a satisfactory solution, whereas for others smaller LMs suffice. Based on this fact, we design a framework for cost effective language model choice, called ''Fly-swat or cannon'' (FORC). Given a set of inputs and a set of candidate LMs, FORC judiciously assigns each input to an LM predicted to do well on the input according to a so-called meta-model, aiming to achieve high overall performance at low cost. The cost--performance tradeoff can be flexibly tuned by the user. Options include, among others, maximizing total expected performance (or the number of processed inputs) while staying within a given cost budget, or minimizing total cost while processing all inputs. We evaluate FORC on 14 datasets covering five natural language tasks, using four candidate LMs of vastly different size and cost. With FORC, we match the performance of the largest available LM while achieving a cost reduction of 63\%. Via our publicly available library, (https://github.com/epfl-dlab/forc) researchers as well as practitioners can thus save large amounts of money without sacrificing performance.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {606–615},
numpages = {10},
keywords = {cost-performance tradeoff, generative models, meta-modelling},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3586183.3606822,
author = {Pu, Kevin and Yang, Jim and Yuan, Angel and Ma, Minyi and Dong, Rui and Wang, Xinyu and Chen, Yan and Grossman, Tovi},
title = {DiLogics: Creating Web Automation Programs with Diverse Logics},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606822},
doi = {10.1145/3586183.3606822},
abstract = {Knowledge workers frequently encounter repetitive web data entry tasks, like updating records or placing orders. Web automation increases productivity, but translating tasks to web actions accurately and extending to new specifications is challenging. Existing tools can automate tasks that perform the same logical trace of UI actions (e.g., input text in each field in order), but do not support tasks requiring different executions based on varied input conditions. We present DiLogics, a programming-by-demonstration system that utilizes NLP to assist users in creating web automation programs that handle diverse specifications. DiLogics first semantically segments input data to structured task steps. By recording user demonstrations for each step, DiLogics generalizes the web macros to novel but semantically similar task requirements. Our evaluation showed that non-experts can effectively use DiLogics to create automation programs that fulfill diverse input instructions. DiLogics provides an efficient, intuitive, and expressive method for developing web automation programs satisfying diverse specifications.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {74},
numpages = {15},
keywords = {PBD, Web automation, neurosymbolic programming},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@article{10.1145/3622841,
author = {Crichton, Will and Gray, Gavin and Krishnamurthi, Shriram},
title = {A Grounded Conceptual Model for Ownership Types in Rust},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622841},
doi = {10.1145/3622841},
abstract = {Programmers learning Rust struggle to understand ownership types, Rust’s core mechanism for ensuring memory safety without garbage collection. This paper describes our attempt to systematically design a pedagogy for ownership types. First, we studied Rust developers’ misconceptions of ownership to create the Ownership Inventory, a new instrument for measuring a person’s knowledge of ownership. We found that Rust learners could not connect Rust’s static and dynamic semantics, such as determining why an ill-typed program would (or would not) exhibit undefined behavior. Second, we created a conceptual model of Rust’s semantics that explains borrow checking in terms of flow-sensitive permissions on paths into memory. Third, we implemented a Rust compiler plugin that visualizes programs under the model. Fourth, we integrated the permissions model and visualizations into a broader pedagogy of ownership by writing a new ownership chapter for The Rust Programming Language, a popular Rust textbook. Fifth, we evaluated an initial deployment of our pedagogy against the original version, using reader responses to the Ownership Inventory as a point of comparison. Thus far, the new pedagogy has improved learner scores on the Ownership Inventory by an average of 9},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {265},
numpages = {29},
keywords = {Rust, concept inventory, ownership types, program state visualization}
}

@inproceedings{10.1145/3696410.3714889,
author = {Gui, Yi and Li, Zhen and Wan, Yao and Shi, Yemin and Zhang, Hongyu and Chen, Bohua and Su, Yi and Chen, Dongping and Wu, Siyuan and Zhou, Xing and Jiang, Wenbin and Jin, Hai and Zhang, Xiangliang},
title = {WebCode2M: A Real-World Dataset for Code Generation from Webpage Designs},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714889},
doi = {10.1145/3696410.3714889},
abstract = {Automatically generating webpage code from webpage designs can significantly reduce the workload of front-end developers, and recent Multimodal Large Language Models (MLLMs) have shown promising potential in this area. However, our investigation reveals that most existing MLLMs are constrained by the absence of high-quality, large-scale, real-world datasets, resulting in inadequate performance in automated webpage code generation. To fill this gap, this paper introduces WebCode2M, a new dataset comprising 2.56 million instances, each containing a design image along with the corresponding webpage code and layout details. Sourced from real-world web resources, WebCode2M offers a rich and valuable dataset for webpage code generation across a variety of applications. The dataset quality is ensured by a scoring model that filters out instances with aesthetic deficiencies or other incomplete elements. To validate the effectiveness of WebCode2M, we introduce a baseline model based on the Vision Transformer (ViT), named WebCoder, and establish a benchmark for fair comparison. Additionally, we introduce a new metric, TreeBLEU, to measure the structural hierarchy recall. The benchmarking results demonstrate that our dataset significantly improves the ability of MLLMs to generate code from webpage designs, confirming its effectiveness and usability for future applications in front-end design tools. Finally, we highlight several practical challenges introduced by our dataset, calling for further research. The code and dataset are publicly available at our project homepage: https://webcode2m.github.io.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {1834–1845},
numpages = {12},
keywords = {code generation, dataset, design to code, ui automation},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3664646.3664774,
author = {Cook, Michael},
title = {The Art of Programming: Challenges in Generating Code for Creative Applications},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664774},
doi = {10.1145/3664646.3664774},
abstract = {Programming has been a key tool for artists and other creatives for decades, and the creative use of programming presents unique challenges, opportunities and perspectives for researchers considering how AI can be used to support coding more generally. In this paper we aim to motivate researchers to look deeper into some of these areas, by highlighting some interesting uses of programming in creative practices, suggesting new research questions posed by these spaces, and briefly raising important issues that work in this area may face.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {139–143},
numpages = {5},
keywords = {code generation, computational creativity, generative systems},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@article{10.1145/3643756,
author = {Deb, Sourav and Jain, Kush and van Tonder, Rijnard and Le Goues, Claire and Groce, Alex},
title = {Syntax Is All You Need: A Universal-Language Approach to Mutant Generation},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643756},
doi = {10.1145/3643756},
abstract = {While mutation testing has been a topic of academic interest for  decades, it is only recently that “real-world” developers, including  industry leaders such as Google and Meta, have adopted mutation  testing. We propose a new approach to the development of mutation  testing tools, and in particular the core challenge of  generating mutants. Current practice tends towards two  limited approaches to mutation generation: mutants are either (1)  generated at the bytecode/IR level, and thus neither human readable nor adaptable to source-level features of languages or projects, or  (2) generated at the source level by language-specific tools that are  hard to write and maintain, and in fact are often abandoned by both  developers and users. We propose instead that source-level mutation  generation is a special case of program transformation in  general, and that adopting this approach allows for a single tool that  can effectively generate source-level mutants for essentially  any programming language. Furthermore, by using parser  parser combinators many of the seeming limitations of an  any-language approach can be overcome, without the need to parse  specific languages. We compare this new  approach to mutation to existing tools, and demonstrate the advantages  of using parser parser combinators to improve on a regular-expression  based approach to generation. Finally, we show that our approach  can provide effective mutant generation even for a language for which  it lacks any language-specific operators, and that is not very similar  in syntax to any language it has been applied to previously.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {30},
numpages = {21},
keywords = {Mutants, Mutation Generation, Software Testing}
}

@inproceedings{10.1145/3660512.3665526,
author = {Zaharia, Raul and Gavrilut, Dragos and Mutu, Gheorghita and Lucanu, Dorel},
title = {Interactive Assistance in Malware Dissemination Detection and Analysis},
year = {2024},
isbn = {9798400706509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660512.3665526},
doi = {10.1145/3660512.3665526},
abstract = {Analysis of a complex cyber-security attack often involves a variety of tools, for each specific payload used in the attack. The information supplied by these tools must be soundly correlated to obtain a correct verdict. We propose a tool, GView, that is designed to investigate cyber-attacks by providing guided analysis for various file types using automatic artifact identification, extraction, coherent correlation&nbsp;&amp;&nbsp;inference, and meaningful&nbsp;&amp;&nbsp;intuitive views at different levels of granularity w.r.t. revealed information. The concept behind GView simplifies navigation through all payloads in a complex attack, streamlining the process for security researchers, and increasing the quality of analysis. Our evaluation shows that GView improves the analysis time of an attack by up to 90\% compared to conventional tools used in forensics. We show a scenario where GView is used to analyze a misleading email.},
booktitle = {Proceedings of the 1st Workshop on Security-Centric Strategies for Combating Information Disorder},
articleno = {7},
numpages = {6},
keywords = {complex binary analysis, forensics investigation, malware research, security operations center},
location = {Singapore, Singapore},
series = {SCID '24}
}

@article{10.1145/3730578,
author = {Silva, Geovana Ramos Sousa and Canedo, Edna Dias},
title = {Privacy in Chatbot Conversation-Driven Development: A Comprehensive Review and Requirements Proposal},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3730578},
doi = {10.1145/3730578},
abstract = {Ensuring data privacy is a major challenge for software developers, especially in chatbots, where balancing privacy protection with response quality is key, given the need for conversation-driven development and data protection regulations. This research identifies privacy requirements and techniques for chatbot development through a literature review, privacy policy analysis, and a practitioner survey. The methodology includes a Systematic Literature Review (SLR), an adapted Gray Literature Review (GLR), privacy requirement formulation, and validation via a survey. Based on the SLR and GLR, eight privacy requirements are proposed, covering personal information protection, user authentication, access control, secure communication, database safety, user rights empowerment, decentralized storage, and reliable infrastructure. Survey results highlight foundational measures like secure communication and scalable infrastructures as priorities, while advanced measures such as decentralized storage or privacy rights implementation scored lower due to complexity and cost. Practitioners also stressed clarity and verifiability, citing gaps in definitions, examples, and validation criteria as challenges to adoption.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {215},
numpages = {44},
keywords = {chatbot, privacy requirements, data protection, security, conversation-driven development}
}

@inproceedings{10.1145/3581783.3612152,
author = {Wang, Xiao and Li, Yaoyu and Gan, Tian and Zhang, Zheng and Lv, Jingjing and Nie, Liqiang},
title = {RTQ: Rethinking Video-language Understanding Based on Image-text Model},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612152},
doi = {10.1145/3581783.3612152},
abstract = {Recent advancements in video-language understanding have been established on the foundation of image-text models, resulting in promising outcomes due to the shared knowledge between images and videos. However, video-language understanding presents unique challenges due to the inclusion of highly complex semantic details, which result in information redundancy, temporal dependency, and scene complexity. Current techniques have only partially tackled these issues, and our quantitative analysis indicates that some of these methods are complementary. In light of this, we propose a novel framework called RTQ (Refine, Temporal model, and Query), which addresses these challenges simultaneously. The approach involves refining redundant information within frames, modeling temporal relations among frames, and querying task-specific information from the videos. Remarkably, our model demonstrates outstanding performance even in the absence of video-language pre-training, and the results are comparable with or superior to those achieved by state-of-the-art pre-training methods.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {557–566},
numpages = {10},
keywords = {video caption, video question answering, video retrieval},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{10.14778/3632093.3632111,
author = {Singh, Mukul and Cambronero, Jos\'{e} and Gulwani, Sumit and Le, Vu and Negreanu, Carina and Nouri, Elnaz and Raza, Mohammad and Verbruggen, Gust},
title = {FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language},
year = {2023},
issue_date = {November 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3632093.3632111},
doi = {10.14778/3632093.3632111},
abstract = {Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires understanding and implementing the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {497–510},
numpages = {14}
}

@inproceedings{10.1109/SCW63240.2024.00193,
author = {Ta\c{s}yaran, Fatih and Yasal, Osman and Morgado, Jos\'{e} A. and Ilic, Aleksandar and Unat, Didem and Kaya, Kamer},
title = {P-MoVE: Performance Monitoring and Visualization with Encoded Knowledge},
year = {2025},
isbn = {9798350355543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SCW63240.2024.00193},
doi = {10.1109/SCW63240.2024.00193},
abstract = {P-MoVE is a modern, open-source framework designed to monitor and visualize live and/or recorded performance data with the ultimate goal of being a digital twin for HPC systems. Leveraging a Knowledge Base (KB), built upon an HPC-specific ontology with an intuitive encoding for comprehending the performance, it rigorously manages telemetry samplers, databases, and visualization frameworks. The KB is generated through an in-depth probing of the system. It enables the configuration and monitoring of performance metric samplers, the generation of real-time visualizations, the establishment of linked-data connections, and the generation of queries for advanced analysis. Furthermore, with an Abstraction Layer, P-MoVE can be used for low-level profiling even on components from different vendors. It is equipped with modern profiling capabilities, including live cache-aware roofline modeling, crafted to provide realtime insights without impeding system performance. P-MoVE's capabilities have been demonstrated on various architectures using microbenchmarks and a common kernel, sparse-matrix vector multiplication.},
booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1531–1542},
numpages = {12},
keywords = {HPC, digital twins for HPC, optimization, performance visualization, profiling},
location = {Atlanta, GA, USA},
series = {SC-W '24}
}

@inproceedings{10.1109/ICSE55347.2025.00072,
author = {Zhong, Chenxing and Feitosa, Daniel and Avgeriou, Paris and Huang, Huang and Li, Yue and Zhang, He},
title = {PairSmell: A Novel Perspective Inspecting Software Modular Structure},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00072},
doi = {10.1109/ICSE55347.2025.00072},
abstract = {Enhancing the modular structure of existing systems has attracted substantial research interest, focusing on two main methods: (1) software modularization and (2) identifying design issues (e.g., smells) as refactoring opportunities. However, re-modularization solutions often require extensive modifications to the original modules, and the design issues identified are generally too coarse to guide refactoring strategies. Combining the above two methods, this paper introduces a novel concept, PairSmell, which exploits modularization to pinpoint design issues necessitating refactoring. We concentrate on a granular but fundamental aspect of modularity principles—modular relation (MR), i.e., whether a pair of entities are separated or collocated. The main assumption is that, if the actual MR of a pair violates its 'apt MR', i.e., an MR agreed on by multiple modularization tools (as raters), it can be deemed likely a flawed architectural decision that necessitates further examination.To quantify and evaluate PairSmell, we conduct an empirical study on 20 C/C++ and Java projects, using 4 established modularization tools to identify two forms of PairSmell: inapt separated pairs InSep and inapt collocated pairs InCol. Our study on 260,003 instances reveals that their architectural impacts are substantial: (1) on average, 14.60\% and 20.44\% of software entities are involved in InSep and InCol MRs respectively; (2) InSep pairs are associated with 190\% more co-changes than properly separated pairs, while InCol pairs are associated with 35\% fewer co-changes than properly collocated pairs, both indicating a successful identification of modular structures detrimental to software quality; and (3) both forms of PairSmell persist across software evolution. This evidence strongly suggests that PairSmell can provide meaningful insights for inspecting modular structure, with the identified issues being both granular and fundamental, making the enhancement of modular design more efficient.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2163–2175},
numpages = {13},
keywords = {modular structure, architectural smell, architecture analysis},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00160,
author = {Zhang, Changjian and Kapoor, Parv and Dardik, Ian and Cui, Leyi and Meira-G\'{o}es, R\^{o}mulo and Garlan, David and Kang, Eunsuk},
title = {Constrained LTL Specification Learning from Examples},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00160},
doi = {10.1109/ICSE55347.2025.00160},
abstract = {Temporal logic specifications play an important role in a wide range of software analysis tasks, such as model checking, automated synthesis, program comprehension, and runtime monitoring. Given a set of positive and negative examples, specified as traces, LTL learning is the problem of synthesizing a specification, in linear temporal logic (LTL), that evaluates to true over the positive traces and false over the negative ones. In this paper, we propose a new type of LTL learning problem called constrained LTL learning, where the user, in addition to positive and negative examples, is given an option to specify one or more constraints over the properties of the LTL formula to be learned. We demonstrate that the ability to specify these additional constraints significantly increases the range of applications for LTL learning, and also allows efficient generation of LTL formulas that satisfy certain desirable properties (such as minimality). We propose an approach for solving the constrained LTL learning problem through an encoding in first-order relational logic and reduction to an instance of the maximal satisfiability (MaxSAT) problem. An experimental evaluation demonstrates that ATLAS, an implementation of our proposed approach, is able to solve new types of learning problems while performing better than or competitively with the state-of-the-art tools in LTL learning.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {629–641},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3613372.3613405,
author = {Gomes, Anderson and Maia, Paulo Henrique M.},
title = {DoME: An Architecture for Domain Model Evolution at Runtime Using NLP},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613405},
doi = {10.1145/3613372.3613405},
abstract = {In traditional information systems, domain models are represented as database tables with attributes and relationships. Changes in the domain models exist due to system evolution and the emergence of new requirements. In these applications, domain models evolve using CRUD operations requested by users. However, it is necessary to support changes in domain models during the applications’ runtime when new (unforeseen) situations may occur. This work presents an architecture called DoME, which relies on natural language processing (NLP) to allow users to trigger changes in the domain models and self-adaptation techniques to update the models at runtime. It is instantiated in a concrete architecture using a chatbot in Telegram and Transformers Libraries for NLP. The architecture has been preliminary evaluated regarding its assertiveness and user satisfaction, resulting in an 82.55\% hit rate and confirming that NL provides good usability and facilitates data manipulation.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {186–195},
numpages = {10},
keywords = {Domain Modelling., Generative Artificial Intelligence, Natural Language Processing, Software Architecture},
location = {Campo Grande, Brazil},
series = {SBES '23}
}

@inproceedings{10.1145/3336191.3371827,
author = {Gu, Yulong and Ding, Zhuoye and Wang, Shuaiqiang and Yin, Dawei},
title = {Hierarchical User Profiling for E-commerce Recommender Systems},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371827},
doi = {10.1145/3336191.3371827},
abstract = {Hierarchical user profiling that aims to model users' real-time interests in different granularity is an essential issue for personalized recommendations in E-commerce. On one hand, items (i.e. products) are usually organized hierarchically in categories, and correspondingly users' interests are naturally hierarchical on different granularity of items and categories. On the other hand, multiple granularity oriented recommendations become very popular in E-commerce sites, which require hierarchical user profiling in different granularity as well. In this paper, we propose HUP, a Hierarchical User Profiling framework to solve the hierarchical user profiling problem in E-commerce recommender systems. In HUP, we provide a Pyramid Recurrent Neural Networks, equipped with Behavior-LSTM to formulate users' hierarchical real-time interests at multiple scales. Furthermore, instead of simply utilizing users' item-level behaviors (e.g., ratings or clicks) in conventional methods, HUP harvests the sequential information of users' temporal finely-granular interactions (micro-behaviors, e.g., clicks on components of items like pictures or comments, browses with navigation of the search engines or recommendations) for modeling. Extensive experiments on two real-world E-commerce datasets demonstrate the significant performance gains of the HUP against state-of-the-art methods for the hierarchical user profiling and recommendation problems. We release the codes and datasets at https://github.com/guyulongcs/WSDM2020_HUP.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {223–231},
numpages = {9},
keywords = {e-commerce, hierarchical user profiling, pyramid recurrent neural networks, recommender systems, user profiling},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@article{10.1145/3714465,
author = {Zhang, Yixuan and Liu, Mugeng and Wang, Haoyu and Ma, Yun and Huang, Gang and Liu, Xuanzhe},
title = {Research on WebAssembly Runtimes: A Survey},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3714465},
doi = {10.1145/3714465},
abstract = {WebAssembly (abbreviated as Wasm) was initially introduced for the Web and quickly extended its reach into various domains beyond the Web. To create Wasm applications, developers can compile high-level programming languages into Wasm binaries or manually write the textual format of Wasm and translate it into Wasm binaries by the toolchain. Regardless of whether it is utilized within or outside the Web, the execution of Wasm binaries is supported by the Wasm runtime. Such a runtime provides a secure, memory-efficient, and sandboxed execution environment to execute Wasm binaries. This paper provides a comprehensive survey of research on Wasm runtimes with 103 collected research papers related to Wasm runtimes following the traditional systematic literature review process. It characterizes existing studies from two different angles, including the internal research of Wasm runtimes (Wasm runtime design, testing, and analysis) and the external research (applying Wasm runtimes to various domains). This paper also proposes future research directions about Wasm runtimes.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {WebAssembly, WebAssembly runtime, WebAssembly System Interface}
}

