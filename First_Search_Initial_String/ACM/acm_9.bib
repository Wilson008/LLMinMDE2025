@inproceedings{10.1145/3672608.3707931,
author = {Amorim, Annie and Assis, Gabriel and Ribeiro, Laura and de Oliveira, Daniel and Paes, Aline},
title = {Language Flavors in the Lusophone World: A BERT-Based Social Media Study of Portuguese in Brazil, Portugal, and Mozambique},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707931},
doi = {10.1145/3672608.3707931},
abstract = {Social networks have become invaluable data sources, enabling many studies about human behavior, language, and social prediction. Users commonly propose new expressions to make communication faster, more fluid, and more original. Moreover, although they follow a specific idiom, the employed language is also mixed-coded, making even the same idiom go beyond the usual variations. This way, texts in Portuguese from different regions that already have linguistic, syntactic, lexical, and orthographic differences go to a new level when written in social media posts. This paper proposes relying on Portuguese language models to analyze the variations of Portuguese in social media posts in Brazil, Portugal, and Mozambique. Our methodology involves pre-trained BERT-based models tuned for the task of text classification and the analysis of the attribution value of the models in the tokens. Moreover, we propose inspecting the ability of the models to predict masked terms specific to regional variants. The analysis reveals that models trained on one variation of Portuguese can perform well on more than one variation. However, there are still challenges in distinguishing between the Portuguese and Mozambican variants.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1927–1934},
numpages = {8},
keywords = {NLP, linguistic variation, portuguese, social media, BERT},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@inproceedings{10.1145/3579027.3608994,
author = {Fadhlillah, Hafiyyan Sayyid and Fern\'{a}ndez, Antonio M. Guti\'{e}rrez and Rabiser, Rick and Zoitl, Alois},
title = {Managing Cyber-Physical Production Systems Variability using V4rdiac: Industrial Experiences},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608994},
doi = {10.1145/3579027.3608994},
abstract = {Cyber-Physical Production Systems (CPPSs) are highly robust and versatile production systems that utilize diverse hardware components through control software. Employing a systematic variability management approach for developing variants of control software can reduce cost and time-to-market to build such complex systems. However, employing this approach in the CPPS domain is challenging. Engineering CPPSs require multidisciplinary engineering knowledge (e.g., process, signal, mechanical). Knowledge about CPPS variability is thus typically scattered across diverse engineering artifacts. Also, variability knowledge is usually not documented explicitly but rather tacit knowledge of mostly senior engineers. Furthermore, control software is commonly implemented using a graphical Domain-Specific Modeling Language (DSML) which only provides minimal support to express variability. This paper describes our experiences dealing with these challenges in an industrial context using a multidisciplinary variability management approach called Variability for 4diac (V4rdiac). V4rdiac is an integrated approach that allows CPPS engineers to conduct stepwise product configuration based on heterogeneous variability models from multiple engineering disciplines. V4rdiac also provides a mechanism to automatically generate control software based on a set of selected configuration options. We evaluate how V4rdiac implements and manages CPPS control software variants in the metallurgical production plant domain. We describe the benefits and lessons learned from using V4rdiac in this domain based on feedback from industrial practitioners.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {223–233},
numpages = {11},
keywords = {Cyber-Physical Production System, Software Configuration, Software Product Line, Variability Modeling},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@article{10.1145/3616111,
author = {Mehmood, Faiza and Shahzadi, Rehab and Ghafoor, Hina and Asim, Muhammad Nabeel and Ghani, Muhammad Usman and Mahmood, Waqar and Dengel, Andreas},
title = {EnML: Multi-label Ensemble Learning for Urdu Text Classification},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3616111},
doi = {10.1145/3616111},
abstract = {Exponential growth of electronic data requires advanced multi-label classification approaches for the development of natural language processing (NLP) applications such as recommendation systems, drug reaction detection, hate speech detection, and opinion recognition/mining. To date, several machine and deep learning–based multi-label classification methodologies have been proposed for English, French, German, Chinese, Arabic, and other developed languages. Urdu is the 11th largest language in the world and has no computer-aided multi-label textual news classification approach. Unlike other languages, Urdu is lacking multi-label text classification datasets that can be used to benchmark the performance of existing machine and deep learning methodologies. With an aim to accelerate and expedite research for the development of Urdu multi-label text classification–based applications, this article provides multiple contributions as follows: First, it provides a manually annotated multi-label textual news classification dataset for the Urdu language. Second, it benchmarks the performance of traditional machine learning approaches particularly by adapting three data transformation approaches along with three top-performing machine learning classifiers and four algorithm adaptation-based approaches. Third, it benchmarks performance of 16 existing deep learning approaches and the four most widely used language models. Finally, it provides an ensemble approach that reaps the benefits of three different deep learning architectures to precisely predict different classes associated with a particular Urdu textual document. Experimental results reveal that proposed ensemble approach performance values (87\% accuracy, 92\% F1-score, and 8\% hamming loss) are significantly higher than adapted machine and deep learning–based approaches.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {227},
numpages = {31},
keywords = {Multi-label Urdu text classification, multi-label Urdu news dataset, traditional machine learning, deep learning, language models, multi-label ensemble learning, data transformation methods}
}

@inproceedings{10.1145/3713082.3730378,
author = {Tsai, Lillian and Bagdasarian, Eugene},
title = {Contextual Agent Security: A Policy for Every Purpose},
year = {2025},
isbn = {9798400714757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3713082.3730378},
doi = {10.1145/3713082.3730378},
abstract = {Judging an action's safety requires knowledge of the context in which the action takes place. To human agents who act in various contexts, this may seem obvious: performing an action such as email deletion may or may not be appropriate depending on the email's content, the goal (e.g., erase sensitive emails or clean up trash), and the type of email address (e.g., work or personal). Unlike people, computational systems have often had only limited agency in limited contexts. Thus, manually crafted policies and user confirmation such as smartphone app permissions or access control lists---while imperfect---have sufficed to restrict harmful actions. However, with the upcoming deployment of generalist agents that support a multitude of tasks (e.g., an automated personal assistant), we argue that we must rethink security designs to adapt to the scale of contexts and capabilities of these systems. As a first step, this paper explores contextual security in the domain of agents and proposes contextual agent security (Conseca), a framework to generate just-in-time, contextual, and human-verifiable security policies.},
booktitle = {Proceedings of the 2025 Workshop on Hot Topics in Operating Systems},
pages = {8–17},
numpages = {10},
location = {Banff, AB, Canada},
series = {HotOS '25}
}

@inproceedings{10.1145/3412841.3442046,
author = {Aronsson, Johan and Lu, Philip and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {A maturity assessment framework for conversational AI development platforms},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442046},
doi = {10.1145/3412841.3442046},
abstract = {Conversational Artificial Intelligence (AI) systems have recently sky-rocketed in popularity and are now used in many applications, from car assistants to customer support. The development of conversational AI systems is supported by a large variety of software platforms, all with similar goals, but different focus points and functionalities. A systematic foundation for classifying conversational AI platforms is currently lacking. We propose a framework for assessing the maturity level of conversational AI development platforms. Our framework is based on a systematic literature review, in which we extracted common and distinguishing features of various open-source and commercial (or in-house) platforms. Inspired by language reference frameworks, we identify different maturity levels that a conversational AI development platform may exhibit in understanding and responding to user inputs. Our framework can guide organizations in selecting a conversational AI development platform according to their needs, as well as helping researchers and platform developers improving the maturity of their platforms.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1736–1745},
numpages = {10},
keywords = {assessment framework, conversational AI, software platforms},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@article{10.1145/3730578,
author = {Silva, Geovana Ramos Sousa and Canedo, Edna Dias},
title = {Privacy in Chatbot Conversation-Driven Development: A Comprehensive Review and Requirements Proposal},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3730578},
doi = {10.1145/3730578},
abstract = {Ensuring data privacy is a major challenge for software developers, especially in chatbots, where balancing privacy protection with response quality is key, given the need for conversation-driven development and data protection regulations. This research identifies privacy requirements and techniques for chatbot development through a literature review, privacy policy analysis, and a practitioner survey. The methodology includes a Systematic Literature Review (SLR), an adapted Gray Literature Review (GLR), privacy requirement formulation, and validation via a survey. Based on the SLR and GLR, eight privacy requirements are proposed, covering personal information protection, user authentication, access control, secure communication, database safety, user rights empowerment, decentralized storage, and reliable infrastructure. Survey results highlight foundational measures like secure communication and scalable infrastructures as priorities, while advanced measures such as decentralized storage or privacy rights implementation scored lower due to complexity and cost. Practitioners also stressed clarity and verifiability, citing gaps in definitions, examples, and validation criteria as challenges to adoption.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {215},
numpages = {44},
keywords = {chatbot, privacy requirements, data protection, security, conversation-driven development}
}

@inproceedings{10.1145/3581783.3612152,
author = {Wang, Xiao and Li, Yaoyu and Gan, Tian and Zhang, Zheng and Lv, Jingjing and Nie, Liqiang},
title = {RTQ: Rethinking Video-language Understanding Based on Image-text Model},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612152},
doi = {10.1145/3581783.3612152},
abstract = {Recent advancements in video-language understanding have been established on the foundation of image-text models, resulting in promising outcomes due to the shared knowledge between images and videos. However, video-language understanding presents unique challenges due to the inclusion of highly complex semantic details, which result in information redundancy, temporal dependency, and scene complexity. Current techniques have only partially tackled these issues, and our quantitative analysis indicates that some of these methods are complementary. In light of this, we propose a novel framework called RTQ (Refine, Temporal model, and Query), which addresses these challenges simultaneously. The approach involves refining redundant information within frames, modeling temporal relations among frames, and querying task-specific information from the videos. Remarkably, our model demonstrates outstanding performance even in the absence of video-language pre-training, and the results are comparable with or superior to those achieved by state-of-the-art pre-training methods.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {557–566},
numpages = {10},
keywords = {video caption, video question answering, video retrieval},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3713082.3730395,
author = {Lazarek, Lukas and Jung, Seong-Heon and Lamprou, Evangelos and Li, Zekai and Narsipur, Anirudh and Zhao, Eric and Greenberg, Michael and Kallas, Konstantinos and Mamouras, Konstantinos and Vasilakis, Nikos},
title = {From Ahead-of- to Just-in-Time and Back Again: Static Analysis for Unix Shell Programs},
year = {2025},
isbn = {9798400714757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3713082.3730395},
doi = {10.1145/3713082.3730395},
abstract = {Shell programming is as prevalent as ever. It is also quite complex, due to the structure of shell programs, their use of opaque software components, and their complex interactions with the broader environment. As a result, even when exercising an abundance of care, shell developers discover devastating bugs in their programs only at runtime: at best, shell programs going wrong crash the execution of a long-running task; at worst, they silently corrupt the broader environment in which they execute---affecting user data, modifying system files, and rendering entire systems unusable. Could the shell's users enjoy the benefits of semantics-driven static analysis before their programs' execution---as offered by most other production languages?},
booktitle = {Proceedings of the 2025 Workshop on Hot Topics in Operating Systems},
pages = {88–95},
numpages = {8},
keywords = {Linux, Unix, inference, shell, static analysis, type systems},
location = {Banff, AB, Canada},
series = {HotOS '25}
}

@article{10.14778/3611479.3611534,
author = {Li, Peng and He, Yeye and Yan, Cong and Wang, Yue and Chaudhuri, Surajit},
title = {Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples},
year = {2023},
issue_date = {July 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611479.3611534},
doi = {10.14778/3611479.3611534},
abstract = {Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30\% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Tableau forums.We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70\% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3391–3403},
numpages = {13}
}

@inproceedings{10.1145/3696443.3708924,
author = {Li, Long and Lai, Jianxin and Yuan, Peng and Sui, Tianxiang and Liu, Yan and Zhu, Qing and Zhang, Xiaojing and Xiao, Linjie and Chen, Wenguang and Xue, Jingling},
title = {ANT-ACE: An FHE Compiler Framework for Automating Neural Network Inference},
year = {2025},
isbn = {9798400712753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696443.3708924},
doi = {10.1145/3696443.3708924},
abstract = {Fully Homomorphic Encryption (FHE) facilitates computations on encrypted data without requiring access to the decryption key, offering substantial privacy benefits for deploying neural network applications in sensitive sectors such as healthcare and finance. Nonetheless, programming these applications within the FHE framework is complex and demands extensive cryptographic expertise to guarantee correctness, performance, and security.                                In this paper, we present ANT-ACE, a production-quality, open-source FHE compiler designed to automate neural network inference on encrypted data. ANT-ACE accepts ONNX models and generates C/C++ programs, leveraging its custom open-source FHE library. We explore the design challenges encountered in the development of ANT-ACE, which is engineered to support a variety of input formats and architectures across diverse FHE schemes through a novel Intermediate Representation (IR) that facilitates multiple levels of abstraction. Comprising 44,000 lines of C/C++ code, ANT-ACE efficiently translates ONNX models into C/C++ programs for encrypted inference on CPUs, specifically utilizing the RNS-CKKS scheme. Preliminary evaluations on a single CPU indicate that ANT-ACE achieves significant speed enhancements in ResNet models, surpassing expert manual implementations and fulfilling our design goals.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {193–208},
numpages = {16},
keywords = {Compilers, FHE, Neural Network Inference},
location = {Las Vegas, NV, USA},
series = {CGO '25}
}

@inproceedings{10.1145/3575693.3575698,
author = {Guo, Liwei and Choe, Wonkyo and Lin, Felix Xiaozhu},
title = {STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575698},
doi = {10.1145/3575693.3575698},
abstract = {Natural Language Processing (NLP) inference is seeing increasing adoption by mobile applications, where on-device inference is desirable for crucially preserving user data privacy and avoiding network roundtrips. Yet, the unprecedented size of an NLP model stresses both latency and memory, creating a tension between the two key resources of a mobile device. To meet a target latency, holding the whole model in memory launches execution as soon as possible but increases one app’s memory footprints by several times, limiting its benefits to only a few inferences before being recycled by mobile memory management. On the other hand, loading the model from storage on demand incurs IO as long as a few seconds, far exceeding the delay range satisfying to a user; pipelining layerwise model loading and execution does not hide IO either, due to the high skewness between IO and computation delays.    To this end, we propose Speedy Transformer Inference (STI). Built on the key idea of maximizing IO/compute resource utilization on the most important parts of a model, STI reconciles the latency v.s. memory tension via two novel techniques. First, model sharding. STI manages model parameters as independently tunable shards, and profiles their importance to accuracy. Second, elastic pipeline planning with a preload buffer. STI instantiates an IO/compute pipeline and uses a small buffer for preload shards to bootstrap execution without stalling at early stages; it judiciously selects, tunes, and assembles shards per their importance for resource-elastic execution, maximizing inference accuracy.    Atop two commodity SoCs, we build STI and evaluate it against a wide range of NLP tasks, under a practical range of target latencies, and on both CPU and GPU. We demonstrate that STI delivers high accuracies with 1–2 orders of magnitude lower memory, outperforming competitive baselines.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {791–803},
numpages = {13},
keywords = {Edge computing, Machine Learning Systems, NLP inference},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@article{10.14778/3494124.3494139,
author = {Ma, Pingchuan and Wang, Shuai},
title = {MT-teql: evaluating and augmenting neural NLIDB on real-world linguistic and schema variations},
year = {2021},
issue_date = {November 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3494124.3494139},
doi = {10.14778/3494124.3494139},
abstract = {Natural Language Interface to Database (NLIDB) translates human utterances into SQL queries and enables database interactions for non-expert users. Recently, neural network models have become a major approach to implementing NLIDB. However, neural NLIDB faces challenges due to variations in natural language and database schema design. For instance, one user intent or database conceptual model can be expressed in various forms. However, existing benchmarks, using hold-out datasets, cannot provide thorough understanding of how good neural NLIDBs really are in real-world situations and its robustness against such variations. A key difficulty is to annotate SQL queries for inputs under real-world variations, requiring considerable manual effort and expert knowledge.To systematically assess the robustness of neural NLIDBs without extensive manual effort, we propose MT-Teql, a unified framework to benchmark NLIDBs against real-world language and schema variations. Inspired by recent advances in DBMS metamorphic testing, MT-Teql implements semantics-preserving transformations on utterances and database schemas to generate their variants. NLIDBs can thus be examined for robustness utilizing utterances/schemas and their variants without requiring manual intervention.We benchmarked nine neural NLIDBs using 62,430 inputs and identified 15,433 defects. We analyzed potential root causes of defects and conducted a user study to show how MT-Teql can assist developers to systematically assess NLIDBs. We further show that the transformed (error-triggering) inputs can be used to augment popular NLIDBs and eliminate 46.5\%(±5.0\%) errors made by them without compromising their accuracy on standard benchmarks. We summarize lessons from this study that can provide insights to select and design NLIDBs that fit particular usage scenarios.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {569–582},
numpages = {14}
}

@inproceedings{10.1145/3586183.3606822,
author = {Pu, Kevin and Yang, Jim and Yuan, Angel and Ma, Minyi and Dong, Rui and Wang, Xinyu and Chen, Yan and Grossman, Tovi},
title = {DiLogics: Creating Web Automation Programs with Diverse Logics},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606822},
doi = {10.1145/3586183.3606822},
abstract = {Knowledge workers frequently encounter repetitive web data entry tasks, like updating records or placing orders. Web automation increases productivity, but translating tasks to web actions accurately and extending to new specifications is challenging. Existing tools can automate tasks that perform the same logical trace of UI actions (e.g., input text in each field in order), but do not support tasks requiring different executions based on varied input conditions. We present DiLogics, a programming-by-demonstration system that utilizes NLP to assist users in creating web automation programs that handle diverse specifications. DiLogics first semantically segments input data to structured task steps. By recording user demonstrations for each step, DiLogics generalizes the web macros to novel but semantically similar task requirements. Our evaluation showed that non-experts can effectively use DiLogics to create automation programs that fulfill diverse input instructions. DiLogics provides an efficient, intuitive, and expressive method for developing web automation programs satisfying diverse specifications.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {74},
numpages = {15},
keywords = {PBD, Web automation, neurosymbolic programming},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@article{10.1145/3729336,
author = {Ding, Yuantian and Qiu, Xiaokang},
title = {A Concurrent Approach to String Transformation Synthesis},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {PLDI},
url = {https://doi.org/10.1145/3729336},
doi = {10.1145/3729336},
abstract = {Program synthesis aims at the automatic generation of programs based on given specifications. Despite   significant progress, the inherent complexity of synthesis tasks and the interplay among intention, invention and adaptation limit its scope. A promising yet challenging avenue is the integration of concurrency to enhance synthesis algorithms. While some efforts have applied basic concurrency by parallelizing search spaces, more intricate synthesis scenarios involving interdependent subproblems remain unexplored. In this paper, we focus on string transformation as the target domain and introduce the first concurrent synthesis algorithm that enables asynchronous coordination between deductive and enumerative processes, featuring an asynchronous deducer for dynamic task decomposition, a versatile enumerator for resolving enumeration requests, and an accumulative case splitter for if-then-else condition/branch search and assembling. Our implementation, Synthphonia exhibits substantial performance improvements over state-of-the-art synthesizers, successfully solving 116 challenging string transformation tasks for the first time.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {233},
numpages = {25},
keywords = {Concurrency, Deduction, Enumeration, String transformations, Syntax-guided synthesis}
}

@article{10.1145/3622841,
author = {Crichton, Will and Gray, Gavin and Krishnamurthi, Shriram},
title = {A Grounded Conceptual Model for Ownership Types in Rust},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622841},
doi = {10.1145/3622841},
abstract = {Programmers learning Rust struggle to understand ownership types, Rust’s core mechanism for ensuring memory safety without garbage collection. This paper describes our attempt to systematically design a pedagogy for ownership types. First, we studied Rust developers’ misconceptions of ownership to create the Ownership Inventory, a new instrument for measuring a person’s knowledge of ownership. We found that Rust learners could not connect Rust’s static and dynamic semantics, such as determining why an ill-typed program would (or would not) exhibit undefined behavior. Second, we created a conceptual model of Rust’s semantics that explains borrow checking in terms of flow-sensitive permissions on paths into memory. Third, we implemented a Rust compiler plugin that visualizes programs under the model. Fourth, we integrated the permissions model and visualizations into a broader pedagogy of ownership by writing a new ownership chapter for The Rust Programming Language, a popular Rust textbook. Fifth, we evaluated an initial deployment of our pedagogy against the original version, using reader responses to the Ownership Inventory as a point of comparison. Thus far, the new pedagogy has improved learner scores on the Ownership Inventory by an average of 9},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {265},
numpages = {29},
keywords = {Rust, concept inventory, ownership types, program state visualization}
}

@inproceedings{10.1145/3664647.3680778,
author = {Liu, Zihao and Wu, Xiaoyu and Wang, Shengjin and Qian, Jiayao},
title = {Adaptively Building a Video-language Model for Video Captioning and Retrieval without Massive Video Pretraining},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680778},
doi = {10.1145/3664647.3680778},
abstract = {Large-scale pretrained image-language models have shown remarkable performance recently. However, building a video-language model is more challenging due to the complexity of video and the difficulty of collecting high-quality data. This paper builds a video-language model in an adaptive manner, which transfers the knowledge from the image domain and can achieve state-of-the-art performance without any further massive video pretraining. The main contributions include a Visual Perception Adapter that seamlessly and efficiently adapts a pretrained image-language model to the video domain and a fine-grained contrastive learning with Inter-modal Token Alignment that bridges semantic gaps between vision, audio, and language with less data. The proposed model is evaluated on video captioning and retrieval. Experiments demonstrate that the proposed model exhibits competitive performance compared to models pretrained on millions of video-text pairs. Notably, our model's CIDEr and R@1 scores on the MSR-VTT dataset exceed the existing state-of-the-art by 6.3\% and 1.3\%.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {4871–4880},
numpages = {10},
keywords = {deep learning, multimodality, transfer learning, video captioning, video retrieval},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3592573.3593106,
author = {Nguyen-Dang, Tien-Thanh and Thai, Xuan-Dang and Vuong, Gia-Huy and Ho, Van-Son and Tran, Minh-Triet and Ninh, Van-Tu and Pham, Minh-Khoi and Le, Tu-Khiem and Healy, Graham},
title = {LifeInsight: An Interactive Lifelog Retrieval System with Comprehensive Spatial Insights and Query Assistance},
year = {2023},
isbn = {9798400701887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592573.3593106},
doi = {10.1145/3592573.3593106},
abstract = {In this paper, we introduce LifeInsight – an interactive lifelog retrieval system developed for the sixth annual Lifelog Search Challenge (LSC’23). LifeInsight incorporates semantic search mechanisms from state-of-the-art lifelog retrieval systems while focusing on providing insights into the lifelogger’s routine using spatial information to support question-answering tasks. The system employs the Bootstrapping Language-Image Pre-training (BLIP) model for zero-shot image-text retrieval, which has been shown to achieve higher recall scores than the CLIP model on the Flickr30K dataset. In addition, the Elastic Search filtering mechanism is utilized to remove irrelevant images. Apart from semantic search mechanisms, the system also supports visual similarity search by comparing the inner product distance between the vectors in the lifelog image corpus and the query image. Furthermore, the system includes an explicit relevance feedback function, AI-based query description rewriting, and visual-example-generating features to re-phrase the query to describe it better and support end-users envisioning the targeted image for retrieval.},
booktitle = {Proceedings of the 6th Annual ACM Lifelog Search Challenge},
pages = {59–64},
numpages = {6},
keywords = {AI-based assistance, interactive retrieval, lifelog, spatial insights},
location = {Thessaloniki, Greece},
series = {LSC '23}
}

@inproceedings{10.1145/3664646.3664774,
author = {Cook, Michael},
title = {The Art of Programming: Challenges in Generating Code for Creative Applications},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664774},
doi = {10.1145/3664646.3664774},
abstract = {Programming has been a key tool for artists and other creatives for decades, and the creative use of programming presents unique challenges, opportunities and perspectives for researchers considering how AI can be used to support coding more generally. In this paper we aim to motivate researchers to look deeper into some of these areas, by highlighting some interesting uses of programming in creative practices, suggesting new research questions posed by these spaces, and briefly raising important issues that work in this area may face.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {139–143},
numpages = {5},
keywords = {code generation, computational creativity, generative systems},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@inproceedings{10.1145/3659463.3660029,
author = {Wang, Weijie and Deng, Haotian and Liang, Jinwen and Zhang, Chuan},
title = {Achieving Privacy-Preserving Optimizer Architecture for Intent Execution on EVM Blockchain},
year = {2025},
isbn = {9798400706387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659463.3660029},
doi = {10.1145/3659463.3660029},
abstract = {Intention adoption represents a migration from an imperative to a declarative paradigm, which is expected to significantly improve the user experience in blockchain. While the development of account abstraction has expanded the range of possibilities for intent expression, the architecture for intent-centric collaboration and coordination remains underdeveloped. Existing works depend on trusted centralized entities and employ broadcasts to the network to disseminate user intent, risking single points of failure and privacy leakage. In addition, no standard intent architecture has been proposed. In this paper, we propose a novel decentralized privacy-preserving optimizer architecture for executing intents on the EVM blockchain. We first propose standard intent structures and zero-knowledge intent structures. Our architecture allows users to transform raw intent structures into zero-knowledge intent structures and send them to a set of trusted optimizers. The optimizers can optimize and execute the intents without revealing the content of the intents. We use zero-knowledge proof and public-key encryption to ensure the privacy and correctness of the intents, and smart contracts to ensure the security and verifiability of the execution. We also design a quantization function to evaluate and compare solutions from different optimizers. We analyze the security and performance of our architecture and show that it can achieve privacy, efficiency, and scalability of intent execution on the EVM blockchain.},
booktitle = {Proceedings of the 6th ACM International Symposium on Blockchain and Secure Critical Infrastructure},
pages = {1–9},
numpages = {9},
keywords = {blockchain, privacy-preserving, intent, zero-knowledge proof},
location = {Singapore, Singapore},
series = {BSCI '24}
}

@inproceedings{10.1145/3660512.3665526,
author = {Zaharia, Raul and Gavrilut, Dragos and Mutu, Gheorghita and Lucanu, Dorel},
title = {Interactive Assistance in Malware Dissemination Detection and Analysis},
year = {2024},
isbn = {9798400706509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660512.3665526},
doi = {10.1145/3660512.3665526},
abstract = {Analysis of a complex cyber-security attack often involves a variety of tools, for each specific payload used in the attack. The information supplied by these tools must be soundly correlated to obtain a correct verdict. We propose a tool, GView, that is designed to investigate cyber-attacks by providing guided analysis for various file types using automatic artifact identification, extraction, coherent correlation&nbsp;&amp;&nbsp;inference, and meaningful&nbsp;&amp;&nbsp;intuitive views at different levels of granularity w.r.t. revealed information. The concept behind GView simplifies navigation through all payloads in a complex attack, streamlining the process for security researchers, and increasing the quality of analysis. Our evaluation shows that GView improves the analysis time of an attack by up to 90\% compared to conventional tools used in forensics. We show a scenario where GView is used to analyze a misleading email.},
booktitle = {Proceedings of the 1st Workshop on Security-Centric Strategies for Combating Information Disorder},
articleno = {7},
numpages = {6},
keywords = {complex binary analysis, forensics investigation, malware research, security operations center},
location = {Singapore, Singapore},
series = {SCID '24}
}

@inproceedings{10.1109/ISCA59077.2024.00072,
author = {Yang, Yifan and Emer, Joel S. and Sanchez, Daniel},
title = {Trapezoid: A Versatile Accelerator for Dense and Sparse Matrix Multiplications},
year = {2025},
isbn = {9798350326581},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA59077.2024.00072},
doi = {10.1109/ISCA59077.2024.00072},
abstract = {Accelerating matrix multiplication is crucial to achieve high performance in many application domains, including neural networks, graph analytics, and scientific computing. These applications process matrices with a wide range of sparsities, from completely dense to highly sparse. Ideally, a single accelerator should handle matrices of all sparsity levels well. However, prior matrix multiplication accelerators each target a limited range of sparsity levels.We present Trapezoid, a versatile accelerator that performs matrix multiplication across all sparsity levels effectively. Trapezoid builds on a 2D spatial array design, which excels at dense matrix multiplication, and extends it with new hardware mechanisms that let it handle sparse inputs. We present a novel inner-product-based dataflow with a multi-fiber intersection unit that handles mildly sparse matrices. Furthermore, novel Gustavson-based dataflows and a multi-level memory hierarchy enable high performance on highly sparse matrices. Trapezoid's hardware extensions are reused across dataflows to minimize area overheads.We evaluate Trapezoid on a broad range of dense and sparse matrix multiplication workloads. Trapezoid has gmean 19.7\texttimes{}, 4.3\texttimes{}, and 2.9\texttimes{} better performance/area than TPU, SIGMA, and Flexagon, prior state-of-the-art accelerators that target dense, mildly sparse, and highly sparse matrices, respectively.},
booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture},
pages = {931–945},
numpages = {15},
keywords = {sparsity, matrix multiplication, accelerator, dataflow},
location = {Buenos Aires, Argentina},
series = {ISCA '24}
}

@article{10.14778/3611540.3611635,
author = {Cheung, Alvin and Ahmad, Maaz Bin Safeer and Haynes, Brandon and Kittivorawong, Chanwut and Laddad, Shadaj and Liu, Xiaoxuan and Wang, Chenglong and Yan, Cong},
title = {Towards Auto-Generated Data Systems},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611635},
doi = {10.14778/3611540.3611635},
abstract = {After decades of progress, database management systems (DBMSs) are now the backbones of many data applications that we interact with on a daily basis. Yet, with the emergence of new data types and hardware, building and optimizing new data systems remain as difficult as the heyday of relational databases. In this paper, we summarize our work towards automating the building and optimization of data systems. Drawing from our own experience, we further argue that any automation technique must address three aspects: user specification, code generation, and result validation. We conclude by discussing a case study using videos data processing, along with opportunities for future research towards designing data systems that are automatically generated.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4116–4129},
numpages = {14}
}

@inproceedings{10.1109/SCW63240.2024.00193,
author = {Ta\c{s}yaran, Fatih and Yasal, Osman and Morgado, Jos\'{e} A. and Ilic, Aleksandar and Unat, Didem and Kaya, Kamer},
title = {P-MoVE: Performance Monitoring and Visualization with Encoded Knowledge},
year = {2025},
isbn = {9798350355543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SCW63240.2024.00193},
doi = {10.1109/SCW63240.2024.00193},
abstract = {P-MoVE is a modern, open-source framework designed to monitor and visualize live and/or recorded performance data with the ultimate goal of being a digital twin for HPC systems. Leveraging a Knowledge Base (KB), built upon an HPC-specific ontology with an intuitive encoding for comprehending the performance, it rigorously manages telemetry samplers, databases, and visualization frameworks. The KB is generated through an in-depth probing of the system. It enables the configuration and monitoring of performance metric samplers, the generation of real-time visualizations, the establishment of linked-data connections, and the generation of queries for advanced analysis. Furthermore, with an Abstraction Layer, P-MoVE can be used for low-level profiling even on components from different vendors. It is equipped with modern profiling capabilities, including live cache-aware roofline modeling, crafted to provide realtime insights without impeding system performance. P-MoVE's capabilities have been demonstrated on various architectures using microbenchmarks and a common kernel, sparse-matrix vector multiplication.},
booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1531–1542},
numpages = {12},
keywords = {HPC, digital twins for HPC, optimization, performance visualization, profiling},
location = {Atlanta, GA, USA},
series = {SC-W '24}
}

@article{10.1145/3699514,
author = {Nickel, Matthias and G\"{o}hringer, Diana},
title = {A Survey on Architectures, Hardware Acceleration and Challenges for In-Network Computing},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3699514},
doi = {10.1145/3699514},
abstract = {By moving data and computation away from the end user to more powerful servers in the cloud or to cloudlets at the edge, end user devices only need to compute locally for small amounts of data and when low latency is required. However, with the advent of 6G and Internet-of-Everything, the demand for more powerful networks continues to grow. The introduction of Software-Defined Networking and Network Function Virtualization has allowed us to rethink networks and use them for more than just routing data to servers. In addition, the use of more powerful network devices is bringing new life to the concept of active networks in the form of in-network computing. In-Network Computing provides the ability to move applications into the network and process data on programmable network devices as they are transmitted. In this work, we provide an overview of in-network computing and its enabling technologies. We take a look at the programmability and different hardware architectures for SmartNICs and switches, focusing primarily on accelerators such as FPGAs. We discuss the state of the art and challenges in this area, and look at CGRAs, a class of hardware accelerators that have not been widely discussed in this context.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {10},
numpages = {34},
keywords = {In-Network Computing, Software-Defined Networking, FPGA, CGRA, SoC}
}

@inproceedings{10.1145/3613372.3613405,
author = {Gomes, Anderson and Maia, Paulo Henrique M.},
title = {DoME: An Architecture for Domain Model Evolution at Runtime Using NLP},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613405},
doi = {10.1145/3613372.3613405},
abstract = {In traditional information systems, domain models are represented as database tables with attributes and relationships. Changes in the domain models exist due to system evolution and the emergence of new requirements. In these applications, domain models evolve using CRUD operations requested by users. However, it is necessary to support changes in domain models during the applications’ runtime when new (unforeseen) situations may occur. This work presents an architecture called DoME, which relies on natural language processing (NLP) to allow users to trigger changes in the domain models and self-adaptation techniques to update the models at runtime. It is instantiated in a concrete architecture using a chatbot in Telegram and Transformers Libraries for NLP. The architecture has been preliminary evaluated regarding its assertiveness and user satisfaction, resulting in an 82.55\% hit rate and confirming that NL provides good usability and facilitates data manipulation.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {186–195},
numpages = {10},
keywords = {Domain Modelling., Generative Artificial Intelligence, Natural Language Processing, Software Architecture},
location = {Campo Grande, Brazil},
series = {SBES '23}
}

@inproceedings{10.1145/3643991.3644927,
author = {Menon, Harshitha and Nichols, Daniel and Bhatele, Abhinav and Gamblin, Todd},
title = {Learning to Predict and Improve Build Successes in Package Ecosystems},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644927},
doi = {10.1145/3643991.3644927},
abstract = {Software has become increasingly complex, with a typical application depending on tens or hundreds of packages. Finding compatible versions and build configurations of these packages is challenging. This paper presents a method to learn the likelihood of software build success, and techniques for leveraging this information to guide dependency solvers to better software configurations. We leverage the heavily parameterized package recipes from the Spack package manager to produce a training data set of builds, and we use Graph Neural Networks to learn whether a given package configuration will build successfully or not. We apply our tool to the U.S. Exascale Computing Project's software stack. We demonstrate its effectiveness in predicting whether a given package will build successfully. We show that our technique can be used to improve the solutions generated by dependency solvers, reducing the need for developers to find working builds by trial and error.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {531–542},
numpages = {12},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3534678.3539187,
author = {Srivastava, Aseem and Suresh, Tharun and Lord, Sarah P. and Akhtar, Md Shad and Chakraborty, Tanmoy},
title = {Counseling Summarization Using Mental Health Knowledge Guided Utterance Filtering},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539187},
doi = {10.1145/3534678.3539187},
abstract = {The psychotherapy intervention technique is a multifaceted conversation between a therapist and a patient. Unlike general clinical discussions, psychotherapy's core components (viz. symptoms) are hard to distinguish, thus becoming a complex problem to summarize later. A structured counseling conversation may contain discussions about symptoms, history of mental health issues, or the discovery of the patient's behavior. It may also contain discussion filler words irrelevant to a clinical summary. We refer to these elements of structured psychotherapy as counseling components. In this paper, the aim is mental health counseling summarization to build upon domain knowledge and to help clinicians quickly glean meaning. We create a new dataset after annotating 12.9K utterances of counseling components and reference summaries for each dialogue. Further, we propose ConSum, a novel counseling-component guided summarization model. ConSum undergoes three independent modules. First, to assess the presence of depressive symptoms, it filters utterances utilizing the Patient Health Questionnaire (PHQ-9), while the second and third modules aim to classify counseling components. At last, we propose a problem-specific Mental Health Information Capture (MHIC) evaluation metric for counseling summaries. Our comparative study shows that we improve on performance and generate cohesive, semantic, and coherent summaries. We comprehensively analyze the generated summaries to investigate the capturing of psychotherapy elements. Human and clinical evaluations on the summary show that ConSum generates quality summary. Further, mental health experts validate the clinical acceptability of the ConSum. Lastly, we discuss the uniqueness in mental health counseling summarization in the real world and show evidences of its deployment on an online application with the support of mpathic.ai},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3920–3930},
numpages = {11},
keywords = {dialogue summarization, natural language processing},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3336191.3371827,
author = {Gu, Yulong and Ding, Zhuoye and Wang, Shuaiqiang and Yin, Dawei},
title = {Hierarchical User Profiling for E-commerce Recommender Systems},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371827},
doi = {10.1145/3336191.3371827},
abstract = {Hierarchical user profiling that aims to model users' real-time interests in different granularity is an essential issue for personalized recommendations in E-commerce. On one hand, items (i.e. products) are usually organized hierarchically in categories, and correspondingly users' interests are naturally hierarchical on different granularity of items and categories. On the other hand, multiple granularity oriented recommendations become very popular in E-commerce sites, which require hierarchical user profiling in different granularity as well. In this paper, we propose HUP, a Hierarchical User Profiling framework to solve the hierarchical user profiling problem in E-commerce recommender systems. In HUP, we provide a Pyramid Recurrent Neural Networks, equipped with Behavior-LSTM to formulate users' hierarchical real-time interests at multiple scales. Furthermore, instead of simply utilizing users' item-level behaviors (e.g., ratings or clicks) in conventional methods, HUP harvests the sequential information of users' temporal finely-granular interactions (micro-behaviors, e.g., clicks on components of items like pictures or comments, browses with navigation of the search engines or recommendations) for modeling. Extensive experiments on two real-world E-commerce datasets demonstrate the significant performance gains of the HUP against state-of-the-art methods for the hierarchical user profiling and recommendation problems. We release the codes and datasets at https://github.com/guyulongcs/WSDM2020_HUP.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {223–231},
numpages = {9},
keywords = {e-commerce, hierarchical user profiling, pyramid recurrent neural networks, recommender systems, user profiling},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1145/3404835.3462938,
author = {Zogan, Hamad and Razzak, Imran and Jameel, Shoaib and Xu, Guandong},
title = {DepressionNet: Learning Multi-modalities with User Post Summarization for Depression Detection on Social Media},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462938},
doi = {10.1145/3404835.3462938},
abstract = {Twitter is currently a popular online social media platform which allows users to share their user-generated content. This publicly-generated user data is also crucial to healthcare technologies because the discovered patterns would hugely benefit them in several ways. One of the applications is in automatically discovering mental health problems, e.g., depression. Previous studies to automatically detect a depressed user on online social media have largely relied upon the user behaviour and their linguistic patterns including user's social interactions. The downside is that these models are trained on several irrelevant content which might not be crucial towards detecting a depressed user. Besides, these content have a negative impact on the overall efficiency and effectiveness of the model. To overcome the shortcomings in the existing automatic depression detection methods, we propose a novel computational framework for automatic depression detection that initially selects relevant content through a hybrid extractive and abstractive summarization strategy on the sequence of all user tweets leading to a more fine-grained and relevant content. The content then goes to our novel deep learning framework comprising of a unified learning machinery comprising of Convolutional Neural Network (CNN) coupled with attention-enhanced Gated Recurrent Units (GRU) models leading to better empirical performance than existing strong baselines.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {133–142},
numpages = {10},
keywords = {deep learning, depression detection, machine learning, social network, text summarization},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@article{10.1145/3626234,
author = {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Whittle, Jon and Zowghi, Didar and Jacquet, Aurelie},
title = {Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3626234},
doi = {10.1145/3626234},
abstract = {Responsible Artificial Intelligence (RAI) is widely considered as one of the greatest scientific challenges of our time and is key to increase the adoption of Artificial Intelligence (AI). Recently, a number of AI ethics principles frameworks have been published. However, without further guidance on best practices, practitioners are left with nothing much beyond truisms. In addition, significant efforts have been placed at algorithm level rather than system level, mainly focusing on a subset of mathematics-amenable ethical principles, such as fairness. Nevertheless, ethical issues can arise at any step of the development lifecycle, cutting across many AI and non-AI components of systems beyond AI algorithms and models. To operationalize RAI from a system perspective, in this article, we present an RAI Pattern Catalogue based on the results of a multivocal literature review. Rather than staying at the principle or algorithm level, we focus on patterns that AI system stakeholders can undertake in practice to ensure that the developed AI systems are responsible throughout the entire governance and engineering lifecycle. The RAI Pattern Catalogue classifies the patterns into three groups: multi-level governance patterns, trustworthy process patterns, and RAI-by-design product patterns. These patterns provide systematic and actionable guidance for stakeholders to implement RAI.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {173},
numpages = {35},
keywords = {Responsible AI, ethical AI, trustworthy AI, AI governance, AI engineering, MLOps, software engineering, software architecture, pattern, best practice}
}

@article{10.1145/3714465,
author = {Zhang, Yixuan and Liu, Mugeng and Wang, Haoyu and Ma, Yun and Huang, Gang and Liu, Xuanzhe},
title = {Research on WebAssembly Runtimes: A Survey},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3714465},
doi = {10.1145/3714465},
abstract = {WebAssembly (abbreviated as Wasm) was initially introduced for the Web and quickly extended its reach into various domains beyond the Web. To create Wasm applications, developers can compile high-level programming languages into Wasm binaries or manually write the textual format of Wasm and translate it into Wasm binaries by the toolchain. Regardless of whether it is utilized within or outside the Web, the execution of Wasm binaries is supported by the Wasm runtime. Such a runtime provides a secure, memory-efficient, and sandboxed execution environment to execute Wasm binaries. This paper provides a comprehensive survey of research on Wasm runtimes with 103 collected research papers related to Wasm runtimes following the traditional systematic literature review process. It characterizes existing studies from two different angles, including the internal research of Wasm runtimes (Wasm runtime design, testing, and analysis) and the external research (applying Wasm runtimes to various domains). This paper also proposes future research directions about Wasm runtimes.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {WebAssembly, WebAssembly runtime, WebAssembly System Interface}
}

