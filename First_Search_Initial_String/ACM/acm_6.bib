@inproceedings{10.1145/3442442.3451375,
author = {Arslan, Yusuf and Allix, Kevin and Veiber, Lisa and Lothritz, Cedric and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Goujon, Anne},
title = {A Comparison of Pre-Trained Language Models for Multi-Class Text Classification in the Financial Domain},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3451375},
doi = {10.1145/3442442.3451375},
abstract = {Neural networks for language modeling have been proven effective on several sub-tasks of natural language processing. Training deep language models, however, is time-consuming and computationally intensive. Pre-trained language models such as BERT are thus appealing since (1) they yielded state-of-the-art performance, and (2) they offload practitioners from the burden of preparing the adequate resources (time, hardware, and data) to train models. Nevertheless, because pre-trained models are generic, they may underperform on specific domains. In this study, we investigate the case of multi-class text classification, a task that is relatively less studied in the literature evaluating pre-trained language models. Our work is further placed under the industrial settings of the financial domain. We thus leverage generic benchmark datasets from the literature and two proprietary datasets from our partners in the financial technological industry. After highlighting a challenge for generic pre-trained models (BERT, DistilBERT, RoBERTa, XLNet, XLM) to classify a portion of the financial document dataset, we investigate the intuition that a specialized pre-trained model for financial documents, such as FinBERT, should be leveraged. Nevertheless, our experiments show that the FinBERT model, even with an adapted vocabulary, does not lead to improvements compared to the generic BERT models.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {260–268},
numpages = {9},
keywords = {financial text classification, FinBERT, BERT},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.14778/3636218.3636227,
author = {Nagrecha, Kabir and Kumar, Arun},
title = {Saturn: An Optimized Data System for Multi-Large-Model Deep Learning Workloads},
year = {2023},
issue_date = {December 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3636218.3636227},
doi = {10.14778/3636218.3636227},
abstract = {Large models such as GPT-3 and ChatGPT have transformed deep learning (DL), powering applications that have captured the public's imagination. Such models must be trained on multiple GPUs due to their size and computational load, driving the development of a bevy of "model parallelism" techniques and tools. Navigating such parallelism choices, however, is a new burden for DL users such as data scientists, domain scientists, etc., who may lack the necessary systems knowhow. The need for model selection, which leads to many models to train due to hyper-parameter tuning or layer-wise finetuning, compounds the situation with two more burdens: resource apportioning and scheduling. In this work, we unify these three burdens by formalizing them as a joint problem that we call SPASE: Select a Parallelism, Allocate resources, and Schedule. We propose a new information system architecture to tackle the SPASE problem holistically, exploiting the performance opportunities presented by joint optimization. We devise an extensible template for existing parallelism schemes and combine it with an automated empirical profiler for runtime estimation. We then formulate SPASE as an MILP. We find that direct use of an MILP-solver is significantly more effective than several baseline heuristics. We optimize the system runtime further with an introspective scheduling approach. We implement all these techniques into a new data system we call Saturn. Experiments with benchmark DL workloads show that Saturn achieves 39-49\% lower model selection runtimes than current DL practice.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {712–725},
numpages = {14}
}

@inproceedings{10.1145/3677052.3698688,
author = {Fatemi, Sorouralsadat and Hu, Yuheng},
title = {FinVision: A Multi-Agent Framework for Stock Market Prediction},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698688},
doi = {10.1145/3677052.3698688},
abstract = {Financial trading has been a challenging task, as it requires the integration of vast amounts of data from various modalities. Traditional deep learning and reinforcement learning methods require large training data and often involve encoding various data types into numerical formats for model input, which limits the explainability of model behavior. Recently, LLM-based agents have demonstrated remarkable advancements in handling multi-modal data, enabling them to execute complex, multi-step decision-making tasks while providing insights into their thought processes. This research introduces a multi-modal multi-agent system designed specifically for financial trading tasks. Our framework employs a team of specialized LLM-based agents, each adept at processing and interpreting various forms of financial data, such as textual news reports, candlestick charts, and trading signal charts. A key feature of our approach is the integration of a reflection module, which conducts analyses of historical trading signals and their outcomes. This reflective process is instrumental in enhancing the decision-making capabilities of the system for future trading scenarios. Furthermore, the ablation studies indicate that the visual reflection module plays a crucial role in enhancing the decision-making capabilities of our framework.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {582–590},
numpages = {9},
keywords = {Large Language Models, Multi-Agent Framework},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@article{10.1145/3725352,
author = {Zhang, Enhao and Sullivan, Nicole and Haynes, Brandon and Krishna, Ranjay and Balazinska, Magdalena},
title = {Self-Enhancing Video Data Management System for Compositional Events with Large Language Models},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3725352},
doi = {10.1145/3725352},
abstract = {Complex video queries can be answered by decomposing them into modular subtasks. However, existing video data management systems assume the existence of predefined modules for each subtask. We introduce VOCAL-UDF, a novel self-enhancing system that supports compositional queries over videos without the need for predefined modules. VOCAL-UDF automatically identifies and constructs missing modules and encapsulates them as user-defined functions (UDFs), thus expanding its querying capabilities. To achieve this, we formulate a unified UDF model that leverages large language models (LLMs) to aid in new UDF generation. VOCAL UDF handles a wide range of concepts by supporting both program-based UDFs (i.e., Python functions generated by LLMs) and distilled-model UDFs (lightweight vision models distilled from strong pretrained models). To resolve the inherent ambiguity in user intent, VOCAL-UDF generates multiple candidate UDFs and uses active learning to efficiently select the best one. With the self-enhancing capability, VOCAL-UDF significantly improves query performance across three video datasets.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {215},
numpages = {29},
keywords = {compositional queries, knowledge distillation, large language models, program generation, scene graphs, video analytics, vision-language models}
}

@article{10.1145/3622863,
author = {Chen, Qiaochu and Banerjee, Arko and Demiralp, \c{C}a\u{g}atay and Durrett, Greg and Dillig, I\c{s}\i{}l},
title = {Data Extraction via Semantic Regular Expression Synthesis},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622863},
doi = {10.1145/3622863},
abstract = {Many data extraction tasks of practical relevance require not only syntactic pattern matching but also semantic reasoning about the content of the underlying text. While regular expressions are very well suited for tasks that require only syntactic pattern matching, they fall short for data extraction tasks that involve both a syntactic and semantic component. To address this issue, we introduce semantic regexes, a generalization of regular expressions that facilitates combined syntactic and semantic reasoning about textual data. We also propose a novel learning algorithm that can synthesize semantic regexes from a small number of positive and negative examples. Our proposed learning algorithm uses a combination of neural sketch generation and compositional type-directed synthesis for fast and effective generalization from a small number of examples.  We have implemented these ideas in a new tool called Smore and evaluated it on representative data extraction tasks involving several textual datasets. Our evaluation shows that semantic regexes can better support complex data extraction tasks than standard regular expressions and that our learning algorithm significantly outperforms existing tools, including state-of-the-art neural networks and program synthesis tools.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {287},
numpages = {30},
keywords = {Program Synthesis, Regular Expression}
}

@article{10.1109/TCBB.2021.3135844,
author = {Sun, Yi and Wang, Jian and Lin, Hongfei and Zhang, Yijia and Yang, Zhihao},
title = {Knowledge Guided Attention and Graph Convolutional Networks for Chemical-Disease Relation Extraction},
year = {2022},
issue_date = {Jan.-Feb. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3135844},
doi = {10.1109/TCBB.2021.3135844},
abstract = {The automatic extraction of the chemical-disease relation (CDR) from the text becomes critical because it takes a lot of time and effort to extract valuable CDR manually. Studies have shown that prior knowledge from the biomedical knowledge base is important for relation extraction. The method of combining deep learning models with prior knowledge is worthy of our study. In this paper, we propose a new model called Knowledge Guided Attention and Graph Convolutional Networks (KGAGN) for CDR extraction. First, to make full advantage of domain knowledge, we train entity embedding as a feature representation of input sequence, and relation embedding to capture weighted contextual information further through the attention mechanism. Then, to make full advantage of syntactic dependency information in cross-sentence CDR extraction, we construct document-level syntactic dependency graphs and encode them using a graph convolution network (GCN). Finally, the chemical-induced disease (CID) relation is extracted by using weighted context features and long-range dependency features both of which contain additional knowledge information We evaluated our model on the CDR dataset published by the BioCreative-V community and achieves an F1-score of 73.3%, surpassing other state-of-the-art methods. the code implemented by PyTorch 1.7.0 deep learning library can be downloaded from Github: &lt;uri&gt;https://github.com/sunyi123/cdr&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {489–499},
numpages = {11}
}

@inproceedings{10.1145/3706598.3714239,
author = {Zhou, Yunfan and Cai, Xiwen and Shi, Qiming and Huang, Yanwei and Li, Haotian and Qu, Huamin and Weng, Di and Wu, Yingcai},
title = {Xavier: Toward Better Coding Assistance in Authoring Tabular Data Wrangling Scripts},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714239},
doi = {10.1145/3706598.3714239},
abstract = {Data analysts frequently employ code completion tools in writing custom scripts to tackle complex tabular data wrangling tasks. However, existing tools do not sufficiently link the data contexts such as schemas and values with the code being edited. This not only leads to poor code suggestions, but also frequent interruptions in coding processes as users need additional code to locate and understand relevant data. We introduce Xavier, a tool designed to enhance data wrangling script authoring in computational notebooks. Xavier maintains users’ awareness of data contexts while providing data-aware code suggestions. It automatically highlights the most relevant data based on the user’s code, integrates both code and data contexts for more accurate suggestions, and instantly previews data transformation results for easy verification. To evaluate the effectiveness and usability of Xavier, we conducted a user study with 16 data analysts, showing its potential to streamline data wrangling scripts authoring.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {850},
numpages = {16},
keywords = {Interactive data wrangling, coding assistance},
location = {
},
series = {CHI '25}
}

@article{10.1145/3734190,
author = {Cal\`{o}, Tommaso and De Russis, Luigi},
title = {Advancing Code Generation from Visual Designs through Transformer-Based Architectures and Specialized Datasets},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3734190},
doi = {10.1145/3734190},
abstract = {Manually translating web designs into code is a costly and time-consuming process, particularly due to the frequent iterations and refinements between designers and developers. Deep learning techniques, which possess the capability to automatically translate designs into functional code using an encoder-decoder architecture, have emerged as a promising solution to enhance this tedious process. However, many current methods depend on simplistic datasets that do not capture the diversity of components found in modern websites. Additionally, the potential of transformer-based models, which have enabled significant progress in vision and language modeling tasks due to their scalability and ability to handle cross-modal relationships, has not been investigated in this context. Addressing these limitations, this paper contributes with: 1) a web scraping methodology to automatically collect and process a diverse dataset of real-world websites with reduced noise and complexity, 2) a synthetic dataset of webpage mockups along with their sketched conversions, and 3) an evaluation of two recent multimodal transformer architectures on these proposed datasets. Results on synthetic and sketch-based datasets demonstrate the architectures potential as effective design-to-code automation solutions, while identifying remaining challenges in modeling real-world website complexity.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {EICS013},
numpages = {37},
keywords = {Transformer-based models, Web design automation, Mockup-to-code, Sketch-to-code, Deep learning in UI design}
}

@article{10.1145/3485535,
author = {Rahmani, Kia and Raza, Mohammad and Gulwani, Sumit and Le, Vu and Morris, Daniel and Radhakrishna, Arjun and Soares, Gustavo and Tiwari, Ashish},
title = {Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {OOPSLA},
url = {https://doi.org/10.1145/3485535},
doi = {10.1145/3485535},
abstract = {Multi-modal program synthesis refers to the task of synthesizing programs (code) from their specification given in different forms, such as a combination of natural language and examples. Examples provide a precise but incomplete specification, and natural language provides an ambiguous but more "complete" task description. Machine-learned pre-trained models (PTMs) are adept at handling ambiguous natural language, but struggle with generating syntactically and semantically precise code. Program synthesis techniques can generate correct code, often even from incomplete but precise specifications, such as examples, but they are unable to work with the ambiguity of natural languages. We present an approach that combines PTMs with component-based synthesis (CBS): PTMs are used to generate candidates programs from the natural language description of the task, which are then used to guide the CBS procedure to find the program that matches the precise examples-based specification. We use our combination approach to instantiate multi-modal synthesis systems for two programming domains: the domain of regular expressions and the domain of CSS selectors. Our evaluation demonstrates the effectiveness of our domain-agnostic approach in comparison to a state-of-the-art specialized system, and the generality of our approach in providing multi-modal program synthesis from natural language and examples in different programming domains.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {158},
numpages = {29},
keywords = {GPT-3, Natural Language Models, Program Inference}
}

@inproceedings{10.1145/3719160.3736638,
author = {Mishra, Anchit and Schneider, Oliver},
title = {TacTalk: Personalizing Haptics Through Conversation},
year = {2025},
isbn = {9798400715273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719160.3736638},
doi = {10.1145/3719160.3736638},
abstract = {Haptic experiences are highly personal, but despite prior work exploring interfaces enabling personalization, we don’t know what process drives the personalization of haptics. To enable a study of this process, including users’ mental models and vocabularies, we introduce TacTalk, a conversational system enabling real time tuning of virtual haptic experiences. We present an application using TacTalk in a popular racing video game, Forza Horizon 5. Through an empirical study, we find that tracking user preference profiles may improve TacTalk’s ability to cater to individual differences, and that TacTalk is more usable than an existing slider-based personalization tool. A thematic analysis of participant interviews reveals an archetypal process of conversational personalization - starting with real-world experiences and domain-specific metaphors, then subsequently inspecting specific aspects of the experience including in-game events and the game controller.},
booktitle = {Proceedings of the 7th ACM Conference on Conversational User Interfaces},
articleno = {50},
numpages = {19},
keywords = {Haptics, Mental Models, Vocabulary, Personalization, Conversational Interfaces},
location = {
},
series = {CUI '25}
}

@inproceedings{10.1145/3672608.3707960,
author = {Malandri, Lorenzo and Mercorio, Fabio and Serino, Antonio},
title = {SkiLLMo: Normalized ESCO Skill Extraction through Transformer Models},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707960},
doi = {10.1145/3672608.3707960},
abstract = {In recent years, natural language processing (NLP) technologies have made a significant contribution in addressing a number of labour market tasks. One of the most interesting challenges is the automatic extraction of competences from unstructured texts.This paper presents a pipeline for efficiently extracting and standardizing skills from job advertisements using NLP techniques. The proposed methodology leverages open-source Transformer and Large Language Models to extract skills and map them to the European labour market taxonomy, ESCO.To address the computational challenges of processing lengthy job advertisements, a BERT model was fine-tuned to identify text segments likely containing skills. This filtering step reduces noise and ensures that only relevant content is processed further. The filtered text is then passed to an LLM, which extracts implicit and explicit hard and soft skills through prompt engineering. The extracted skills are subsequently matched with entries in a vector store containing the ESCO taxonomy to achieve standardization.Evaluation by domain experts shows that the pipeline achieves a precision of 91\% for skill extraction, 80\% for skill standardization and a combined overall precision of 79\%. These results demonstrate the effectiveness of the proposed approach in facilitating structured and standardized skill extraction from job postings.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1969–1978},
numpages = {10},
keywords = {skill extraction, large language models, transformer models, information extraction, labor market},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@article{10.1109/TASLP.2022.3198802,
author = {Zhang, Zhihao and Zuo, Yuan and Wu, Junjie},
title = {Aspect Sentiment Triplet Extraction: A Seq2Seq Approach With Span Copy Enhanced Dual Decoder},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3198802},
doi = {10.1109/TASLP.2022.3198802},
abstract = {Aspect Sentiment Triplet Extraction (ASTE) is a relatively new and very challenging task that attempts to provide an integral solution for aspect-based sentiment analysis. Aspect sentiment triplets in a sentence usually have overlaps when, e.g., one aspect is associated with multiple opinions and vice versa. Recently, end-to-end ASTE methods are becoming more and more popular for they can avoid the error propagation problem of pipeline-based methods. However, existing tagging-based end-to-end methods face difficulty to obtain a satisfactory recall, and generative methods fail to take a full account of the underlying interactions between aspects, opinions and their corresponding sentiments. In this paper, we formalize the ASTE task as a Seq2Seq learning problem with span copy mechanism for extracting multiple and possibly overlapped triplets. A novel dual decoder is devised purposefully for the ASTE task, where a multi-head attention based span copy mechanism is proposed to copy multi-token aspects and opinions. The dual decoder benefits from the rich output of encoder that can fuse multi-type information including word semantic, POS tag and BIO tag. Experiments on various benchmark datasets demonstrate that our approach achieves new state-of-the-art results. We also conduct analytical experiments to verify the effectiveness of various model components particularly for overlapped triplets extraction. We find that our model can be further improved through data augmentation and post-training.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2729–2742},
numpages = {14}
}

@inproceedings{10.1145/3689492.3690054,
author = {Marron, Mark},
title = {A Programming Language for Data and Configuration!},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689492.3690054},
doi = {10.1145/3689492.3690054},
abstract = {A day in the life of a developer often involves more time working with schemas, configurations, and data description systems than writing code and logic in a classical programming language. As more systems move into distributed worlds, e.g. cloud and microservices, and developers make increasing use of libraries and frameworks, the need to interact with a range of data formats and configuration mechanisms is only increasing. This is a treacherous world, where a misspelled property name or missing field can render an entire service inoperable, a mistake that a number in an API represents     seconds instead of milli-seconds can lead to a message being set for delivery in several months instead of in an hour, misconfigured schema can lead to public exposure of sensitive data, and corrupt or erroneous results from a misunderstood data format could result in massive financial and/or reputational damage.        To address these challenges this paper casts the problems of data and configuration descriptions, not as a problem of data representation, but as a type system problem, that can be addressed with well understood and highly effective programming language techniques! The novel challenge is that data representation and configuration are universal concerns in a system and, particularly in modern cloud or micro-service systems, these systems may involve many programming languages. In the past this has led to specification systems that use a least-common-denominator set of data types, often little more than strings and numbers, and then rely on conventions or (out-of-date) documentation to ensure that the data is interpreted correctly. This paper shows that, with careful design, it is possible to create a rich universal system that can be used to express data and configuration specifications in a way that is human readable/writable and that can be produced/consumed, much like JSON, by a wide range of programming languages and systems.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {147–161},
numpages = {15},
keywords = {Configuration, Data Specification, Programming Language},
location = {Pasadena, CA, USA},
series = {Onward! '24}
}

@inproceedings{10.1109/CGO57630.2024.10444873,
author = {Jangda, Abhinav and Maleki, Saeed and Dehnavi, Maryam Mehri and Musuvathi, Madan and Saarikivi, Olli},
title = {A Framework for Fine-Grained Synchronization of Dependent GPU Kernels},
year = {2024},
isbn = {9798350395099},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO57630.2024.10444873},
doi = {10.1109/CGO57630.2024.10444873},
abstract = {Machine Learning (ML) models execute several parallel computations including Generalized Matrix Multiplication, Convolution, Dropout, etc. These computations are commonly executed on Graphics Processing Units (GPUs), by dividing the computation into independent processing blocks, known as tiles. Since the number of tiles are usually higher than the execution units of a GPU, tiles are executed on all execution units in one or more waves. However, the number of tiles is not always a multiple of the number of execution units. Thus, tiles executed in the final wave can under-utilize the GPU.To address this issue, we present cuSync, a framework for synchronizing dependent kernels using a user-defined finegrained synchronization policy to improve the GPU utilization. cuSync synchronizes tiles instead of kernels, which allows executing independent tiles of dependent kernels concurrently. We also present a compiler to generate diverse fine-grained synchronization policies based on dependencies between kernels. Our experiments found that synchronizing CUDA kernels using cuSync reduces the inference times of four popular ML models: MegatronLM GPT-3 by up to 15\%, LLaMA by up to 14\%, ResNet-38 by up to 22\%, and VGG-19 by up to 16\% over several batch sizes.},
booktitle = {Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {93–105},
numpages = {13},
keywords = {CUDA, GPU, generalized matrix multiplication, convolution, fine-grained synchronization, machine learning},
location = {Edinburgh, United Kingdom},
series = {CGO '24}
}

@article{10.1145/3656420,
author = {Pelton, Blake and Sapek, Adam and Eguro, Ken and Lo, Daniel and Forin, Alessandro and Humphrey, Matt and Xi, Jinwen and Cox, David and Karandikar, Rajas and de Fine Licht, Johannes and Babin, Evgeny and Caulfield, Adrian and Burger, Doug},
title = {Wavefront Threading Enables Effective High-Level Synthesis},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {PLDI},
url = {https://doi.org/10.1145/3656420},
doi = {10.1145/3656420},
abstract = {Digital systems are growing in importance and computing hardware is growing more heterogeneous. Hardware design, however, remains laborious and expensive, in part due to the limitations of conventional hardware description languages (HDLs) like VHDL and Verilog. A longstanding research goal has been programming hardware like software, with high-level languages that can generate efficient hardware designs. This paper describes Kanagawa, a language that takes a new approach to combine the programmer productivity benefits of traditional High-Level Synthesis (HLS) approaches with the expressibility and hardware efficiency of Register-Transfer Level (RTL) design. The language’s concise syntax, matched with a hardware design-friendly execution model, permits a relatively simple toolchain to map high-level code into efficient hardware implementations.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {190},
numpages = {25},
keywords = {Hardware Threading, Wavefront Threading, Wavefront Consistency}
}

@article{10.1145/3702234,
author = {Fang, Chen and Wang, Yidong and Song, Yunze and Long, Qingqing and Lu, Wang and Chen, Linghui and Feng, Guihai and Zhou, Yuanchun and Li, Xin},
title = {How do Large Language Models understand Genes and Cells},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3702234},
doi = {10.1145/3702234},
abstract = {Researching genes and their interactions is crucial for deciphering the fundamental laws of cellular activity, advancing disease treatment, drug discovery, and more. Large language Models (LLMs), with their profound text comprehension and generation capabilities, have made significant strides across various natural science fields. However, their application in cell biology remains limited and a systematic evaluation of their performance is lacking. To address this gap, in this paper, we select seven mainstream LLMs and evaluate their performance across nine gene-related problem scenarios. Our findings indicate that LLMs possess a certain level of understanding of genes and cells, but still lag behind domain-specific models in comprehending transcriptional expression profiles. Moreover, we have improved the current method of textual representation of cells, enhancing the LLMs’ ability to tackle cell annotation tasks. We encourage cell biology researchers to leverage LLMs for problem-solving while being mindful of the associated challenges. We release our code and data at .},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
keywords = {large language models, cell biology, gene gene interaction, cell annotation}
}

@article{10.1145/3657299,
author = {Xia, Bolun (Namir) and Rawte, Vipula and Gupta, Aparna and Zaki, Mohammed},
title = {FETILDA: Evaluation Framework for Effective Representations of Long Financial Documents},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {7},
issn = {1556-4681},
url = {https://doi.org/10.1145/3657299},
doi = {10.1145/3657299},
abstract = {In the financial sphere, there is a wealth of accumulated unstructured financial data, such as the textual disclosure documents that companies submit on a regular basis to regulatory agencies, such as the Securities and Exchange Commission. These documents are typically very long and tend to contain valuable soft information about a company’s performance that is not present in quantitative predictors. It is therefore of great interest to learn predictive models from these long textual documents, especially for forecasting numerical key performance indicators. In recent years, there has been great progress in natural language processing via pre-trained language models (LMs) learned from large corpora of textual data. This prompts the important question of whether they can be used effectively to produce representations for long documents, as well as how we can evaluate the effectiveness of representations produced by various LMs. Our work focuses on answering this critical question, namely, the evaluation of the efficacy of various LMs in extracting useful soft information from long textual documents for prediction tasks. In this article, we propose and implement a deep learning evaluation framework that utilizes a sequential chunking approach combined with an attention mechanism. We perform an extensive set of experiments on a collection of 10-K reports submitted annually by U.S. banks, and another dataset of reports submitted by U.S. companies, to investigate thoroughly the performance of different types of language models. Overall, our framework using LMs outperforms strong baseline methods for textual modeling as well as for numerical regression. Our work provides better insights into how utilizing pre-trained domain-specific and fine-tuned long-input LMs for representing long documents can improve the quality of representation of textual data and, therefore, help in improving predictive analyses.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {182},
numpages = {27},
keywords = {Text regression, language models, long text documents, financial documents, 10-K reports}
}

@article{10.1145/3674971,
author = {Ellouze, Mourad and Hadrich Belguith, Lamia},
title = {Artificial Intelligence application for the analysis of personality traits and disorders in social media: A Survey},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3674971},
doi = {10.1145/3674971},
abstract = {Personality analysis has a positive influence on humanity as it aids in identifying personality traits and disorders. In addition, it facilitates the monitoring of cases and enriches doctors’ knowledge bases, particularly in decision-making processes. This study includes a comprehensive literature review on personality analysis approaches from social media, aiming to gain a thorough understanding of the current studies on personality therapy. Moreover, the objective of this study is to identify various limitations present in these studies and explore potential avenues for enhancement. More specifically, this research begins with an introduction that discusses the main concepts of traits and personality disorders, as well as the importance of psychological analysis. Following that, four cluster studies related to personality analysis on social media are presented: personality traits, personality disorders, detection of links between diseases, and monitoring patient status. Then, the majority of the currently available works for each cluster are exposed. Afterward, a comparative study of the different presented works is proposed. Finally, an outline of plans for further research in this area is provided, detailing potential paths for exploration.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
keywords = {Social Media, Personality Traits, Personality Disorders, Artificial Intelligence, Text Mining, Natural Language Processing}
}

@inproceedings{10.1145/3706598.3713708,
author = {Gebreegziabher, Simret Araya and Yang, Yukun and Glassman, Elena L. and Li, Toby Jia-Jun},
title = {Supporting Co-Adaptive Machine Teaching through Human Concept Learning and Cognitive Theories},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713708},
doi = {10.1145/3706598.3713708},
abstract = {An important challenge in interactive machine learning, particularly in subjective or ambiguous domains, is fostering bi-directional alignment between humans and models. Users teach models their concept definition through data labeling, while refining their own understandings throughout the process. To facilitate this, we introduce Mocha, an interactive machine learning tool informed by two theories of human concept learning and cognition. First, it utilizes a neuro-symbolic pipeline to support Variation Theory-based counterfactual data generation. By asking users to annotate counterexamples that are syntactically and semantically similar to already-annotated data but predicted to have different labels, the system can learn more effectively while helping users understand the model and reflect on their own label definitions. Second, Mocha uses Structural Alignment Theory to present groups of counterexamples, helping users comprehend alignable differences between data items and annotate them in batch. We validated Mocha’s effectiveness and usability through a lab study with 18 participants.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {533},
numpages = {18},
keywords = {human-AI collaboration, machine teaching, variation theory, structural alignment theory},
location = {
},
series = {CHI '25}
}

@article{10.1145/3712296,
author = {Sun, Wei and Li, Mingxiao and Sileo, Damien and Davis, Jesse and Moens, Marie-Francine},
title = {Generating Explanations in Medical Question-Answering by Expectation Maximization Inference over Evidence},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3712296},
doi = {10.1145/3712296},
abstract = {Medical Question Answering (medical QA) systems play an essential role in assisting healthcare workers in finding answers to their questions. However, it is not sufficient to merely provide answers by medical QA systems because users might want explanations, that is, more analytic statements in natural language that describe the elements and context that support the answer. To do so, we propose a novel approach for generating natural language explanations for answers predicted by medical QA systems. As high-quality medical explanations require additional medical knowledge, so that our system extracts knowledge from medical textbooks to enhance the quality of explanations during the explanation generation process. Concretely, we designed an Expectation-Maximization approach that makes inferences about the evidence found in these texts, offering an efficient way to focus attention on lengthy evidence passages. Experimental results, conducted on two datasets MQAE-diag and MQAE, demonstrate the effectiveness of our framework for reasoning with textual evidence. Our approach outperforms state-of-the-art models, achieving a significant improvement of 6.13 and 5.47 percentage points on the Rouge-L score; 6.49 and 5.28 percentage points on the Bleu-4 score on the MQAE-diag and MQAE datasets.},
journal = {ACM Trans. Comput. Healthcare},
month = apr,
articleno = {23},
numpages = {23},
keywords = {Expectation Maximization, Medical Question Answering, Explanation Generation}
}

@inproceedings{10.1145/3659677.3659691,
author = {Ouaddi, Charaf and Benaddi, Lamya and Souha, Adnane and Jakimi, Abeslam and Ouchao, Brahim and Saadane, Rachid},
title = {Exploring and Analyzing the Impact of Chatbots in Tourism Industry},
year = {2024},
isbn = {9798400709296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659677.3659691},
doi = {10.1145/3659677.3659691},
abstract = {Nowadays, the fast evolution of artificial intelligence (AI) technologies and the increasing amount of data are posing a challenge for the tourism sector, paving the way for the massive use of smart software applications; one of these applications is chatbots, which are computer programs designed to simulate conversation with human users using natural language. They are most beneficial due to the fast response times and facility-to-use. However, there has been a lack of academic research on the impact of chatbots on the tourism sector so far. and also, building chatbots is a tricky task for software developers, requiring a wide array of AI skills. Therefore, this work aims to explore the impact of AI-based chatbots on tourism through an in-depth analysis of the six As components for analyzing tourism destinations. Then, the tools for developing chatbots are examined.},
booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
articleno = {7},
numpages = {6},
keywords = {Artificial intelligence, Chatbot, Framework, Platform, Smart-tourism, Tourism},
location = {Meknes, AA, Morocco},
series = {NISS '24}
}

@inproceedings{10.1109/MICRO56248.2022.00051,
author = {Hong, Seongmin and Moon, Seungjae and Kim, Junsoo and Lee, Sungjae and Kim, Minsub and Lee, Dongsoo and Kim, Joo-Young},
title = {DFX: A Low-Latency Multi-FPGA Appliance for Accelerating Transformer-Based Text Generation},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00051},
doi = {10.1109/MICRO56248.2022.00051},
abstract = {Transformer is a deep learning language model widely used for natural language processing (NLP) services in datacenters. Among transformer models, Generative Pre-trained Transformer (GPT) has achieved remarkable performance in text generation, or natural language generation (NLG), which needs the processing of a large input context in the summarization stage, followed by the generation stage that produces a single word at a time. The conventional platforms such as GPU are specialized for the parallel processing of large inputs in the summarization stage, but their performance significantly degrades in the generation stage due to its sequential characteristic. Therefore, an efficient hardware platform is required to address the high latency caused by the sequential characteristic of text generation.In this paper, we present DFX, a multi-FPGA acceleration appliance that executes GPT-2 model inference end-to-end with low latency and high throughput in both summarization and generation stages. DFX uses model parallelism and optimized dataflow that is model-and-hardware-aware for fast simultaneous workload execution among devices. Its compute cores operate on custom instructions and provide GPT-2 operations end-to-end. We implement the proposed hardware architecture on four Xilinx Alveo U280 FPGAs and utilize all of the channels of the high bandwidth memory (HBM) and the maximum number of compute resources for high hardware efficiency. DFX achieves 5.58\texttimes{} speedup and 3.99\texttimes{} energy efficiency over four NVIDIA V100 GPUs on the modern GPT-2 model. DFX is also 8.21\texttimes{} more cost-effective than the GPU appliance, suggesting that it is a promising solution for text generation workloads in cloud datacenters.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {616–630},
numpages = {15},
keywords = {natural language processing, GPT, text generation, datacenter, multi-FPGA acceleration, model parallelism},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1145/3689096.3689458,
author = {Gautam, Sushant and Stor\r{a}s, Andrea M. and Midoglu, Cise and Hicks, Steven A. and Thambawita, Vajira and Halvorsen, P\r{a}l and Riegler, Michael A.},
title = {Kvasir-VQA: A Text-Image Pair GI Tract Dataset},
year = {2024},
isbn = {9798400712074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689096.3689458},
doi = {10.1145/3689096.3689458},
abstract = {We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and Kvasir-Instrument datasets, augmented with question- and-answer annotations to facilitate advanced machine learning tasks in Gastrointestinal (GI) diagnostics. This dataset comprises 6,500 annotated images spanning various GI tract conditions and surgical instruments, and it supports multiple question types including yes/no, choice, location, and numerical count. The dataset is intended for applications such as image captioning, Visual Question Answering (VQA), text-based generation of synthetic medical images, object detection, and classification. Our experiments demonstrate the dataset's effectiveness in training models for three selected tasks, showcasing significant applications in medical image analysis and diagnostics. We also present evaluation metrics for each task, highlighting the usability and versatility of our dataset. The dataset and supporting artifacts are available at https://datasets.simula.no/kvasir-vqa.},
booktitle = {Proceedings of the First International Workshop on Vision-Language Models for Biomedical Applications},
pages = {3–12},
numpages = {10},
keywords = {gastrointestinal diagnostics, machine learning in healthcare, medical image analysis, medical image captioning, visual question answering (vqa)},
location = {Melbourne VIC, Australia},
series = {VLM4Bio'24}
}

@inproceedings{10.1145/3716489.3728431,
author = {Mirindi, Derrick and Sinkhonde, David and Mirindi, Frederic and Bezabith, Tajebe},
title = {Advanced evaluation of BIM-GenAI using OpenAI o1 and ethical considerations},
year = {2025},
isbn = {9798400714979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716489.3728431},
doi = {10.1145/3716489.3728431},
abstract = {The rapid advancement of artificial intelligence (AI) has led the AI community to speculate that artificial superintelligence (ASI) may be within reach, particularly if an AI system can iteratively search for solutions, learn from results, and leverage improved knowledge for more searches. In this context, this study explores the integration of Generative Artificial Intelligence (GenAI) into Building Information Modeling (BIM) by focusing on the four key pillars of the OpenAI o1 model and their ethical implications. Through comprehensive analysis of existing literature, we examine these pillars—policy initialization, reward design, search strategies, and learning mechanisms—and their application in BIM-GenAI within a continuous improvement cycle. Results demonstrate that policy initialization generates human-like reasoning behaviors and domain-specific knowledge for BIM tasks. Reward design, central to reinforcement learning, optimizes BIM objectives through measurable metrics and learned evaluation methods. Search strategies prove valuable for exploring complex design spaces and generating high-quality BIM solutions, while learning mechanisms, including policy gradient and behavior cloning, enable continuous model improvement through feedback. The study emphasizes the importance of establishing BIM-AI protocols, maintaining human expertise in decision-making, and balancing automation with human input. Our findings suggest that while GenAI, powered by reinforcement learning, offers significant potential for enhancing BIM capabilities, three critical ethical considerations—data privacy and security, algorithmic bias mitigation, and transparency and accountability—must guide responsible implementation. This research contributes to the growing body of knowledge on AI in construction technologies and provides a foundation for the ethical advancement of BIM-GenAI systems using OpenAI o1.},
booktitle = {Proceedings of the 2025 Computers and People Research Conference},
articleno = {1},
numpages = {8},
keywords = {Artificial Intelligence, Building Information Modeling, Generative Artificial Intelligence, OpenAI o1},
location = {
},
series = {SIGMIS-CPR '25}
}

@inproceedings{10.1145/3638530.3664157,
author = {Coombe, Cameron and Howard, David and Browne, Will},
title = {Learning Classifier Systems as a Solver for the Abstraction and Reasoning Corpus},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664157},
doi = {10.1145/3638530.3664157},
abstract = {The abstraction and reasoning corpus (ARC) is a challenging AI benchmark as it requires models to learn unseen relationships from a few data points. Each puzzle only contains 2--5 training examples, which makes it hard for models that require training on large datasets. Models which do not require training on large datasets like learning classifier systems (LCSs) have the potential to solve this kind of complex, low-data problem due to their flexible representation, niche-based learning, and ability to generalise. Whilst some learners that can operate on low data have been applied to ARC, LCS-based architectures remain entirely unexplored. We propose a simple LCS architecture employing a windowing approach. This architecture solves 19 of 400 test grids (4.75\%) in the ARC training set, which shows promise by outperforming other na\"{\i}ve approaches. Additionally, the system uses minimal prior knowledge to achieve this result, bringing it closer to the original vision of an ARC solver, which is to only rely on a core set of concepts. We provide directions on how this basic model could be expanded upon to include more complex structures making use of LCSs' ability to integrate many diverse kinds of representations.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1770–1778},
numpages = {9},
keywords = {learning classifier systems, abstraction, abstraction and reasoning corpus},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@article{10.1145/3674735,
author = {Thangamani, Arun and Loechner, Vincent and Genaud, St\'{e}phane},
title = {A Survey of General-purpose Polyhedral Compilers},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3674735},
doi = {10.1145/3674735},
abstract = {Since the 1990s, many implementations of polyhedral compilers have been written and distributed, either as source-to-source translating compilers or integrated into wider-purpose compilers. This article provides a survey on those various available implementations as of today, 2024.First, we list and describe most commonly available polyhedral schedulers and compiler implementations. Then, we compare the general-purpose polyhedral compilers using two main criteria—robustness and performance—on the PolyBench/C set of benchmarks.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {72},
numpages = {26},
keywords = {Polyhedral compilation, benchmark, performance analysis}
}

@inproceedings{10.1145/3611643.3616317,
author = {Du, Xueying and Lou, Yiling and Liu, Mingwei and Peng, Xin and Yang, Tianyong},
title = {KG4CraSolver: Recommending Crash Solutions via Knowledge Graph},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616317},
doi = {10.1145/3611643.3616317},
abstract = {Fixing crashes is challenging, and developers often discuss their encountered crashes and refer to similar crashes and solutions on online Q&amp;A forums (e.g., Stack Overflow). However, a crash often involves very complex contexts, which includes different contextual elements, e.g., purposes, environments, code, and crash traces. Existing crash solution recommendation or general solution  recommendation techniques only use an incomplete context or treat the entire context as pure texts to search relevant solutions for a given crash, resulting in inaccurate recommendation results. In this work, we propose a novel crash solution knowledge graph (KG) to summarize the complete crash context and its solution with a graph-structured representation. To construct the crash solution KG automatically, we propose to leverage prompt learning to construct the KG from SO threads with a small set of labeled data. Based on the constructed KG, we further propose a novel KG-based crash solution recommendation technique KG4CraSolver, which precisely finds the relevant SO thread for an encountered crash by finely analyzing and matching the complete crash context based on the crash  solution KG. The evaluation results show that the constructed KG is of high quality and KG4CraSolver outperforms baselines in terms of all metrics (e.g., 13.4\%-113.4\% MRR improvements). Moreover, we perform a user study and find that KG4CraSolver helps participants find crash solutions 34.4\% faster and 63.3\% more accurately.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1242–1254},
numpages = {13},
keywords = {Crash Solution Recommendation, Knowledge Graph, Stack Overflow},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3613904.3641910,
author = {Wu, Zihan and Ericson, Barbara J.},
title = {SQL Puzzles: Evaluating Micro Parsons Problems With Different Feedbacks as Practice for Novices},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641910},
doi = {10.1145/3613904.3641910},
abstract = {This paper investigates using micro Parsons problems as a novel practice approach for learning Structured Query Language (SQL). In micro Parsons problems learners arrange predefined code fragments to form a SQL statement instead of typing the code. SQL is a standard language for working with relational databases. Targeting beginner-level SQL statements, we evaluated the efficacy of micro Parsons problems with block-based feedback and execution-based feedback compared to traditional text-entry problems. To delve into learners’ experiences and preferences for the three problem types, we conducted a within-subjects think-aloud study with 12 participants. We found that learners reported very different preferences. Factors they considered included perceived learning, task authenticity, and prior knowledge. Next, we conducted two between-subjects classroom studies to evaluate the effectiveness of micro Parsons problems with different feedback types versus text-entry problems for SQL practice. We found that learners who practiced by solving Parsons problems with block-based feedback had a significantly higher learning gain than those who practiced with traditional text-entry problems.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {930},
numpages = {15},
keywords = {SQL education, empirical study, learning, programming puzzle},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3691620.3695520,
author = {Tang, Shuncheng and Zhang, Zhenya and Zhou, Jixiang and Lei, Lei and Zhou, Yuan and Xue, Yinxing},
title = {LeGEND: A Top-Down Approach to Scenario Generation of Autonomous Driving Systems Assisted by Large Language Models},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695520},
doi = {10.1145/3691620.3695520},
abstract = {Autonomous driving systems (ADS) are safety-critical and require comprehensive testing before their deployment on public roads. While existing testing approaches primarily aim at the criticality of scenarios, they often overlook the diversity of the generated scenarios that is also important to reflect system defects in different aspects. To bridge the gap, we propose LeGEND, that features a top-down fashion of scenario generation: it starts with abstract functional scenarios, and then steps downwards to logical and concrete scenarios, such that scenario diversity can be controlled at the functional level. However, unlike logical scenarios that can be formally described, functional scenarios are often documented in natural languages (e.g., accident reports) and thus cannot be precisely parsed and processed by computers. To tackle that issue, LeGEND leverages the recent advances of large language models (LLMs) to transform textual functional scenarios to formal logical scenarios. To mitigate the distraction of useless information in functional scenario description, we devise a two-phase transformation that features the use of an intermediate language; consequently, we adopt two LLMs in LeGEND, one for extracting information from functional scenarios, the other for converting the extracted information to formal logical scenarios. We experimentally evaluate LeGEND on Apollo, an industry-grade ADS from Baidu. Evaluation results show that LeGEND can effectively identify critical scenarios, and compared to baseline approaches, LeGEND exhibits evident superiority in diversity of generated scenarios. Moreover, we also demonstrate the advantages of our two-phase transformation framework, and the accuracy of the adopted LLMs.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1497–1508},
numpages = {12},
keywords = {autonomous driving systems, critical scenario generation, large language models},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3729319,
author = {He, Yang and Fang, Ruijie and Dillig, I\c{s}\i{}l and Wang, Yuepeng},
title = {Graphiti: Bridging Graph and Relational Database Queries},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {PLDI},
url = {https://doi.org/10.1145/3729319},
doi = {10.1145/3729319},
abstract = {This paper presents an automated reasoning technique for checking equivalence between graph database queries written in Cypher and relational queries in SQL. To formalize a suitable notion of equivalence in this setting, we introduce the concept of database transformers, which transform database instances between graph and relational models. We then propose a novel verification methodology that checks equivalence modulo a given transformer by reducing the original problem to verifying equivalence between a pair of SQL queries. This reduction is achieved by embedding a subset of Cypher into SQL through syntax-directed translation, allowing us to leverage existing research on automated reasoning for SQL while obviating the need for reasoning simultaneously over two different data models. We have implemented our approach in a tool called Graphiti and used it to check equivalence between graph and relational queries. Our experiments demonstrate that Graphiti is useful both for verification and refutation and that it can uncover subtle bugs, including those found in Cypher tutorials and academic papers.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {216},
numpages = {25},
keywords = {Equivalence Checking, Graph Databases, Program Verification, Relational Databases}
}

@inproceedings{10.1109/ASE56229.2023.00117,
author = {Ramos, Daniel and Mitchell, Hailie and Lynce, In\^{e}s and Manquinho, Vasco and Martins, Ruben and Goues, Claire Le},
title = {MELT: Mining Effective Lightweight Transformations from Pull Requests},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00117},
doi = {10.1109/ASE56229.2023.00117},
abstract = {Software developers often struggle to update APIs, leading to manual, time-consuming, and error-prone processes. We introduce Melt, a new approach that generates lightweight API migration rules directly from pull requests in popular library repositories. Our key insight is that pull requests merged into open-source libraries are a rich source of information sufficient to mine API migration rules. By leveraging code examples mined from the library source and automatically generated code examples based on the pull requests, we infer transformation rules in Comby, a language for structural code search and replace. Since inferred rules from single code examples may be too specific, we propose a generalization procedure to make the rules more applicable to client projects. Melt rules are syntax-driven, interpretable, and easily adaptable. Moreover, unlike previous work, our approach enables rule inference to seamlessly integrate into the library workflow, removing the need to wait for client code migrations. We evaluated Melt on pull requests from four popular libraries, successfully mining 461 migration rules from code examples in pull requests and 114 rules from autogenerated code examples. Our generalization procedure increases the number of matches for mined rules by 9\texttimes{}. We applied these rules to client projects and ran their tests, which led to an overall decrease in the number of warnings and fixing some test cases demonstrating MELT's effectiveness in real-world scenarios.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1516–1528},
numpages = {13},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3649329.3663500,
author = {Masouros, Dimosthenis and Ferikoglou, Aggelos and Zervakis, Georgios and Xydis, Sotirios and Soudris, Dimitrios},
title = {Late Breaking Results: Language-level QoR modeling for High-Level Synthesis},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3663500},
doi = {10.1145/3649329.3663500},
abstract = {This paper proposes a language-level modeling approach for HighLevel Synthesis based on the state-of-the-art Transformer architecture. Our approach estimates the performance and required resources of HLS applications directly from the source code when different synthesis directives, in terms of HLS #pragmas, are applied. Results show that the proposed architecture achieves 96.02\% accuracy for predicting the feasibility class of applications and an average of 0.95 and 0.91 R2 scores for predicting the actual performance and required resources, respectively.},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {351},
numpages = {2},
location = {San Francisco, CA, USA},
series = {DAC '24}
}

@inproceedings{10.1145/3726302.3729901,
author = {Sakhovskiy, Andrey and Tutubalina, Elena},
title = {BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3729901},
doi = {10.1145/3726302.3729901},
abstract = {In recent years, there has been substantial progress in using pretrained Language Models (LMs) on a range of tasks aimed at improving the understanding of biomedical texts. Nonetheless, existing biomedical LLMs show limited comprehension of complex, domain-specific concept structures and the factual information encoded in biomedical Knowledge Graphs (KGs). In this work, we propose BALI (Biomedical Knowledge Graph and Language Model Ali gnment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph. For a given textual sequence, we link biomedical concept mentions to the Unified Medical Language System (UMLS) KG and utilize local KG subgraphs as cross-modal positive samples for these mentions. Our empirical findings indicate that implementing our method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations, even with minimal pre-training on a small alignment dataset sourced from PubMed scientific abstracts.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1152–1164},
numpages = {13},
keywords = {biomedical knowledge graph, biomedical language model, biomedical natural language processing, contrastive learning, natural language processing, representation learning},
location = {Padua, Italy},
series = {SIGIR '25}
}

@article{10.1145/3563327,
author = {Bavishi, Rohan and Joshi, Harshit and Cambronero, Jos\'{e} and Fariha, Anna and Gulwani, Sumit and Le, Vu and Radi\v{c}ek, Ivan and Tiwari, Ashish},
title = {Neurosymbolic repair for low-code formula languages},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563327},
doi = {10.1145/3563327},
abstract = {Most users of low-code platforms, such as Excel and PowerApps, write programs in domain-specific formula languages to carry out nontrivial tasks. Often users can write most of the program they want, but introduce small mistakes that yield broken formulas. These mistakes, which can be both syntactic and semantic, are hard for low-code users to identify and fix, even though they can be resolved with just a few edits. We formalize the problem of producing such edits as the last-mile repair problem. To address this problem, we developed LaMirage, a LAst-MIle RepAir-engine GEnerator that combines symbolic and neural techniques to perform last-mile repair in low-code formula languages. LaMirage takes a grammar and a set of domain-specific constraints/rules, which jointly approximate the target language, and uses these to generate a repair engine that can fix formulas in that language. To tackle the challenges of localizing errors and ranking candidate repairs, LaMirage leverages neural techniques, whereas it relies on symbolic methods to generate candidate edits. This combination allows LaMirage to find repairs that satisfy the provided grammar and constraints, and then pick the most natural repair. We compare LaMirage to state-of-the-art neural and symbolic approaches on 400 real Excel and Power Fx formulas, where LaMirage outperforms all baselines. We release these benchmarks to encourage subsequent work in low-code domains.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {164},
numpages = {30},
keywords = {Program Repair, Neurosymbolic, Low-Code}
}

@article{10.1145/3758967,
author = {Ravi, Kamalakkannan and Yuan, Jiann-Shiun},
title = {ALERT: Active Learning and Explainable AI for Robust Threat Detection in Telegram},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3758967},
doi = {10.1145/3758967},
abstract = {The increasing regulatory scrutiny of social media, particularly regarding extremist content and misinformation, underscores the need for advanced threat detection systems. This paper presents ALERT (Active Learning and Explainable AI for Robust Threat Detection in Telegram), a novel framework that enhances threat classification by introducing refined categories and creating tailored datasets1. ALERT processes 2,301,110 replies from 17 Telegram channels, focusing on extreme content, with a dataset that predominantly reflects far-right discourse, consistent with activity trends on the platform. By leveraging an iterative active learning approach, it reduces labeling efforts by 86.5\%, yielding a labeled dataset of 15,076 replies. ALERT's RoBERTa+ model, pre-trained on domain-specific data, achieved over 90\% in precision, recall, accuracy, and F1-score, demonstrating strong generalization for threat detection. The integrated explainable AI (XAI) modules highlight key text features driving model predictions, ensuring transparency while maintaining performance. ALERT offers significant improvements in classification precision and user confidence, providing a critical tool2 for addressing digital threats while navigating regulatory and privacy challenges.},
note = {Just Accepted},
journal = {Digital Threats},
month = aug,
keywords = {active learning, cyberbullying, explainable AI, extremism, Learning (artificial intelligence), natural language processing, online radicalization, political violence, social media, telegram, user-generated content}
}

@inproceedings{10.1109/SC41406.2024.00038,
author = {Herten, Andreas and Achilles, Sebastian and Alvarez, Damian and Badwaik, Jayesh and Behle, Eric and Bode, Mathis and Breuer, Thomas and Caviedes-Voulli\`{e}me, Daniel and Cherti, Mehdi and Dabah, Adel and El Sayed, Salem and Frings, Wolfgang and Gonzalez-Nicolas, Ana and Gregory, Eric B. and Mood, Kaveh Haghighi and Hater, Thorsten and Jitsev, Jenia and John, Chelsea Maria and Meinke, Jan H. and Meyer, Catrin I. and Mezentsev, Pavel and Mirus, Jan-Oliver and Nassyr, Stepan and Penke, Carolin and R\"{o}mmer, Manoel and Sinha, Ujjwal and von St. Vieth, Benedikt and Stein, Olaf and Suarez, Estela and Willsch, Dennis and Zhukov, Ilya},
title = {Application-Driven Exascale: The JUPITER Benchmark Suite},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00038},
doi = {10.1109/SC41406.2024.00038},
abstract = {Benchmarks are essential in the design of modern HPC installations, as they define key aspects of system components. Beyond synthetic workloads, it is crucial to include real applications that represent user requirements into benchmark suites, to guarantee high usability and widespread adoption of a new system. Given the significant investments in leadership-class supercomputers of the exascale era, this is even more important and necessitates alignment with a vision of Open Science and reproducibility. In this work, we present the JUPITER Benchmark Suite, which incorporates 16 applications from various domains. It was designed for and used in the procurement of JUPITER, the first European exascale supercomputer. We identify requirements and challenges and outline the project and software infrastructure setup. We provide descriptions and scalability studies of selected applications and a set of key takeaways. The JUPITER Benchmark Suite is released as open source software with this work at github.com/FZJ-JSC/jubench.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {32},
numpages = {45},
keywords = {Accelerator, Benchmark, Exascale, GPU, Procurement, System Architecture, System Design},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@article{10.1145/3712701,
author = {Lee, Seungpil and Sim, Woochang and Shin, Donghyeon and Seo, Wongyu and Park, Jiwon and Lee, Seokki and Hwang, Sanha and Kim, Sejin and Kim, Sundong},
title = {Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3712701},
doi = {10.1145/3712701},
abstract = {The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been predominantly results-centric, making it challenging to assess the inference process comprehensively. We introduce a novel approach using the Abstraction and Reasoning Corpus (ARC) benchmark to evaluate the inference and contextual understanding abilities of LLMs in a process-centric manner, focusing on three key components from the Language of Thought Hypothesis (LoTH): Logical Coherence, Compositionality, and Productivity. Our carefully designed experiments reveal that while LLMs demonstrate some inference capabilities, they still significantly lag behind human-level reasoning in these three aspects. The main contribution of this paper lies in introducing the LoTH perspective, which provides a method for evaluating the reasoning process that conventional results-oriented approaches fail to capture, thereby offering new insights into the development of human-level reasoning in artificial intelligence systems.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
keywords = {Large Language Models, Abstraction and Reasoning Corpus, Language of Thought Hypothesis, Logical Coherence, Compositionality, Productivity}
}

@article{10.1145/3715069,
author = {Varshney, Deeksha and Behera, Niranshu and Katari, Prajeet and Ekbal, Asif},
title = {MedProm: Bridging Dialogue Gaps in Healthcare with Knowledge-Enhanced Generative Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715069},
doi = {10.1145/3715069},
abstract = {In medical dialogue systems, recent advancements underscore the critical role of incorporating relevant medical knowledge to enhance performance. However, existing knowledge bases often lack completeness, posing a challenge in sourcing pertinent information. We present MedProm, a novel generative model tailored for medical dialogue generation to address this gap. Motivated by the need for comprehensive and contextually relevant responses, MedProm leverages state-of-the-art language models such as BioGPT. Our model is designed to integrate extensive medical knowledge into conversations, facilitating effective communication between patients and healthcare providers. At the core of MedProm lies the MediConnect Graph, a meticulously constructed knowledge graph capturing intricate relationships among medical entities extracted from dialogue contexts. By employing a KnowFusion encoder with a pretraining objective and masked multi-head self-attention, MedProm effectively processes the MediConnect graph, enabling precise control over information flow to capture its underlying structure. Furthermore, MedProm incorporates a sophisticated Curriculum Knowledge Decoder, leveraging transformer-based decoding to generate response utterances conditioned on input representations from the KnowFusion Encoder. The training process is guided through curriculum learning, gradually increasing optimization difficulty based on a coherence-based criterion. Experimental results on two datasets demonstrate the efficacy of MedProm in generating accurate and contextually relevant responses compared to state-of-the-art models.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = jan,
keywords = {Medical Dialogue Systems (MDS), Generative Neural Model, MediConnect Graph, KnowFusion Encoder, Curriculum Knowledge Decoder}
}

@inproceedings{10.1145/3627673.3680009,
author = {Agrawal, Sanjay and Merugu, Srujana and Sembium, Vivek},
title = {Boosting Entity Recognition by leveraging Cross-task Domain Models for Weak Supervision},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3680009},
doi = {10.1145/3627673.3680009},
abstract = {Entity Recognition (ER) is a common natural language processing task encountered in a number of real-world applications. For common domains and named entities such as places and organisations, there exists sufficient high quality annotated data and foundational models such as T5 and GPT-3.5 also provide highly accurate predictions. However, for niche domains such as e-commerce and medicine with specialized entity types, there is a paucity of labeled data since manual labeling of tokens is often time-consuming and expensive, which makes entity recognition challenging for such domains. Recent works such as NEEDLE [48] propose hybrid solutions to efficiently combine a small amount of strongly labeled (human-annotated) with a large amount of weakly labeled (distant supervision) data to yield superior performance relative to supervised training. The extensive noise in the weakly labeled data, however, remains a challenge. In this paper, we propose WeSDoM (Weak Supervision with Domain Models), which leverages pretrained encoder models from the same domain but different tasks to create domain ontologies that can enable the creation of less noisy weakly labeled data. Experiments on internal e-commerce and public biomedical NER datasets demonstrate that WeSDoM outperforms existing SOTA baselines by a significant margin. We achieve new SOTA F1 scores on two popular Biomedical NER datasets, BC5CDR-chem 94.27, BC5CDR-disease 91.23.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {4324–4331},
numpages = {8},
keywords = {cross-task domain encoder, entity recognition, ontologies, weak supervision},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3615318.3615329,
author = {Adam, Julien and Besnard, Jean-Baptiste and Canat, Paul and Taboada, Hugo and Roussel, Adrien and P\'{e}rache, Marc and Jaeger, Julien and Shende, Sameer},
title = {Generating and Scaling a Multi-Language Test-Suite for MPI},
year = {2023},
isbn = {9798400709135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615318.3615329},
doi = {10.1145/3615318.3615329},
abstract = {High-Performance Computing (HPC) is currently facing significant challenges. The hardware pressure has become increasingly difficult to manage due to the lack of parallel abstractions in applications. As a result, parallel programs must undergo drastic evolution to effectively exploit underlying hardware parallelism. Failure to do so results in inefficient code. In this constrained environment, parallel runtimes play a critical role, and their testing becomes crucial. This paper focuses on the MPI interface and leverages the MPI binding tools to develop a multi-language test suite for MPI. By doing so and building on previous work from the Forum document editors, we implement a systematic testing of MPI symbols in the context of the Parallel Computing Validation System (PCVS), which is an HPC validation platform dedicated to running and managing test suites at scale. We first describe PCVS, then outline the process of generating the MPI API test suite, and finally, run these tests at scale. All data sets, code generators, and implementations are made available in open-source to the community. We also set up a dedicated website showcasing the results, which self-updates thanks to the Spack package manager.},
booktitle = {Proceedings of the 30th European MPI Users' Group Meeting},
articleno = {11},
numpages = {10},
keywords = {HPC, MPI, api, test, validation},
location = {Bristol, United Kingdom},
series = {EuroMPI '23}
}

@article{10.1145/3729316,
author = {Ferreira, Margarida and Nicolet, Victor and Dodds, Joey and Kroening, Daniel},
title = {Program Synthesis from Partial Traces},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {PLDI},
url = {https://doi.org/10.1145/3729316},
doi = {10.1145/3729316},
abstract = {We present the first technique to synthesize programs that compose side-effecting functions, pure functions, and control flow, from partial traces containing records of only the side-effecting functions. This technique can be applied to synthesize API composing scripts from logs of calls made to those APIs, or a script from traces of system calls made by a workload, for example. All of the provided traces are positive examples, meaning that they describe desired behavior. Our approach does not require negative examples. Instead, it generalizes over the examples and uses cost metrics to prevent over-generalization. Because the problem is too complex for traditional monolithic program synthesis techniques, we propose a new combination of optimizing rewrites and syntax-guided program synthesis. The resulting program is correct by construction, so its output will always be able to reproduce the input traces.   We evaluate the quality of the programs synthesized when considering various optimization metrics and the synthesizer's efficiency on real-world benchmarks. The results show that our approach can generate useful real-world programs.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {213},
numpages = {24},
keywords = {Logic and verification, Theory of computation → Automated reasoning}
}

@inproceedings{10.1145/3711896.3737403,
author = {Qin, Chuan and Chen, Xin and Wang, Chengrui and Wu, Pengmin and Chen, Xi and Cheng, Yihang and Zhao, Jingyi and Xiao, Meng and Dong, Xiangchao and Long, Qingqing and Pan, Boya and Wu, Han and Li, Chengzan and Zhou, Yuanchun and Xiong, Hui and Zhu, Hengshu},
title = {SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737403},
doi = {10.1145/3711896.3737403},
abstract = {In recent years, the rapid advancement of Artificial Intelligence (AI) technologies, particularly Large Language Models (LLMs), has revolutionized the paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a dynamic and evolving field. However, there is still a lack of an effective framework for the overall assessment of AI4Science, particularly from a holistic perspective on data quality and model capability. Therefore, in this study, we propose SciHorizon, a comprehensive assessment framework designed to benchmark the readiness of AI4Science from both scientific data and LLM perspectives. First, we introduce a generalizable framework for assessing AI-ready scientific data, encompassing four key dimensions-Quality, FAIRness, Explainability, and Compliance-which are subdivided into 15 sub-dimensions. Drawing on data resource papers published between 2018 and 2023 in peer-reviewed journals, we present recommendation lists of AI-ready datasets for Earth, Life, and Materials Sciences, making a novel and original contribution to the field. Concurrently, to assess the capabilities of LLMs across multiple scientific disciplines, we establish 16 assessment dimensions based on five core indicators-Knowledge, Understanding, Reasoning, Multimodality, and Values-spanning Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space Sciences. Using the developed benchmark datasets, we have conducted a comprehensive evaluation of over 50 representative open-source and closed-source LLMs. All the results are publicly available and can be accessed online at www.scihorizon.cn/en.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {5754–5765},
numpages = {12},
keywords = {AI-for-science, AI-ready, benchmarking, large language models, scientific data},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inbook{10.1145/3676641.3716022,
author = {Yang, Chenyuan and Zhao, Zijie and Zhang, Lingming},
title = {KernelGPT: Enhanced Kernel Fuzzing via Large Language Models},
year = {2025},
isbn = {9798400710797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676641.3716022},
abstract = {Bugs in operating system kernels can affect billions of devices and users all over the world. As a result, a large body of research has been focused on kernel fuzzing, i.e., automatically generating syscall (system call) sequences to detect potential kernel bugs or vulnerabilities. Kernel fuzzing aims to generate valid syscall sequences guided by syscall specifications that define both the syntax and semantics of syscalls. While there has been existing work trying to automate syscall specification generation, this remains largely manual work, and a large number of important syscalls are still uncovered.In this paper, we propose KernelGPT, the first approach to automatically synthesizing syscall specifications via Large Language Models (LLMs) for enhanced kernel fuzzing. Our key insight is that LLMs have seen massive kernel code, documentation, and use cases during pre-training, and thus can automatically distill the necessary information for making valid syscalls. More specifically, KernelGPT leverages an iterative approach to automatically infer the specifications, and further debug and repair them based on the validation feedback. Our results demonstrate that KernelGPT can generate more new and valid specifications and achieve higher coverage than state-of-the-art techniques. So far, by using newly generated specifications, KernelGPT has already detected 24 new unique bugs in Linux kernel, with 12 fixed and 11 assigned with CVE numbers. Moreover, a number of specifications generated by KernelGPT have already been merged into the kernel fuzzer Syzkaller, following the request from its development team.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {560–573},
numpages = {14}
}

@inproceedings{10.1145/3698205.3729558,
author = {Malik, Rizwaan and Hao, Rebecca Li and Kacholia, Ritika and Demszky, Dorottya},
title = {MathemaTikZ: A Dataset and Benchmark for Mathematical Diagram Generation},
year = {2025},
isbn = {9798400712913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698205.3729558},
doi = {10.1145/3698205.3729558},
abstract = {Diagrams play a fundamental role in mathematics education, serving both as essential components of mathematical problems and as powerful scaffolding tools to support student comprehension. While AI tools have shown promise in supporting teachers with lesson preparation, especially with text-based mathematical content, they still struggle with reliably generating visual diagrams. Our work makes two main contributions: (1) We introduce MathemaTikZ, a dataset derived from the Illustrative Mathematics curriculum, comprising 3,793 mathematical diagrams paired with their natural language descriptions, problem contexts, and TikZ implementations. These span the full range of diagrams utilized in the K12 math curriculum. (2) We conduct comprehensive baseline evaluations using state-of-the-art language models (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) to assess current capabilities in mathematical diagram generation. Our findings reveal that even the best-performing models achieve a 73.9\% success rate in accurately generating mathematical diagrams, with performance varying significantly across different types of visualizations. Through detailed error analysis, we identify four key challenge areas that future work should address: spatial reasoning and element placement, adherence to geometric constraints, pedagogical knowledge of mathematical diagrams, and preservation of mathematical relationships. Our results establish baselines for mathematical diagram generation and highlight critical areas for improvement in making AI tools more effective for mathematics education.},
booktitle = {Proceedings of the Twelfth ACM Conference on Learning @ Scale},
pages = {95–104},
numpages = {10},
keywords = {curriculum development, large language models, mathematics education, visual generation},
location = {Palermo, Italy},
series = {L@S '25}
}

@inproceedings{10.1145/3597638.3608423,
author = {Jiang, Chutian and Lei, Wentao and Kuang, Emily and Han, Teng and Fan, Mingming},
title = {Understanding Strategies and Challenges of Conducting Daily Data Analysis (DDA) Among Blind and Low-vision People},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608423},
doi = {10.1145/3597638.3608423},
abstract = {Being able to analyze and derive insights from data, which we call Daily Data Analysis (DDA), is an increasingly important skill in everyday life. While the accessibility community has explored ways to make data more accessible to blind and low-vision (BLV) people, little is known about how BLV people perform DDA. Knowing BLV people’s strategies and challenges in DDA would allow the community to make DDA more accessible to them. Toward this goal, we conducted a mixed-methods study of interviews and think-aloud sessions with BLV people (N=16). Our study revealed five key approaches for DDA (i.e., overview obtaining, column comparison, key statistics identification, note-taking, and data validation) and the associated challenges. We discussed the implications of our findings and highlighted potential directions to make DDA more accessible for BLV people.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {46},
numpages = {15},
keywords = {BLV, DDA, blind and low vision, daily data analysis, data accessibility, data exploration, interview, qualitative study, think-aloud},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3524610.3527894,
author = {Yang, Shouliang and Gu, Xiaodong and Shen, Beijun},
title = {Self-supervised learning of smart contract representations},
year = {2022},
isbn = {9781450392983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524610.3527894},
doi = {10.1145/3524610.3527894},
abstract = {Learning smart contract representations can greatly facilitate the development of smart contracts in many tasks such as bug detection and clone detection. Existing approaches for learning program representations are difficult to apply to smart contracts which have insufficient data and significant homogenization. To overcome these challenges, in this paper, we propose SRCL, a novel, self-supervised approach for learning smart contract representations. Unlike existing supervised methods, which are tied on task-specific data labels, SRCL leverages large-scale unlabeled data by self-supervised learning of both local and global information of smart contracts. It automatically extracts structural sequences from abstract syntax trees (ASTs). Then, two discriminators are designed to guide the Transformer encoder to learn local and global semantic features of smart contracts. We evaluate SRCL on a dataset of 75,006 smart contracts collected from Etherscan. Experimental results show that SRCL considerably outperforms the state-of-the-art code representation models on three downstream tasks.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension},
pages = {82–93},
numpages = {12},
keywords = {code representation learning, data augmentation, self-supervised learning, smart contract},
location = {Virtual Event},
series = {ICPC '22}
}

@article{10.1145/3735499,
author = {Ousmer, Mehdi and Vanderdonckt, Jean and Bilius, Laura-Bianca and Vatavu, Radu-Daniel and Terenti, Mihail},
title = {Paired Sketching of Distributed User Interfaces: Workflow, Protocol, Software Support, and Experiment},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3735499},
doi = {10.1145/3735499},
abstract = {The evolving landscape of distributed user interfaces requires the prototyping stage also be distributed between users, tasks, platforms, and environments. To create a cohesive distribution of the user interface elements in such ecosystems, paired sketching has emerged as a collaborative design method that leverages multiple stakeholders’ strengths, including designers, developers, and end users, working in pairs. In the context of developer experience applied to paired sketching for distributed user interfaces, we decomposed a workflow into four disciplines according to the Software and Systems Process Engineering Meta-Model (SPEM) notation. First, we defined a protocol to deploy paired sketching of distributed user interfaces, supported by UbiSketch, a collaborative software environment tailored featuring sketch recognition and whiteboarding. Second, to evaluate paired sketching for engineering interactive systems, we conducted an experiment involving five pairs of stakeholders who sketched a distributed user interface for inside-the-vehicule interaction distributed on four platforms: smartphone, tablet, pen display, and tabletop. Empirical results from questionnaires, reactivity, intention, perceived satisfaction, and free comments, suggest a preference order in which the tabletop is ranked first, followed by the tablet, smartphone, and pen display. Based on these results, we discuss the potential of paired sketching for distributed user interfaces.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {EICS018},
numpages = {31},
keywords = {Distributed user interfaces, Multi-platform user interfaces, Paired sketching}
}

@inproceedings{10.1145/3477495.3532077,
author = {Ren, Zhaochun and Tian, Zhi and Li, Dongdong and Ren, Pengjie and Yang, Liu and Xin, Xin and Liang, Huasheng and de Rijke, Maarten and Chen, Zhumin},
title = {Variational Reasoning about User Preferences for Conversational Recommendation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532077},
doi = {10.1145/3477495.3532077},
abstract = {Conversational recommender systems (CRSs) provide recommendations through interactive conversations. CRSs typically provide recommendations through relatively straightforward interactions, where the system continuously inquires about a user's explicit attribute-aware preferences and then decides which items to recommend. In addition, topic tracking is often used to provide naturally sounding responses. However, merely tracking topics is not enough to recognize a user's real preferences in a dialogue.In this paper, we address the problem of accurately recognizing and maintaining user preferences in CRSs. Three challenges come with this problem: (1) An ongoing dialogue only provides the user's short-term feedback; (2) Annotations of user preferences are not available; and (3) There may be complex semantic correlations among items that feature in a dialogue. We tackle these challenges by proposing an end-to-end variational reasoning approach to the task of conversational recommendation. We model both long-term preferences and short-term preferences as latent variables with topical priors for explicit long-term and short-term preference exploration, respectively. We use an efficient stochastic gradient variational Bayesian (SGVB) estimator for optimizing the derived evidence lower bound. A policy network is then used to predict topics for a clarification utterance or items for a recommendation response. The use of explicit sequences of preferences with multi-hop reasoning in a heterogeneous knowledge graph helps to provide more accurate conversational recommendation results.Extensive experiments conducted on two benchmark datasets show that our proposed method outperforms state-of-the-art baselines in terms of both objective and subjective evaluation metric},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {165–175},
numpages = {11},
keywords = {conversational recommendation, task-oriented dialogue systems, user preference tracking, variational inference},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3650212.3652119,
author = {Zhou, Mingyi and Gao, Xiang and Liu, Pei and Grundy, John and Chen, Chunyang and Chen, Xiao and Li, Li},
title = {Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652119},
doi = {10.1145/3650212.3652119},
abstract = {Recent studies show that on-device deployed deep learning (DL) models, such as those of Tensor Flow Lite (TFLite), can be easily extracted from real-world applications and devices by attackers to generate many kinds of adversarial and other attacks. Although securing deployed on-device DL models has gained increasing attention, no existing methods can fully prevent these attacks. Traditional software protection techniques have been widely explored. If on-device models can be implemented using pure code, such as C++, it will open the possibility of reusing existing robust software protection techniques. However, due to the complexity of DL models, there is no automatic method that can translate DL models to pure code. To fill this gap, we propose a novel method, CustomDLCoder, to automatically extract on-device DL model information and synthesize a customized executable program for a wide range of DL models. CustomDLCoder first parses the DL model, extracts its backend computing codes, configures the extracted codes, and then generates a customized program to implement and deploy the DL model without explicit model representation. The synthesized program hides model information for DL deployment environments since it does not need to retain explicit model representation, preventing many attacks on the DL model. In addition, it improves ML performance because the customized code removes model parsing and preprocessing steps and only retains the data computing process. Our experimental results show that CustomDLCoder improves model security by disabling on-device model sniffing. Compared with the original on-device platform (i.e., TFLite), our method can accelerate model inference by 21.0\% and 24.3\% on x86-64 and ARM64 platforms, respectively. Most importantly, it can significantly reduce memory consumption by 68.8\% and 36.0\% on x86-64 and ARM64 platforms, respectively.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {174–185},
numpages = {12},
keywords = {AI safety, SE for AI, software optimization for AI deployment},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3485447.3511941,
author = {Islam, Sk Mainul and Bhattacharya, Sourangshu},
title = {AR-BERT: Aspect-relation enhanced Aspect-level Sentiment Classification with Multi-modal Explanations},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511941},
doi = {10.1145/3485447.3511941},
abstract = {Aspect level sentiment classification (ALSC) is a difficult problem with state-of-the-art models showing less than 80\% macro-F1 score on benchmark datasets. Existing models do not incorporate information on aspect-aspect relations in knowledge graphs (KGs), e.g. DBpedia. Two main challenges stem from inaccurate disambiguation of aspects to KG entities, and the inability to learn aspect representations from the large KGs in joint training with ALSC models. We propose AR-BERT, a novel two-level global-local entity embedding scheme that allows efficient joint training of KG-based aspect embeddings and ALSC models. A novel incorrect disambiguation detection technique addresses the problem of inaccuracy in aspect disambiguation. We also introduce the problem of determining mode significance in multi-modal explanation generation, and propose a two step solution. The proposed methods show a consistent improvement of 2.5 − 4.1 percentage points, over the recent BERT-based baselines on benchmark datasets.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {987–998},
numpages = {12},
keywords = {Explainable Deep Learning, Knowledge Graph Embedding, Sentiment Analysis},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

