@article{10.1145/3712008,
author = {Burgue\~{n}o, Lola and Di Ruscio, Davide and Sahraoui, Houari and Wimmer, Manuel},
title = {Automation in Model-Driven Engineering: A Look Back, and Ahead},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712008},
doi = {10.1145/3712008},
abstract = {Model-Driven Engineering (MDE) provides a huge body of knowledge of automation for many different engineering tasks, especially those involving transitioning from design to implementation. With the huge progress made in AI, questions arise about the future of MDE, such as how existing MDE techniques and technologies can be improved or how other activities that currently lack dedicated support can also be automated. However, at the same time, it has to be revisited where and how models should be used to keep the engineers in the loop for creating, operating, and maintaining complex systems. To trigger dedicated research on these open points, we discuss the history of automation in MDE and present perspectives on how automation in MDE can be further improved and which obstacles have to be overcome in both the medium and long-term.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {122},
numpages = {25},
keywords = {Model-Driven Engineering (MDE), automation}
}

@inproceedings{10.1145/3652620.3687773,
author = {Bucchiarone, Antonio and Cicchetti, Antonio and V\'{a}zquez-Ingelmo, Andrea and Adami, Filippo and Schiavo, Gianluca and Garc\'{\i}a-Holgado, Alicia and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e}},
title = {Designing and Generating Lesson Plans combining Open Educational Content and Generative AI},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687773},
doi = {10.1145/3652620.3687773},
abstract = {In this paper, we propose an approach for assisting educators in deriving lesson plans for complex learning subjects like Model-Driven Engineering (MDE) from existing educational materials, leveraging generative AI techniques. Our method focuses on guiding teachers in defining learning objectives and suggesting concrete learning activities for students. Central to our approach is the development of a metamodel that characterizes the methodology and serves as the foundation for implementing supporting tools. By utilizing available Open Educational Resources (OERs) and incorporating them into specific learning activities, our method provides a general framework for supporting educators in designing lesson plans. We present the methodology to generate lesson plans, the metamodel conceptualizing plans ingredients, and demonstrate their application through supporting tools, illustrating the potential of our approach in facilitating the development of MDE teaching materials.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {78–86},
numpages = {9},
keywords = {open educational resources, OERs, model-driven engineering, MDE, generative AI, educational paradigms, tailored learning activities, customizable learning content},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3722113,
author = {Tang, Yu and Yin, Lujia and Li, Qiao and Zhu, Hongyu and Li, Hengjie and Zhang, Xingcheng and Qiao, Linbo and Li, Dongsheng and Li, Jiaxin},
title = {Koala: Efficient Pipeline Training through Automated Schedule Searching on Domain-Specific Language},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3722113},
doi = {10.1145/3722113},
abstract = {Pipeline parallelism is a crucial technique for large-scale model training, enabling parameter splitting and performance enhancement. However, creating effective pipeline schedules often requires significant manual effort and coding skills, leading to practical inconveniences and complex debugging. Major frameworks such as DeepSpeed and ColossalAI simplify the process by adopting predefined pipeline schedule strategies, such as GPipe and 1F1B. The use of predefined schedules offers limited flexibility and suboptimal training efficiency, as the limited number of manually set candidates cannot provide the optimal strategy for arbitrary model training. To deal with the issue, this article aims to automatically search for the optimal strategy with high efficiency. Since current frameworks only support a limited set of fixed strategies, lacking the technical capability to create a comprehensive strategy search space, we first design a novel domain-specific language (DSL) for pipeline schedule development. The DSL exhibits great understandability, agility, and reusability, supporting the development of all known pipeline schedule strategies and their variants. Second, we are the first to model the complete pipeline schedule strategy space via the DSL, enabling an automated end-to-end globally optimal pipeline schedule searching, while past work may get stuck in a local optimum. Finally, we propose to optimize pipeline performance by modeling and solving the pipeline schedule as a Binary-Tree-Traversing (BTT) optimization problem. Based on the formalization, we further adopt a Dynamic Try-Test Genetic Algorithm&nbsp;to search for the best pipeline schedule strategy, which overwhelms a variety of pre-defined ones. Experimental results show that Koala achieves an enhanced performance by up to  (1.53times)  over state-of-the-art approaches. Besides, the pipeline schedule strategy searched by Koala outperforms pre-defined pipeline schedule strategies by (1.10times sim 1.55times) . Moreover, Koala has superior scalability and effectiveness in combining with data parallelism and tensor parallelism.},
journal = {ACM Trans. Archit. Code Optim.},
month = jul,
articleno = {63},
numpages = {25},
keywords = {Pipeline parallelism, domain-specific language, automated large-scale model training}
}

@inproceedings{10.1145/3696630.3728550,
author = {Wang, Yong and Ge, Ning and Li, Jingyao and Wang, Loulin and Zhou, Guangyu and Deng, Chengrui and Zhang, Li and Hu, Chunming},
title = {SemServGen: Advancing Industrial Domain-Specific Language Engineering through Semantic Service Generation},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728550},
doi = {10.1145/3696630.3728550},
abstract = {Domain-Specific Languages (DSLs) are widely used across industries for addressing challenges within specific domains. However, designing a DSL is just the first step in DSL engineering. To truly enhance development efficiency and system quality, comprehensive syntax and semantic services are essential. Currently, semantic services are primarily implemented manually, leading to high development costs. Automating their generation can significantly improve efficiency and reduce costs in DSL projects. This paper presents SemServGen, a framework that defines a structured approach to DSL semantic service development. SemServGen comprises SemaDSL, a unified semantic service expression language; SemServ-editor, which provides editing, syntax, semantic, template, and DSL-binding services; and SemServ-gen, an automated semantic service generator. SemaDSL enables the specification of both generic and domain-specific semantic services, while SemServ-gen translates these formal specifications into executable semantic services. We evaluated SemServGen with industry partners using CRL and SemaDSL as target DSLs. The results demonstrate that SemaDSL effectively models semantic service domain concepts with high usability, while SemServGen increases development efficiency by 2.5 times for CRL and 1.9 times for SemaDSL compared to manual implementation. Additionally, the generated semantic services meet industry-standard performance benchmarks for service analysis time.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {250–261},
numpages = {12},
keywords = {domain-specific language engineering, semantic service, semantic specification language, code generation},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3696630.3728568,
author = {Wynn-Williams, Stephen and Tyrrell, Ryan and Pantelic, Vera and Lawford, Mark and Menghi, Claudio and Nalla, Phaneendra and Artail, Hassan},
title = {Can Generative AI Produce Test Cases? An Experience from the Automotive Domain},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728568},
doi = {10.1145/3696630.3728568},
abstract = {Engineers need automated support for software testing. Generative AI is a novel technology for generating new content; however, its applicability for test case generation is still unclear. This work considers the following question: Can generative AI produce test cases in industrial software applications? We framed our question in the automotive domain. We performed our evaluation in collaboration with a large automotive manufacturer to assess to what extent generative AI can produce test cases (a.k.a. test scripts) from informal test case specifications. We considered 1) informal test case specifications defined in Rational Quality Manager, an industrial test management tool from IBM, and 2) executable test scripts specified as ecu.test packages supported by the ecu.test tool from Tracetronic. We used generative AI to produce the test scripts from the informal test case descriptions. Our results show that generative AI can produce correct or near-correct test scripts in a reasonable number of cases. We also analyzed the effects of prompt design, choice of generative AI model, and context accuracy on the effectiveness of our solution and reflected on our results.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {456–467},
numpages = {12},
keywords = {software testing, LLM, generative AI, automotive software},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@article{10.1145/3729356,
author = {Li, Yue and Liu, Bohan and Zhang, Ting and Wang, Zhiqi and Lo, David and Yang, Lanxin and Lyu, Jun and Zhang, He},
title = {A Knowledge Enhanced Large Language Model for Bug Localization},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729356},
doi = {10.1145/3729356},
abstract = {A significant number of bug reports are generated every day as software systems continue to develop. Large Language Models (LLMs) have been used to correlate bug reports with source code to locate bugs automatically. The existing research has shown that LLMs are effective for bug localization and can increase software development efficiency. However, these studies still have two limitations. First, these models fail to capture context information about bug reports and source code. Second, these models are unable to understand the domain-specific expertise inherent to particular projects, such as version information in projects that are composed of alphanumeric characters without any semantic meaning. To address these challenges, we propose a Knowledge Enhanced Pre-Trained model using project documents and historical code, called KEPT, for bug localization. Project documents record, revise, and restate project information that provides rich semantic information about those projects. Historical code contains rich code semantic information that can enhance the reasoning ability of LLMs. Specifically, we construct knowledge graphs from project documents and source code. Then, we introduce knowledge graphs to the LLM through soft-position embedding and visible matrices, enhancing its contextual and professional reasoning ability. To validate our model, we conducted a series of experiments on seven open-source software projects with over 6,000 bug reports. Compared with the traditional model (Locus), KEPT performs better by 33.2\% to 59.5\% in terms of mean reciprocal rank, mean average precision, and Top@N. Compared with the best-performing non-commercial LLM (CodeT5), KEPT achieves an improvement of 36.6\% to 63.7\%. Compared to the state-of-the-art commercial LLM developed by OpenAI, called text-embedding-ada-002, KEPT achieves an average improvement of 7.8\% to 17.4\%. The results indicate that introducing knowledge graphs contributes to enhance the effectiveness of the LLM in bug localization.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE086},
numpages = {23},
keywords = {bug localization, information retrieval, knowledge enhancement, large language model}
}

@inproceedings{10.1145/3652620.3687804,
author = {Birchler De Allende, Alan and Sultan, Bastien and Apvrille, Ludovic},
title = {From Attack Trees to Attack-Defense Trees with Generative AI \&amp; Natural Language Processing},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687804},
doi = {10.1145/3652620.3687804},
abstract = {Attack-defense trees, an extension of attack trees, are extensively used by security engineers to document potential countermeasures for security threats present in a system's design. These trees help integrate initial system models with countermeasures, allowing for early testing of their efficiency and impact in the design cycle. Despite advancements in automating attack tree construction, selecting the initial set of countermeasures for conversion into an attack-defense tree remains largely manual. This paper proposes an approach and a tool that extends the TTool-AI attack tree generation feature by leveraging large language models and natural language processing to create a set of countermeasures and generate attack-defense trees based on an input attack tree. To evaluate our contribution, our approach is tested using attack-defense trees generated from attack trees, each representing possible threats to an associated system specification. In addition, we introduce metrics to assess the semantic correctness and completeness of the generated attack-defense trees. We compared, using our metrics, the attack-defense trees created from our methodology to those created by an engineer and found that attack-defense trees created using AI and secondary mitigation data provided better trees than solely using AI. We also discovered that this approach generated trees that were comparable to the quality of attack-defense trees generated from a security engineer at the associate level. From these results, we believe that our contribution could aid engineers in identifying not only appropriate countermeasures for attack trees but also the optimal number of countermeasures, avoiding the complexity of redundant mitigations. Furthermore, our approach complements standard modeling practices, particularly during the initial design phase, reducing the need for time-consuming re-engineering throughout the system's lifecycle.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {561–569},
numpages = {9},
keywords = {artificial intelligence, large-language models, attack-defense trees, model-driven engineering},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3641399.3641403,
author = {Rajbhoj, Asha and Somase, Akanksha and Kulkarni, Piyush and Kulkarni, Vinay},
title = {Accelerating Software Development Using Generative AI: ChatGPT Case Study},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641399.3641403},
doi = {10.1145/3641399.3641403},
abstract = {The Software Development Life Cycle (SDLC) comprises multiple phases, each requiring Subject Matter Experts (SMEs) with phase-specific skills. The efficacy and quality of deliverables of each phase are skill dependent. In recent times, Generative AI techniques, including Large-scale Language Models (LLMs) like GPT, have become significant players in software engineering. These models, trained on extensive text data, can offer valuable contributions to software development. Interacting with LLMs involves feeding prompts with the context information and guiding the generation of textual responses. The quality of the response is dependent on the quality of the prompt given. This paper proposes a systematic prompting approach based on meta-model concepts for SDLC phases. The approach is validated using ChatGPT for small but complex business application development. We share the approach and our experience, learnings, benefits obtained, and the challenges encountered while applying the approach using ChatGPT. Our experience indicates that Generative AI techniques, such as ChatGPT, have the potential to reduce the skills barrier and accelerate software development substantially.},
booktitle = {Proceedings of the 17th Innovations in Software Engineering Conference},
articleno = {5},
numpages = {11},
keywords = {AI in SDLC, Automated Software Development, ChatGPT, Generative AI, Large Language Models, SDLC automation},
location = {Bangalore, India},
series = {ISEC '24}
}

@inproceedings{10.1145/3660605.3660944,
author = {Schneider, Nadav and Hasabnis, Niranjan and Vo, Vy A. and Kadosh, Tal and Krien, Neva and Capota, Mihai and Tamir, Guy and Willke, Theodore L. and Ahmed, Nesreen and Pinter, Yuval and Mattson, Timothy and Oren, Gal},
title = {MPIrigen: MPI Code Generation through Domain-Specific Language Models},
year = {2024},
isbn = {9798400706523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660605.3660944},
doi = {10.1145/3660605.3660944},
abstract = {The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. While MPI serves as a cornerstone for large-scale parallelism, its seamless integration into codebases, especially concerning domain decomposition, has proven challenging. Static tools aimed at addressing this challenge have exhibited limited effectiveness and scalability. On the other hand, contemporary language models designed for programming problems have demonstrated utility in parallel programming tasks such as OpenMP pragma generation. However, the challenging parallel programming task of generating MPI-based parallel programs has remained unexplored.This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pre-trained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose an innovative preprocessing for completion only after observing the whole code, thus enabling better completion with a wider context. Comparative analysis against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation method, demonstrates that MPIrigen excels in generating accurate MPI functions calls. The success of this tailored solution underscores the importance of domain-specific fine-tuning in optimizing language models for parallel computing code generation, paving the way for a new generation of automatic parallelization tools.The sources of this work are available at our GitHub MPIrigen repository.},
booktitle = {Proceedings of the 2024 Workshop on AI For Systems},
pages = {1–6},
numpages = {6},
keywords = {MPI, domain decomposition, transformer, LLM, AI, code generation},
location = {Pisa, Italy},
series = {AI4Sys '24}
}

@inproceedings{10.1145/3706598.3714135,
author = {Chen, Wei-Hao and Tong, Weixi and Case, Amanda, Ph.D. and Zhang, Tianyi},
title = {Dango: A Mixed-Initiative Data Wrangling System using Large Language Model},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714135},
doi = {10.1145/3706598.3714135},
abstract = {Data wrangling is a time-consuming and challenging task in a data science pipeline. While many tools have been proposed to automate or facilitate data wrangling, they often misinterpret user intent, especially in complex tasks. We propose Dango, a mixed-initiative multi-agent system for data wrangling. Compared to existing tools, Dango enhances user communication of intent by: (1) allowing users to demonstrate on multiple tables and use natural language prompts in a conversation interface, (2) enabling users to clarify their intent by answering LLM-posed multiple-choice clarification questions, and (3) providing multiple forms of feedback such as step-by-step NL explanations and data provenance to help users evaluate the data wrangling scripts. We conducted a within-subjects user study (n=38) and demonstrated that Dango’s features can significantly improve intent clarification, accuracy, and efficiency in data wrangling. Furthermore, we demonstrated the generalizability of Dango by applying it to a broader set of data wrangling tasks.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {389},
numpages = {28},
keywords = {Data Wrangling, Data Science, Large Language Model},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3560071.3560084,
author = {Cao, Yiling and Fang, Lu and Zheng, Zhongguang},
title = {Enriching Pre-Trained Language Model with Multi-Task Learning and Context for Medical Concept Normalization},
year = {2022},
isbn = {9781450397087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560071.3560084},
doi = {10.1145/3560071.3560084},
abstract = {Herein, we focus on the problem of automatically medical concept normalization in social media posts. Specifically, the task is to map medical mentions within social media texts to the suitable concepts in a reference knowledge base. We propose a new medical concept normalization model using multi-task learning. The model uses BioBERT to encode mentions and their contexts, and classifies their concept IDs and types of mention. We evaluate our approach on two datasets and achieve new state-of-the-art performance.},
booktitle = {Proceedings of the 2022 International Conference on Intelligent Medicine and Health},
pages = {79–83},
numpages = {5},
keywords = {BioBERT, Context information, Medical concept normalization, Multi-task learning},
location = {Xiamen, China},
series = {ICIMH '22}
}

@article{10.1145/3704905,
author = {Cai, Yufan and Hou, Zhe and Sanan, David and Luan, Xiaokun and Lin, Yun and Sun, Jun and Dong, Jin Song},
title = {Automated Program Refinement: Guide and Verify Code Large Language Model with Refinement Calculus},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {POPL},
url = {https://doi.org/10.1145/3704905},
doi = {10.1145/3704905},
abstract = {Recently, the rise of code-centric Large Language Models (LLMs) has reshaped the software engineering world with low-barrier tools like Copilot that can easily generate code. However, there is no correctness guarantee for the code generated by LLMs, which suffer from the hallucination problem, and their output is fraught with risks. Besides, the end-to-end process from specification to code through LLMs is a non-transparent and uncontrolled black box. This opacity makes it difficult for users to understand and trust the generated code. Addressing these challenges is both necessary and critical. In contrast, program refinement transforms high-level specification statements into executable code while preserving correctness. Traditional tools for program refinement are primarily designed for formal methods experts and lack automation and extensibility. We apply program refinement to guide LLM and validate the LLM-generated code while transforming refinement into a more accessible and flexible framework.                To initiate this vision, we propose Refine4LLM, an approach that aims to:                (1) Formally refine the specifications,                (2) Automatically prompt and guide the LLM using refinement calculus,                (3) Interact with the LLM to generate the code,                (4) Verify that the generated code satisfies the constraints, thus guaranteeing its correctness,                (5) Learn and build more advanced refinement laws to extend the refinement calculus.                We evaluated Refine4LLM against the state-of-the-art baselines on program refinement and LLMs benchmarks.The experiment results show that Refine4LLM can efficiently generate more robust code and reduce the time for refinement and verification.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {69},
numpages = {33},
keywords = {Large Language Model, Program Refinement, Program Synthesis}
}

@inproceedings{10.1145/3640310.3674081,
author = {Jahan, Munima and Hassan, Mohammad Mahdi and Golpayegani, Reza and Ranjbaran, Golshid and Roy, Chanchal and Roy, Banani and Schneider, Kevin},
title = {Automated Derivation of UML Sequence Diagrams from User Stories: Unleashing the Power of Generative AI vs. a Rule-Based Approach},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674081},
doi = {10.1145/3640310.3674081},
abstract = {User stories are informal, non-technical descriptions of features from a user's perspective that guide collaboration and iterative development in Agile projects. However, ambiguities in user stories can lead to miscommunication among stakeholders. Design models, such as UML sequence diagrams, are essential for enhancing communication, clarifying system behavior, and improving the development process. This paper presents an automated approach for generating behavioral models specifically sequence diagrams from natural language requirements expressed as user stories. We also investigate the effectiveness of a Large Language Model (LLM) in using generative AI for this task. By applying our approach and ChatGPT to two benchmark datasets with the same set of user stories, we generated corresponding sequence diagrams for comparison. Expert evaluations in Software Engineering reveal that our approach effectively produces relevant, simplified diagrams for straightforward user stories, whereas the LLM tends to create more complex diagrams that sometimes go beyond the simplicity of the original user stories.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {138–148},
numpages = {11},
keywords = {Generative Model, Large Language Model, Model Generation, Natural Language Processing, Rule-based approach, Sequence Diagram, User Story},
location = {Linz, Austria},
series = {MODELS '24}
}

@article{10.1145/3728878,
author = {Yu, Lei and Huang, Zhirong and Yuan, Hang and Cheng, Shiqi and Yang, Li and Zhang, Fengjun and Shen, Chenjie and Ma, Jiajia and Zhang, Jingyuan and Lu, Junyi and Zuo, Chun},
title = {Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728878},
doi = {10.1145/3728878},
abstract = {Smart contract vulnerability detection is a critical challenge in the rapidly evolving blockchain landscape. Existing vulnerability detection methods face two main issues: (1) Existing datasets lack comprehensiveness and sufficient quality, with limited vulnerability type coverage and insufficient distinction between high-quality and low-quality explanations for preference learning. (2) Large language models (LLMs) often struggle with accurately interpreting specific concepts in smart contract security. Through our empirical analysis, we found that even after continual pre-training and supervised fine-tuning, LLMs still exhibit limitations in precisely understanding the execution order of state changes in smart contracts, which can lead to incorrect vulnerability explanations despite making correct detection decisions. These limitations result in poor detection performance, leading to potentially severe financial losses. To address these challenges, we propose Smart-LLaMA-DPO, an advanced detection method based on the LLaMA-3.1-8B. First, we construct a comprehensive dataset covering four vulnerability types and machine-unauditable vulnerabilities, containing labels, detailed explanations, and precise vulnerability locations for Supervised Fine-Tuning (SFT), as well as paired high-quality and low-quality outputs for Direct Preference Optimization (DPO). Second, we perform continual pre-training using large-scale smart contract code to enhance the LLM's understanding of specific security practices in smart contracts. Futhermore, we conduct supervised fine-tuning with our comprehensive dataset. Finally, we apply DPO, which leverages human feedback to improve the quality of generated explanations. Smart-LLaMA-DPO utilizes a specially designed loss function that encourages the LLM to increase the probability of preferred outputs while decreasing the probability of non-preferred outputs, thereby enhancing the LLM's ability to generate high-quality explanations. We evaluate Smart-LLaMA-DPO on four major vulnerability types: reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall, as well as machine-unauditable vulnerabilities. Our method significantly outperforms state-of-the-art baselines, with average improvements of 10.43\% in F1 score and 7.87\% in accuracy. Moreover, both LLM evaluation and human evaluation demonstrate the superior quality of explanations generated by Smart-LLaMA-DPO in terms of correctness, thoroughness, and clarity.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA009},
numpages = {24},
keywords = {Direct Preference Optimization, Large Language Models, Smart Contract}
}

@inproceedings{10.5555/3712729.3712990,
author = {Leathrum, James F. and Shen, Yuzhong and Sosonkina, Masha},
title = {Investigating the Use of Generative AI in M&amp;S Education},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {Large Language Models (LLMs) are rapidly creating a place for themselves in society. There are numerous reports, both good and bad, of their use in business, academia, government and society. While some organizations are trying to limit, or eliminate, their use, it appears that it is inevitable they will become a common "tool". In education, there is a fear that students will not acquire critical thinking in the future, but we argue that LLMs will become a tool to assist students with critical thinking, giving guidance, feedback, and assessment. This paper investigates how the current state of LLMs can be integrated into modeling and simulation (M&amp;S) education. Example cases for modeling and simulation development are presented showing how an LLM can assist M&amp;S design and education in anticipation of LLMs becoming a common tool for M&amp;S practitioners. Current limitations are also highlighted, and where possible, short-term solutions are proposed.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3142–3153},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3659677.3659736,
author = {Amri, Samir and Bani, Rkia and Bani, Saida},
title = {An Approach to the Analysis of Financial Documents Using Generative AI},
year = {2024},
isbn = {9798400709296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659677.3659736},
doi = {10.1145/3659677.3659736},
abstract = {This project tackles the challenge of advancing document analysis with generative AI techniques. It explores two main approaches:• Fine-Tuning and Retrieval Augmented Generation (RAG).• Fine-Tuning: This approach utilizes the BART model in conjunction with a specialized vector database. The Fine-Tuning phase involves a comprehensive process of data acquisition, cleaning, and processing. This phase provides valuable insights into the challenges and considerations involved in document analysis using Fine-Tuning.Retrieval Augmented Generation (RAG): This novel method leverages generative AI for contextual understanding and response generation. The RAG section delves into the objectives, methodology, and results achieved with this cutting-edge approach.A comparative analysis is then conducted to shed light on the distinct contributions of Fine-Tuning and RAG to document analysis. Furthermore, the project extends beyond AI models by developing an interactive User Interface (UI). This UI utilizes various technologies to ensure a seamless user experience. Key features include functionalities for file upload, error handling, responsive design, and smooth integration with the backend system.},
booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
articleno = {31},
numpages = {5},
location = {Meknes, AA, Morocco},
series = {NISS '24}
}

@inbook{10.5555/3712729.3712960,
author = {Carreira-Munich, Tobias and Paz-Marcolla, Valent\'{\i}n and Castro, Rodrigo},
title = {DEVS Copilot: Towards Generative AI-Assisted Formal Simulation Modelling Based on Large Language Models},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {In this paper we explore to which extent generative AI, in the form of Large Language Models such as GPT-4, can assist in obtaining a correct executable simulation model. The starting point is a high-level description of a system, expressed in natural language, which evolves through a conversational process based on user input, including suggestions for corrections. We introduce a methodology and a tool inspired by the metaphor of a copilot, a form of human-AI teaming strategy well known for its success in programming tasks. We adopt the Discrete Event System Specification (DEVS), a suitable candidate formalism that allows general-purpose simulation models to be specified in a simple yet rigorous modular and hierarchical way. The result is DEVS Copilot, an AI-based prototype that we systematically test in a case study that builds several lighting control systems of increasing complexity. In all cases, DEVS Copilot succeeds at producing correct DEVS simulations.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2785–2796},
numpages = {12}
}

@inproceedings{10.1145/3675249.3675253,
author = {Li, Peiyan and Liu, Xiaomeng and Wang, Yongxing},
title = {A Novel Method based on Large Language Model for MBTI Classification: A Novel MBTI Classification Method},
year = {2024},
isbn = {9798400718267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675249.3675253},
doi = {10.1145/3675249.3675253},
abstract = {Personality analysis has a wide and important application in psychology, helping us explain various psychological phenomena and the developmental laws of personality. Accurately identifying personality will contribute to research in fields such as cognitive science, public opinion analysis, and cybersecurity. The most widely used models in the field of personality recognition are the Big Five personality model and MBTI model in trait genre. With the rise of social media and Large Language Model(LLM), massive corpora and deep learning models have been used for personality analysis and have achieved good results. In view of this, this article introduces a novel method based on transformer-based pre-trained language model named mDeBERTa and uses the MBTI-500 and MBTI-1 datasets as examples to demonstrate the advantages of this model over traditional SVM models and BERT models. At the same time, we explore the possibility of using large models and multimodal data for more accurate personality analysis.},
booktitle = {Proceedings of the 2024 International Conference on Computer and Multimedia Technology},
pages = {11–16},
numpages = {6},
location = {Sanming, China},
series = {ICCMT '24}
}

@inproceedings{10.1145/3613904.3642937,
author = {Kim, Taewan and Bae, Seolyeong and Kim, Hyun Ah and Lee, Su-Woo and Hong, Hwajung and Yang, Chanmo and Kim, Young-Ho},
title = {MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients' Journaling},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642937},
doi = {10.1145/3613904.3642937},
abstract = {Large Language Models (LLMs) offer promising opportunities in mental health domains, although their inherent complexity and low controllability elicit concern regarding their applicability in clinical settings. We present MindfulDiary, an LLM-driven journaling app that helps psychiatric patients document daily experiences through conversation. Designed in collaboration with mental health professionals, MindfulDiary takes a state-based approach to safely comply with the experts’ guidelines while carrying on free-form conversations. Through a four-week field study involving 28 patients with major depressive disorder and five psychiatrists, we examined how MindfulDiary facilitates patients’ journaling practice and clinical care. The study revealed that MindfulDiary supported patients in consistently enriching their daily records and helped clinicians better empathize with their patients through an understanding of their thoughts and daily contexts. Drawing on these findings, we discuss the implications of leveraging LLMs in the mental health domain, bridging the technical feasibility and their integration into clinical settings.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {701},
numpages = {20},
keywords = {chatbot, clinical setting, journaling, large language models, mental health, psychiatric patient},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3717383.3717391,
author = {Barat, Souvik and Mulpuru, Dushyanthi and Yadav, Abhishek and Korabu, Reshma and Thogaru, Himabindu and Kulkarni, Vinay},
title = {Constructing Enterprise Digital Twins by Augmenting LLMs with MDE},
year = {2025},
isbn = {9798400714245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3717383.3717391},
doi = {10.1145/3717383.3717391},
abstract = {Over the past year, Large Language Models (LLMs) have proven their value across a diverse range of industrial applications starting from supporting software development to automating customer interactions and enhancing process automation. We harness their potential for constructing Enterprise Digital Twins (EDTs), an emerging decision-making aid for a wide range of business sectors. EDT offers an effective "in silico" business experimentation leading to evidence-based informed decision-making, but its construction requires deep domain expertise spanning multiple aspects of enterprises across multiple stakeholders. Moreover, constructing an effective EDT demands seamless coordination between domain experts and expert modelers. These critical dependencies make the EDT construction challenging. This paper investigates the role of LLMs as domain experts and expert modelers to reduce excessive dependencies on both specializations and their coordination to an extent. Our approach integrates meta-modelling and Model Driven Engineering (MDE) techniques to effectively utilize LLMs with increased precision to alleviate the cognitive burden on domain experts and provide a systematic metamodel guided method for constructing purposive digital twins. We illustrate the approach and demonstrate its efficacy using a real-life EDT use case.},
booktitle = {Proceedings of the 18th Innovations in Software Engineering Conference},
articleno = {11},
numpages = {11},
keywords = {Large Language Model, LLM, ChatGPT, Model Driven Engineering, Digital Twin},
location = {
},
series = {ISEC '25}
}

@article{10.1145/3458754,
author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3458754},
doi = {10.1145/3458754},
abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \&amp; Reasoning Benchmark) at .},
journal = {ACM Trans. Comput. Healthcare},
month = oct,
articleno = {2},
numpages = {23},
keywords = {Biomedical, NLP, domain-specific pretraining}
}

@inproceedings{10.1145/3632410.3632465,
author = {Jain, Ayush and Padmanaban, Manikandan and Hazra, Jagabondhu and Godbole, Shantanu and Weldemariam, Komminist},
title = {A Framework for Emission Reduction in Scope 3 Climate Actions using Domain-adapted Foundation Model},
year = {2024},
isbn = {9798400716348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632410.3632465},
doi = {10.1145/3632410.3632465},
abstract = {Large enterprises face a crucial imperative to achieve the Sustainable Development Goals (SDGs), especially goal 13, which focuses on combating climate change and its impacts. To mitigate the effects of climate change, reducing enterprise Scope 3 (supply chain emissions) is vital, as it accounts for more than 90\% of total emission inventories. However, tracking Scope 3 emissions proves challenging, as data must be collected from thousands of upstream and downstream suppliers. We propose a novel framework that uses domain-adapted NLP foundation model to estimate Scope 3 emissions by leveraging financial transactions as a proxy of embodied emission of purchased goods and services and recommends appropriate climate actions to reduce scope3 emission through counterfactual queries. Our results show that the domain-adapted foundation model outperforms state-of-the-art text mining techniques and performs as well as a subject matter expert (SME). We also show how the proposed framework can identify Scope 3 hotspots and explain the factors that create them. Finally, we carry out what-if analysis to take climate actions that help achieve SDG 13. We present a case study demonstrating how this framework can be used by a real estate enterprise to take Scope 3 climate actions.},
booktitle = {Proceedings of the 7th Joint International Conference on Data Science \&amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)},
pages = {316–324},
numpages = {9},
keywords = {EEIO, Foundation model, Recommendation, Scope 3 emissions},
location = {Bangalore, India},
series = {CODS-COMAD '24}
}

@inproceedings{10.1145/3640310.3674093,
author = {Morales, Sergio and Claris\'{o}, Robert and Cabot, Jordi},
title = {A DSL for Testing LLMs for Fairness and Bias},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674093},
doi = {10.1145/3640310.3674093},
abstract = {Large language models (LLMs) are increasingly integrated into software systems to enhance them with generative AI capabilities. But LLMs may reflect a biased behavior, resulting in systems that could discriminate against gender, age or ethnicity, among other ethical concerns. Society and upcoming regulations will force companies and development teams to ensure their AI-enhanced software is ethically fair. To facilitate such ethical assessment, we propose LangBiTe, a model-driven solution to specify ethical requirements, and customize and automate the testing of ethical biases in LLMs. The evaluation can raise awareness on the biases of the LLM-based components of the system and/or trigger a change in the LLM of choice based on the requirements of that particular application. The model-driven approach makes both the requirements specification and the test generation platform-independent, and provides end-to-end traceability between the requirements and their assessment. We have implemented an open-source tool set, available on GitHub, to support the application of our approach.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {203–213},
numpages = {11},
keywords = {Bias, Domain-Specific Language, Ethics, Large Language Models, Model-Driven Engineering, Red Teaming, Testing},
location = {Linz, Austria},
series = {MODELS '24}
}

@article{10.14778/3611540.3611624,
author = {Chen, Zui and Cao, Lei and Madden, Sam},
title = {Lingua Manga : A Generic Large Language Model Centric System for Data Curation},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611624},
doi = {10.14778/3611540.3611624},
abstract = {Data curation is a wide-ranging area which contains many critical but time-consuming data processing tasks. However, the diversity of such tasks makes it challenging to develop a general-purpose data curation system. To address this issue, we present Lingua Manga, a user-friendly and versatile system that utilizes pre-trained large language models. Lingua Manga offers automatic optimization for achieving high performance and label efficiency while facilitating flexible and rapid development. Through three example applications with distinct objectives and users of varying levels of technical proficiency, we demonstrate that Lingua Manga can effectively assist both skilled programmers and low-code or even no-code users in addressing data curation challenges.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4074–4077},
numpages = {4}
}

@article{10.1145/3744920,
author = {Ben Chaaben, Meriem and Burgue\~{n}o, Lola and David, Istvan and Sahraoui, Houari},
title = {On the Utility of Domain Modeling Assistance with Large Language Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3744920},
doi = {10.1145/3744920},
abstract = {Model-driven engineering (MDE) simplifies software development through abstraction, yet challenges such as time constraints, incomplete domain understanding, and adherence to syntactic constraints hinder the design process. This paper presents a study to evaluate the usefulness of a novel approach utilizing large language models (LLMs) and few-shot prompt learning to assist in domain modeling. The aim of this approach is to overcome the need for extensive training of traditional AI-based completion algorithms on domain-specific datasets and to offer versatile support for various modeling activities, providing valuable recommendations to software modelers. To support this approach, we developed MAGDA, a user-friendly tool, through which we conduct a user study and assess the real-world applicability of our approach in the context of domain modeling, offering valuable insights into its usability and effectiveness.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
keywords = {domain modeling, generative AI, language models, model-driven engineering, prompt learning, user study}
}

@inproceedings{10.1145/3613904.3642492,
author = {Newman, Michele and Sun, Kaiwen and Dalla Gasperina, Ilena B and Shin, Grace Y. and Pedraja, Matthew Kyle and Kanchi, Ritesh and Song, Maia B. and Li, Rannie and Lee, Jin Ha and Yip, Jason},
title = {"I want it to talk like Darth Vader": Helping Children Construct Creative Self-Efficacy with Generative AI},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642492},
doi = {10.1145/3613904.3642492},
abstract = {The emergence of generative artificial intelligence (GenAI) has ignited discussions surrounding its potential to enhance creative pursuits. However, distinctions between children’s and adult’s creative needs exist, which is important when considering the possibility of GenAI for children’s creative usage. Building upon work in Human-Computer Interaction (HCI), fostering children’s computational thinking skills, this study explores interactions between children (aged 7-13) and GenAI tools through methods of participatory design. We seek to answer two questions: (1) How do children in co-design workshops perceive GenAI tools and their usage for creative works? and (2) How do children navigate the creative process while using GenAI tools? How might these interactions support their confidence in their ability to create? Our findings contribute a model that describes the potential contexts underpinning child-GenAI creative interactions and explores implications of this model for theories of creativity, design, and use of GenAI as a constructionist tool for creative self-efficacy.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {117},
numpages = {18},
keywords = {Artificial Intelligence, Children, Co-Design, Constructionism, Creativity, Participatory Design},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3652620.3686244,
author = {Wiesmayr, Bianca and Zoitl, Alois and H\"{a}stbacka, David},
title = {Modeling Service Choreographies and Collaborative Tasks for Autonomous Mixed-Fleet Systems},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3686244},
doi = {10.1145/3652620.3686244},
abstract = {Complex processes require the cooperation of a variety of subsystems, such as robots, autonomous vehicles, and human-operated devices. These so-called mixed-fleet systems are found in logistics and production use cases, which also demand a high flexibility. Hence, the choreography that orchestrates the involved systems must be adaptable and reconfigurable. Enabling to add or remove subsystems flexibly during runtime requires a strong decoupling, which is found in multi-agent systems. In this paper, we explore a model-driven engineering process for service choreographies of flexible, heterogeneous, and autonomous mixed-fleet systems. Each complex process is decomposed into services and tasks, which are flexibly assigned to resources. The resulting layered service-oriented architecture is realized as an event-based system. We define requirements for modeling services, tasks, and events and evaluate different modeling language based on their applicability for each layer, i.e., BPMN, SysML/UML, and IEC 61499. We demonstrate and evaluate our architecture using a logistics use case scenario. The results show that these languages are suitable candidates for modeling event-based process models and that the diagrams can be used to capture service choreography models for decentralized systems. Future work will investigate how these models can be validated comprehensively and used for system implementation.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {234–244},
numpages = {11},
keywords = {model-driven engineering, services, event-based architecture},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3687997.3695650,
author = {Pontes Miranda, James William and Bruneliere, Hugo and Tisi, Massimo and Suny\'{e}, Gerson},
title = {Towards an In-Context LLM-Based Approach for Automating the Definition of Model Views},
year = {2024},
isbn = {9798400711800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687997.3695650},
doi = {10.1145/3687997.3695650},
abstract = {In the Model-Driven Engineering (MDE) of complex systems, multiple models represent various systems' aspects. In practice, these models are often unconnected and specified using different modeling languages. Model view solutions can be employed to automatically combine such models. However, writing model view definitions is not trivial. When modeling languages are semantically distant and/or have a large number of concepts, it can quickly become difficult to manually identify the language elements to be selected, associated, or queried to build a model view. As a solution, this paper proposes an in-context Large Language Model (LLM)-based approach to assist engineers in writing model-view definitions. Notably, we rely on LLMs and Prompt Engineering techniques to automatically generate drafts of model-view definitions by providing as input only minimal information on the modeling languages to be combined. We implemented our approach by integrating the EMF Views solution for model views with the LangChain framework for LLM-based applications. To this end, we tailored LangChain to handle EMF metamodels. We validated our approach and implementation on a set of model views originally specified either in VPDL, the ViewPoint Definition Language of EMF Views, or as ATL model-to-model transformations. We compared these original model view definitions with the ones we automatically generated. The obtained results show the feasibility and applicability of our approach.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {29–42},
numpages = {14},
keywords = {Large language models, Model views, Model-driven engineering, Modeling languages, Prompt engineering},
location = {Pasadena, CA, USA},
series = {SLE '24}
}

@inproceedings{10.1145/3652620.3688224,
author = {Khalilipour, Alireza and Challenger, Moharram},
title = {Towards Intelligent Model Management: An Exploratory Study and Road-mapping},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688224},
doi = {10.1145/3652620.3688224},
abstract = {With data science entering various domains, new branches are emerging due to the extraction of latent knowledge from each domain's data. Model-based engineering and modeling are no exceptions. Now is the time to open a new chapter in this field by leveraging advanced artificial intelligence techniques. As the number and complexity of models increase, NP-complete problems arise that cannot be effectively addressed through deterministic management solutions. An effective way to address these challenges is by applying non-deterministic intelligent methodologies and data science-derived solutions. The increasing number of models and the formation of large model repositories necessitate intelligent model management, which aims to recognize hidden patterns and knowledge within these repositories using data science, machine learning techniques, and statistical and probabilistic methods for reuse. Despite the progress made in this area, both theoretically and practically, intelligent model management has not yet secured a prominent place in the body of knowledge of model-driven engineering. In this paper, we aim to clarify the exact position of intelligent model management by providing precise definitions, distinguishing it from conventional management, and identifying associated challenges. The above objective outlines the research approach in this area, making it easy for researchers to comprehend the procedures they employ for conducting their investigations.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1015–1024},
numpages = {10},
keywords = {intelligent model management, model repositories, machine learning, model-driven engineering, model reuse},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3686248,
author = {Bonetti, Federico and Bucchiarone, Antonio and Michael, Judith and Cicchetti, Antonio and Marconi, Annapaola and Rumpe, Bernhard},
title = {Digital Twins of Socio-Technical Ecosystems to Drive Societal Change},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3686248},
doi = {10.1145/3652620.3686248},
abstract = {While the engineering of digital twins (DTs) of cyber-physical systems already faces a number of challenges, DTs of socio-technical systems are made even more complex by human and social factors, and a comprehensive representation of their internal relations is currently lacking. DTs for socio-technical systems could open up new ways of achieving common societal goals by i) providing an understanding of complex interactions and processes, and by ii) facilitating the design of and participation in collective actions. In this context, dynamic adaptation and motivational strategies would be required to swiftly address sub-optimal system behavior. To enable the model-driven engineering of DTs responding to such requirements, we propose a conceptual model of socio-technical systems and discuss it with use-case scenarios. The presented approach supports our vision of future DT-based model-driven interventions, empowering citizens and stakeholders in driving societal change and increasing community resilience.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {275–286},
numpages = {12},
keywords = {digital twin, modeling, socio-technical system, model-driven engineering, system engineering},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3688334,
author = {Hagel, Nathan and Hili, Nicolas and Schwab, Didier},
title = {Turning Low-Code Development Platforms into True No-Code with LLMs},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688334},
doi = {10.1145/3652620.3688334},
abstract = {The relevance of low-code / no-code development has grown substantially in research and practice over the years to allow nontechnical users to create applications and, therefore, democratise software development. One problem in this domain still persists: many platforms remain low-code as the underlying modeling layer still requires professionals to write/design a model, often using Domain Specific Languages (DSLs). With the rise of generative AI and Large Language Models (LLMs) and their capabilities, new possibilities emerge on how Low Code Development Platforms (LCDPs) can be improved.This paper shows how the capabilities of LLMs can be leveraged to turn DSL-based low-code platforms into true no-code. We analyzed how textual modeling can be replaced by generating the required model using LLMs. We conducted a user experiment to compare textual modeling with the use of LLMs for that task. Our results show that task completion time could be significantly improved, and the majority of users prefer using the LLM-aided modeling. Based on these findings, we discuss the integration of these techniques into an existing low-code platform to transform it into true no-code.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {876–885},
numpages = {10},
keywords = {LLM, AI, low-code development platform, meta-model, model-driven engineering, DSL},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3640310.3674091,
author = {L\'{o}pez, Jos\'{e} Antonio Hern\'{a}ndez and F\"{o}ldi\'{a}k, M\'{a}t\'{e} and Varr\'{o}, D\'{a}niel},
title = {Text2VQL: Teaching a Model Query Language to Open-Source Language Models with ChatGPT},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674091},
doi = {10.1145/3640310.3674091},
abstract = {While large language models (LLMs) like ChatGPT has demonstrated impressive capabilities in addressing various software engineering tasks, their use in a model-driven engineering (MDE) context is still in an early stage. Since the technology is proprietary and accessible solely through an API, its use may be incompatible with the strict protection of intellectual properties in industrial models. While there are open-source LLM alternatives, they often lack the power of proprietary models and require extensive data fine-tuning to realize their full potential. Furthermore, open-source datasets tailored for MDE tasks are scarce, posing challenges for training such models effectively.In this work, we introduce Text2VQL, a framework that generates graph queries captured in the VIATRA Query Language (VQL) from natural language specifications using open-source LLMs. Initially, we create a high-quality synthetic dataset comprising pairs of queries and their corresponding natural language descriptions using ChatGPT and VIATRA parser. Leveraging this dataset, we use parameter-efficient tuning to specialize three open-source LLMs, namely, DeepSeek Coder 1b, DeepSeek Coder 7b, and CodeLlama 7b for VQL query generation. Our experimental evaluation demonstrates that the fine-tuned models outperform the base models in query generation, highlighting the usefulness of our synthetic dataset. Moreover, one of the fine-tuned models achieves performance comparable to ChatGPT.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {13–24},
numpages = {12},
keywords = {ChatGPT, VIATRA Query Language (VQL), large language model (LLM), model query language, query generation},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3640310.3674089,
author = {Costa, Carlos Dur\'{a} and L\'{o}pez, Jos\'{e} Antonio Hern\'{a}ndez and Cuadrado, Jes\'{u}s S\'{a}nchez},
title = {ModelMate: A recommender for textual modeling languages based on pre-trained language models},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674089},
doi = {10.1145/3640310.3674089},
abstract = {Current DSL environments lack smart editing facilities intended to enhance modeler productivity and cannot keep pace of current developments of integrated development environments based on AI. In this paper, we propose an approach to address this shortcoming through a recommender system specifically tailored for textual DSLs based on the fine-tuning of pre-trained language models. We identify three main tasks: identifier suggestion, line completion, and block completion, which we implement over the same fine-tuned model and we propose a workflow to apply these tasks to any textual DSL. We have evaluated our approach with different pre-trained models for three DSLs: Emfatic, Xtext and a DSL to specify domain entities, showing that the system performs well and provides accurate suggestions. We compare it against existing approaches in the feature name recommendation task showing that our system outperforms the alternatives. Moreover, we evaluate the inference time of our approach obtaining low latencies, which makes the system adequate for live assistance. Finally, we contribute a concrete recommender, named ModelMate, which implements the training, evaluation and inference steps of the workflow as well as providing integration into Eclipse-based textual editors.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {183–194},
numpages = {12},
keywords = {Machine learning, Meta-modeling, Model-Driven Engineering, Recommendation},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3652620.3687811,
author = {M. Mosthaf, My and Wasowski, Andrzej},
title = {From a Natural to a Formal Language with DSL Assistant},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687811},
doi = {10.1145/3652620.3687811},
abstract = {The development of domain-specific languages (DSLs) is a laborious and iterative process that seems to naturally lean to the use of generative artificial intelligence. We design and prototype DSL Assistant, a tool that integrates generative language models to support the development of DSLs. DSL Assistant uses OpenAI's assistant API with GPT-4o to generate DSL grammars and example instances. To reflect real-world use, DSL Assistant supports several different interaction modes for evolving a DSL design, and includes automatic error repair. Our experiments show that DSL Assistant helps users to create and modify DSLs. However the quality of the generated DSLs depends on the specific domain and the followed interaction patterns.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {541–549},
numpages = {9},
keywords = {domain-specific languages, generative AI, large language models},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3687802,
author = {Buchmann, Thomas},
title = {Prompting Bidirectional Model Transformations - The Good, The Bad and The Ugly},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687802},
doi = {10.1145/3652620.3687802},
abstract = {This paper investigates the comparative effectiveness of model-to-model transformations generated by an LLM based upon user prompts versus those created with dedicated model transformation languages, using a standard benchmark. The emergence of Generative AI offers a novel approach, allowing developers to specify transformations in natural language rather than learning specialized languages. However, our findings suggest that, in its current state, generative AI does not yet pose a threat to dedicated model transformation languages. While AI-assisted approaches promise to provide flexibility and accessibility, dedicated model transformation languages still offer structured advantages crucial for complex transformations, especially when bidirectionality and incrementality are mandatory requirements. This research contributes to the ongoing discourse on the role of AI in software engineering, highlighting its potential and current limitations in enhancing model transformation processes.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {550–555},
numpages = {6},
keywords = {modeling, LLM, MDE, AI, modeltransformation, Bx},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3587102.3588815,
author = {Daun, Marian and Brings, Jennifer},
title = {How ChatGPT Will Change Software Engineering Education},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588815},
doi = {10.1145/3587102.3588815},
abstract = {This position paper discusses the potential for using generative AIs like ChatGPT in software engineering education. Currently, discussions center around potential threats emerging from student's use of ChatGPT. For instance, generative AI will limit the usefulness of graded homework dramatically. However, there exist potential opportunities as well. For example, ChatGPT's ability to understand and generate human language allows providing personalized feedback to students, and can thus accompany current software engineering education approaches. This paper highlights the potential for enhancing software engineering education. The availability of generative AI will improve the individualization of education approaches. In addition, we discuss the need to adapt software engineering curricula to the changed profiles of software engineers. Moreover, we point out why it is important to provide guidance for using generative AI and, thus, integrate it in courses rather than accepting the unsupervised use by students, which can negatively impact the students' learning.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {110–116},
numpages = {7},
keywords = {ChatGPT, generative AI, software engineering education},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3729434.3729456,
author = {B\"{o}hm, Karsten},
title = {Deep and Surface Representation of Competences in Academic Curricula: A new approach to address human consumers and formal structures using Generative AI},
year = {2025},
isbn = {9798400712630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3729434.3729456},
doi = {10.1145/3729434.3729456},
abstract = {The curricular design of programs in Higher Education is increasingly oriented towards competence-based learning goals in order to provide more specific and explicit qualifications. At the same time the descriptions of competences are mostly based on textual descriptions often in informal style targeted towards different audiences making descriptions vague and harder to compare. Formal models for the specification of competences exist for some time and even semantic models are being developed, e.g., the European Learning Model. These different approaches lead to a gap between formal and informal competence descriptions that require manual efforts of curriculum designer to maintain. This research borrows the concept of Deep and Surface representation models from the field of linguistics to unify the different approaches. Additionally, it focuses on the functionality of Generative Artificial Intelligence in the form of Large Language Models to close the aforementioned gap. It demonstrates the potential of the technology in both directions and discusses application potential of the concept.},
booktitle = {Proceedings of the 6th International Conference on Modern Educational Technology},
pages = {78–85},
numpages = {8},
keywords = {Curricular Design, European Learning Model, Generative Artificial Intelligence, Higher Education, LLM, Semantic Web},
location = {
},
series = {ICMET '24}
}

@inproceedings{10.1145/3652620.3688336,
author = {Bucchiarone, Antonio and Panciera, Marco and Cicchetti, Antonio and Mana, Nadia and Castelluccio, Carlotta and Stott, Lee},
title = {PromptDeck: A No-Code Platform for Modular Prompt Engineering},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688336},
doi = {10.1145/3652620.3688336},
abstract = {This paper introduces a no-code platform for modular prompt engineering, designed to democratize access to generative AI for nondevelopers. By integrating advanced technologies such as Node.js, Express, MongoDB, and Azure OpenAI services, the platform provides a robust and flexible environment for creating and managing AI-driven tasks. The intuitive frontend, built with React and TypeScript, enables users with minimal coding expertise to design, execute, and evaluate complex AI workflows. A key feature of the platform is its extensible plugin system, which allows users to easily incorporate additional functionalities to meet their specific needs. This no-code approach empowers a broader audience to harness the power of generative AI, fostering innovation and enabling diverse applications across various fields. By lowering the technical barriers, the platform paves the way for widespread adoption of AI technologies, driving the future of AI-enhanced solutions.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {895–904},
numpages = {10},
keywords = {low-code development platforms, no-code, generative AI, prompt engineering, modularization},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3687810,
author = {Tabassum, Mirza Rehenuma and Ritchie, Matthew J. and Mustafiz, Sadaf and Kienzle, J\"{o}rg},
title = {Using LLMs for Use Case Modelling of IoT Systems: An Experience Report},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687810},
doi = {10.1145/3652620.3687810},
abstract = {Requirements engineering (RE) plays an essential role in the success of system and software development. Textual use case models are valuable for capturing diverse scenarios describing the interactions between the system and its actors, but their development, particularly for the Internet of Things (IoT), can be tedious and error-prone due to the added complexities and heterogeneous nature of such systems. Automating requirements elicitation and specification tasks with the use of generative AI is highly desirable. This paper explores the potential of large language models (LLMs) for generating interaction models for IoT systems from informal problem descriptions. We investigate the capabilities of OpenAI's GPT-4 and Google's Gemini for generating standard and UCM4IoT textual use cases by carrying out a comparative study using four IoT applications. While both of these LLMs show promise as supporting tools, our findings indicate a need for further refinement and domain-specific training to enhance their precision and reliability in requirements development for the IoT domain.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {611–619},
numpages = {9},
keywords = {requirements engineering, use case modelling, large language model, LLM, model-based development},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3688338,
author = {Lachance, Alexandre and Mosser, Sebastien},
title = {Building deduplicated model repositories to assess domain-specific languages evolution},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688338},
doi = {10.1145/3652620.3688338},
abstract = {Software evolution and maintenance is a real challenge in modern software engineering. In the context of model-driven development, which heavily rely on interconnected (meta-)models, tools and generators, evolving both models and their associated meta-models is particularly complex. This issue is also prevalent in language engineering, where evolving a language's grammar or semantics must remain consistent with the pre-existing models. In this paper, we explore how techniques inspired by repository mining can help a model designer/language engineer to build a deduplicated dataset of existing models available in open source repositories. Deduplication is essential to ensure the evolution made on the meta-model/language can be efficiently assessed. We apply the method to the P4 language, an industrial domain-specific language (Intel, Linux foundation) used to model software defined networks.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1025–1034},
numpages = {10},
keywords = {compiler, DSL, model, mining, evolution},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3687782,
author = {Lamas, Victor and R. Luaces, Miguel and Garcia-Gonzalez, Daniel},
title = {DSL-Xpert: LLM-driven Generic DSL Code Generation},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687782},
doi = {10.1145/3652620.3687782},
abstract = {Nowadays, large language models (LLMs) are an extremely useful and fast tool to complement and help in many jobs and current problems. However, there are cases where a pretty specific vocabulary is used in which these models were not previously trained, leading to less satisfactory results. More specifically, these models are less effective when dealing with less-known or unpublished domain-specific languages (DSLs). Within this field, the automatic generation of code based on such languages, starting from natural language, would speed up the development times of any related project, as well as the understanding of such DSLs. Therefore, this paper presents a tool in which developers can perform what is known as semantic parsing. In other words, the developer can ask a pre-trained LLM to translate a natural language instruction into the vocabulary of the established DSL. Thus, by setting the DSL grammar as context (grammar prompting) and providing usage examples (few-shot learning), the LLM can quickly generate reliable domain-specific code, significantly improving the quality of life of the developers. A video demonstration of the tool is shown in the following link: https://zenodo.org/records/12610506.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {16–20},
numpages = {5},
keywords = {domain-specific languages (DSLS), large language models (LLMS), semantic parsing, grammar prompting, few-shot learning},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1109/CGO57630.2024.10444830,
author = {Okuda, Katsumi and Amarasinghe, Saman},
title = {AskIt: Unified Programming Interface for Programming with Large Language Models},
year = {2024},
isbn = {9798350395099},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO57630.2024.10444830},
doi = {10.1109/CGO57630.2024.10444830},
abstract = {Large Language Models (LLMs) exhibit a unique phenomenon known as emergent abilities, demonstrating adeptness across numerous tasks, from text summarization to code generation. While these abilities open up novel avenues in software design and crafting, their incorporation presents substantial challenges. Developers face decisions regarding the use of LLMs for directly performing tasks within applications as well as for generating and executing code to accomplish these tasks. Moreover, effective prompt design becomes a critical concern, given the necessity of extracting data from natural language outputs. To address these complexities, this paper introduces AskIt, a domain-specific language (DSL) specifically designed for LLMs. AskIt simplifies LLM integration by providing a unified interface that not only allows for direct task execution using LLMs but also supports the entire cycle of code generation and execution. This dual capability is achieved through (1) type-guided output control, (2) template-based function definitions, and (3) prompt generation for both usage modes. Our evaluations underscore AskIt's effectiveness. Across 50 tasks, AskIt generated concise prompts, achieving a 16.14 \% reduction in prompt length compared to benchmarks. Additionally, by enabling a seamless transition between using LLMs directly in applications and for generating code, AskIt achieved significant efficiency improvements, as observed in our GSM8K benchmark experiments. The implementations of AskIt in TypeScript and Python are available at https://github.com/katsumiok/ts-askit and https://github.com/katsumiok/pyaskit, respectively.},
booktitle = {Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {41–54},
numpages = {14},
keywords = {domain specific language, code generation, large language model, software engineering, artificial intelligence},
location = {Edinburgh, United Kingdom},
series = {CGO '24}
}

@inproceedings{10.1145/3650105.3652290,
author = {Abukhalaf, Seif and Hamdaqa, Mohammad and Khomh, Foutse},
title = {PathOCL: Path-Based Prompt Augmentation for OCL Generation with GPT-4},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652290},
doi = {10.1145/3650105.3652290},
abstract = {The rapid progress of AI-powered programming assistants, such as GitHub Copilot, has facilitated the development of software applications. These assistants rely on large language models (LLMs), which are foundation models (FMs) that support a wide range of tasks related to understanding and generating language. LLMs have demonstrated their ability to express UML model specifications using formal languages like the Object Constraint Language (OCL). However, the context size of the prompt is limited by the number of tokens an LLM can process. This limitation becomes significant as the size of UML class models increases. In this study, we introduce PathOCL, a novel path-based prompt augmentation technique designed to facilitate OCL generation. PathOCL addresses the limitations of LLMs, specifically their token processing limit and the challenges posed by large UML class models. PathOCL is based on the concept of chunking, which selectively augments the prompts with a subset of UML classes relevant to the English specification. Our findings demonstrate that PathOCL, compared to augmenting the complete UML class model (UML-Augmentation), generates a higher number of valid and correct OCL constraints using the GPT-4 model. Moreover, the average prompt size crafted using PathOCL significantly decreases when scaling the size of the UML class models.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {108–118},
numpages = {11},
keywords = {object constraint language (OCL), simple path, prompt engineering, large language model (LLM), generative pre-trained transformer (GPT), foundation model (FM)},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@article{10.1145/3736775,
author = {Ma, Xinyu and Meng, Hengyu and Wu, Ziwei and Wang, Zeyu and von Chamier-Waite, Clea T},
title = {Becoming Space: Exploring Agential Materiality through AI-Generated Metamorphosis in Artistic Practice},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3736775},
doi = {10.1145/3736775},
abstract = {This paper examines the intersection of materiality, agency, and transformation in AI-assisted artistic practice through Becoming Space, an installation of 3D printed sculptures and video. Drawing from Ovid’s Metamorphoses, the work investigates states of transformation between human and animal forms using generative AI and additive manufacturing. Through a theoretical investigation, this paper tries to analyze how material-discursive practices emerge in the convergence of language and physical materials. The work demonstrates how agency distributes across human and nonhuman actors in creative processes, challenging traditional notions of artistic authorship and subjectivity. This investigation contributes to understanding how generative AI technologies participate in artistic practices, revealing creativity as an entangled phenomenon where multiple agencies converge and transform.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = jul,
articleno = {29},
numpages = {11},
keywords = {Materiality, Agency, Becoming-animal, Metamorphoses, Generative AI, 3D Printing, Sculpture, Nonhuman}
}

@inproceedings{10.1145/3652620.3687784,
author = {Ardimento, Pasquale and Bernardi, Mario Luca and Cimitile, Marta and Scalera, Michele},
title = {A RAG-based Feedback Tool to Augment UML Class Diagram Learning},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687784},
doi = {10.1145/3652620.3687784},
abstract = {This paper introduces an advanced functionality designed to facilitate the learning of UML class diagram construction. Built upon an integrated Retrieval Augmented Generation Large Language Model, the functionality provides enriched feedback by leveraging accumulated knowledge. The functionality is implemented in an existing tool named UML Miner, a Visual Paradigm plugin that captures and analyzes student-generated UML diagrams by applying process mining techniques. By offering personalized feedback and continuous support during modeling, the tool aims to enhance learning outcomes and students' engagement.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {26–30},
numpages = {5},
keywords = {learning, UML, software modeling, retrieval augmented generation, large language model, tool},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3567512.3567534,
author = {Bertram, Vincent and Bo\ss{}, Miriam and Kusmenko, Evgeny and Nachmann, Imke Helene and Rumpe, Bernhard and Trotta, Danilo and Wachtmeister, Louis},
title = {Neural Language Models and Few Shot Learning for Systematic Requirements Processing in MDSE},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567512.3567534},
doi = {10.1145/3567512.3567534},
abstract = {Systems engineering, in particular in the automotive domain, needs to cope with the massively increasing numbers of requirements that arise during the development process.  The language in which requirements are written is mostly informal and highly individual. This hinders  automated processing of requirements as well as the linking of requirements  to models. Introducing formal requirement notations in existing  projects leads to the challenge of translating masses of requirements and the necessity of training for requirements engineers.  In this paper, we derive domain-specific language constructs helping us to avoid ambiguities in requirements and increase the level  of formality. The main contribution is the adoption and evaluation of few-shot learning with large pretrained language models for the automated translation of informal requirements to structured languages such as a requirement DSL.},
booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {260–265},
numpages = {6},
keywords = {few-shot learning, model-driven engineering, model-driven requirements engineering, natural language processing},
location = {Auckland, New Zealand},
series = {SLE 2022}
}

@inproceedings{10.1145/3664647.3680814,
author = {Cai, Pengxiang and Liu, Zhiwei and Zhu, Guibo and Niu, Yunfang and Wang, Jinqiao},
title = {Auto DragGAN: Editing the Generative Image Manifold in an Autoregressive Manner},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680814},
doi = {10.1145/3664647.3680814},
abstract = {Pixel-level fine-grained image editing remains an open challenge. Previous works fail to achieve an ideal trade-off between control granularity and inference speed. They either fail to achieve pixel-level fine-grained control, or their inference speed requires optimization. To address this, this paper for the first time employs a regression-based network to learn the variation patterns of StyleGAN latent codes during the image dragging process. This method enables pixel-level precision in dragging editing with little time cost. Users can specify handle points and their corresponding target points on any GAN-generated images, and our method will move each handle point to its corresponding target point. Through experimental analysis, we discover that a short movement distance from handle points to target points yields a high-fidelity edited image, as the model only needs to predict the movement of a small portion of pixels. To achieve this, we decompose the entire movement process into multiple sub-processes. Specifically, we develop a transformer encoder-decoder based network named 'Latent Predictor' to predict the latent code motion trajectories from handle points to target points in an autoregressive manner. Moreover, to enhance the prediction stability, we introduce a component named 'Latent Regularizer', aimed at constraining the latent code motion within the distribution of natural images. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) inference speed and image editing performance at the pixel-level granularity.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {3372–3380},
numpages = {9},
keywords = {autoregressive model, gans, image editing},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1145/3637228,
author = {Ca\~{n}izares, Pablo C. and L\'{o}pez-Morales, Jose Mar\'{\i}a and P\'{e}rez-Soler, Sara and Guerra, Esther and de Lara, Juan},
title = {Measuring and Clustering Heterogeneous Chatbot Designs},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3637228},
doi = {10.1145/3637228},
abstract = {Conversational agents, or chatbots, have become popular to access all kind of software services. They provide an intuitive natural language interface for interaction, available from a wide range of channels including social networks, web pages, intelligent speakers or cars. In response to this demand, many chatbot development platforms and tools have emerged. However, they typically lack support to statically measure properties of the chatbots being built, as indicators of their size, complexity, quality or usability. Similarly, there are hardly any mechanisms to compare and cluster chatbots developed with heterogeneous technologies.To overcome this limitation, we propose a suite of 21 metrics for chatbot designs, as well as two clustering methods that help in grouping chatbots along their conversation topics and design features. Both the metrics and the clustering methods are defined on a neutral chatbot design language, becoming independent of the implementation platform. We provide automatic translations of chatbots defined on some major platforms into this neutral notation to perform the measurement and clustering. The approach is supported by our tool Asymob, which we have used to evaluate the metrics and the clustering methods over a set of 259 Dialogflow and Rasa chatbots from open-source repositories. The results open the door to incorporating the metrics within chatbot development processes for the early detection of quality issues, and to exploit clustering to organise large collections of chatbots into significant groups to ease chatbot comprehension, search and comparison.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {90},
numpages = {43},
keywords = {Chatbot design, metrics, clustering, quality assurance, model-driven engineering}
}

@inproceedings{10.1145/3640310.3674085,
author = {Ben Chaaben, Meriem and Ben Sghaier, Oussama and Dhaouadi, Mouna and Elrasheed, Nafisa and Darif, Ikram and Jaoua, Imen and Oakes, Bentley and Syriani, Eugene and Hamdaqa, Mohammad},
title = {Toward Intelligent Generation of Tailored Graphical Concrete Syntax},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674085},
doi = {10.1145/3640310.3674085},
abstract = {In model-driven engineering, the concrete syntax of a domain-specific modeling language (DSML) is fundamental as it constitutes the primary point of interaction between the user and the DSML. Nevertheless, the conventional one-size-fits-all approach to concrete syntax often undermines the effectiveness of DSMLs, as it fails to accommodate the diverse constraints and specific requirements inherent to diverse users and usage contexts. Such shortcomings can lead to a significant decline in the performance, usability, and efficiency of DSMLs. This vision paper proposes a conceptual framework to generate concrete syntax intelligently. Our framework considers multiple concerns of users and aims to align the concrete syntax with the context of the DSML usage. Additionally, we detail a baseline process to employ our framework in practice, leveraging large language models to expedite the generation of tailored concrete syntax. We illustrate the potential of our vision with two concrete examples and discuss the shortcomings and research challenges of current intelligent generation techniques.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {160–171},
numpages = {12},
keywords = {Artificial Intelligence, Concrete Syntax, Domain-specific Modeling Languages, Large Language Models},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3640310.3674079,
author = {Sultan, Bastien and Apvrille, Ludovic},
title = {AI-Driven Consistency of SysML Diagrams},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674079},
doi = {10.1145/3640310.3674079},
abstract = {Graphical modeling languages, expected to simplify systems analysis and design, present a challenge in maintaining consistency across their varied views. Traditional rule-based methods for ensuring consistency in languages like UML often fall short in addressing complex semantic dimensions. Moreover, the integration of Large Language Models (LLMs) into Model Driven Engineering (MDE) introduces additional consistency challenges, as LLM's limited output contexts requires the integration of responses. This paper presents a new framework that automates the detection and correction of inconsistencies across different views, leveraging formally defined rules and incorporating OpenAI's GPT, as implemented in TTool. Focusing on the consistency between use case and block diagrams, the framework is evaluated through its application to three case studies, highlighting its potential to significantly enhance consistency management in graphical modeling.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {149–159},
numpages = {11},
location = {Linz, Austria},
series = {MODELS '24}
}

